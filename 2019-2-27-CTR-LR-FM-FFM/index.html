<!DOCTYPE html><html lang="zh-CN"><head><meta name="generator" content="Hexo 3.8.0"><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description"><title>CTR系列（一）：LR、FM/FFM | CHAI' blog</title><link rel="stylesheet" type="text/css" href="/css/style.css?v=0.0.0"><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/normalize/8.0.1/normalize.min.css"><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/pure/1.0.0/pure-min.css"><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/pure/1.0.0/grids-responsive-min.css"><link rel="stylesheet" href="//lib.baomitu.com/font-awesome/4.7.0/css/font-awesome.min.css"><script type="text/javascript" src="//lib.baomitu.com/jquery/3.4.0/jquery.min.js"></script><link rel="icon" mask sizes="any" href="/favicon.ico"><link rel="Shortcut Icon" type="image/x-icon" href="/favicon.ico"><link rel="apple-touch-icon" href="/apple-touch-icon.png"><link rel="apple-touch-icon-precomposed" href="/apple-touch-icon.png"></head><body><div class="body_container"><div id="header"><div class="site-name"><h1 class="hidden">CTR系列（一）：LR、FM/FFM</h1><a id="logo" href="/.">CHAI' blog</a><p class="description"></p></div><div id="nav-menu"><a class="current" href="/."><i class="fa fa-home"> 首页</i></a><a href="/archives/"><i class="fa fa-archive"> 归档</i></a><a href="/about/"><i class="fa fa-user"> 关于</i></a><a href="/atom.xml"><i class="fa fa-rss"> 订阅</i></a></div></div><div class="pure-g" id="layout"><div class="pure-u-1 pure-u-md-3-4"><div class="content_container"><div class="post"><h1 class="post-title">CTR系列（一）：LR、FM/FFM</h1><div class="post-meta">Feb 27, 2019<span> | </span><span class="category"><a href="/categories/Machine-Learning/">Machine Learning</a></span></div><div class="clear"><div class="toc-article" id="toc"><div class="toc-title">文章目录</div><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#Logistic-Regression-LR"><span class="toc-number">1.</span> <span class="toc-text">Logistic Regression (LR)</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Factorization-Machine-FM"><span class="toc-number">2.</span> <span class="toc-text">Factorization Machine (FM)</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Field-aware-Factorization-Machines-FFM"><span class="toc-number">3.</span> <span class="toc-text">Field-aware Factorization Machines (FFM)</span></a></li></ol></div></div><div class="post-content"><h2 id="Logistic-Regression-LR"><a href="#Logistic-Regression-LR" class="headerlink" title="Logistic Regression (LR)"></a>Logistic Regression (LR)</h2><p>tensorflow的实习网上有很多，这里作者也实现了一份：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> roc_auc_score</span><br><span class="line"><span class="keyword">from</span> tqdm <span class="keyword">import</span> tqdm</span><br><span class="line"><span class="keyword">import</span> config</span><br><span class="line"><span class="keyword">import</span> utils</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LR</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, train_loader, test_loader, var_list, opt_algo=<span class="string">'gd'</span>, </span></span></span><br><span class="line"><span class="function"><span class="params">                       learning_rate=<span class="number">0.01</span>, epoch=<span class="number">10</span>, early_stop_round=None, </span></span></span><br><span class="line"><span class="function"><span class="params">                       l2_w=<span class="number">0.</span>, random_seed=None)</span>:</span></span><br><span class="line">        self.graph = tf.Graph()</span><br><span class="line"></span><br><span class="line">        self.train_loader = train_loader</span><br><span class="line">        self.test_loader = test_loader</span><br><span class="line">        self.var_list = var_list</span><br><span class="line">        self.opt_algo = opt_algo</span><br><span class="line">        self.learning_rate = learning_rate</span><br><span class="line">        self.epoch = epoch</span><br><span class="line">        self.early_stop_round = early_stop_round</span><br><span class="line">        self.l2_w = l2_w</span><br><span class="line">        self.random_seed = random_seed</span><br><span class="line"></span><br><span class="line">        self.train_scores = []</span><br><span class="line">        self.test_scores = []</span><br><span class="line"></span><br><span class="line">        <span class="keyword">with</span> self.graph.as_default():</span><br><span class="line">            <span class="keyword">if</span> self.random_seed <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">                tf.set_random_seed(self.random_seed)</span><br><span class="line">            self.X = tf.sparse_placeholder(config.DTYPE)</span><br><span class="line">            self.y = tf.placeholder(config.DTYPE)</span><br><span class="line"></span><br><span class="line">            self.var_dict = utils.get_var(self.var_list)</span><br><span class="line">            w = self.var_dict[<span class="string">'w'</span>]</span><br><span class="line">            b = self.var_dict[<span class="string">'b'</span>]</span><br><span class="line"></span><br><span class="line">            xw = tf.sparse_tensor_dense_matmul(self.X, w)</span><br><span class="line">            logits = tf.reshape(tf.add(xw, b), [<span class="number">-1</span>])</span><br><span class="line">            self.y_preds = tf.sigmoid(logits)</span><br><span class="line"></span><br><span class="line">            self.loss = tf.reduce_mean(</span><br><span class="line">                tf.nn.sigmoid_cross_entropy_with_logits(labels=self.y, logits=logits)</span><br><span class="line">            ) + self.l2_w * tf.nn.l2_loss(w)</span><br><span class="line"></span><br><span class="line">            self.optimizer = utils.get_optimizer(self.opt_algo, self.learning_rate, self.loss)</span><br><span class="line"></span><br><span class="line">            self.sess = tf.Session()</span><br><span class="line">            tf.global_variables_initializer().run(session=self.sess)</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">run</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">for</span> num_epoch <span class="keyword">in</span> tqdm(range(self.epoch)):</span><br><span class="line">            train_iter = self.train_loader.sparse_iter()</span><br><span class="line">            train_y_hat = []</span><br><span class="line">            <span class="keyword">for</span> X, y <span class="keyword">in</span> train_iter:</span><br><span class="line">                _, ll = self.sess.run([self.optimizer, self.loss], feed_dict=&#123;self.X: X, self.y: y&#125;)</span><br><span class="line">                y_hat = self.sess.run(self.y_preds, feed_dict=&#123;self.X: X, self.y: <span class="literal">None</span>&#125;)</span><br><span class="line">                train_y_hat.extend(y_hat)</span><br><span class="line">            self.train_scores.append(roc_auc_score(self.train_loader.y, train_y_hat))</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> self.test_loader <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">                test_iter = self.test_loader.sparse_iter()</span><br><span class="line">                test_y_hat = []</span><br><span class="line">                <span class="keyword">for</span> X, y <span class="keyword">in</span> test_iter:</span><br><span class="line">                    y_hat = self.sess.run(self.y_preds, feed_dict=&#123;self.X: X, self.y: <span class="literal">None</span>&#125;)</span><br><span class="line">                    test_y_hat.extend(y_hat)</span><br><span class="line">                self.test_scores.append(roc_auc_score(self.test_loader.y, test_y_hat))</span><br><span class="line">            <span class="keyword">if</span> self.early_stop_round <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">                <span class="keyword">if</span> num_epoch &gt; self.early_stop_round:</span><br><span class="line">                    <span class="keyword">if</span> np.argmax(self.test_scores) == num_epoch - self.early_stop_round <span class="keyword">and</span> self.test_scores[<span class="number">-1</span>] - self.test_scores[num_epoch - self.early_stop_round] &lt; <span class="number">1e-3</span>:</span><br><span class="line">                        <span class="keyword">break</span></span><br></pre></td></tr></table></figure>
<p>其他的函数在文章末尾有介绍。</p>
<h2 id="Factorization-Machine-FM"><a href="#Factorization-Machine-FM" class="headerlink" title="Factorization Machine (FM)"></a>Factorization Machine (FM)</h2><p>FM是通过自动对特征进行了组合，实现包含特征组合的模型。</p>
<p>$$y(x) = \omega_0 + \sum_{i=0}^{N} \omega_i x_i + \sum_{i=1}^{N} \sum_{j=i+1}^{N} w_{ij} x_i x_j$$</p>
<p>其中，$$N$$表示特征的数量。此时，参数的数量为$$N(N-1)/2 + N + 1$$。</p>
<p>进一步分解，</p>
<p>$$y(x) = \omega_0 + \sum_{i=0}^{N} \omega_i x_i + \sum_{i=1}^{N} \sum_{j=i+1}^{N} &lt;v_i, v_j&gt; x_i x_j$$</p>
<p>其中，$$v_i$$表示第$$i$$维特征的参数，$$&lt;\cdot, \cdot&gt;$$表示向量点积。此时，参数的数量减少为$KN+N+1$个。</p>
<p>为了方便计算，进一步简化</p>
<p>$$\sum_{i=1}^{N} \sum_{j=i+1}^{N} &lt;v_i, v_j&gt; x_i x_j = \frac{1}{2} \sum_{k=1}^{K} ((\sum_{i=1}^{N} v_{ik} x_i)^2 - \sum_{i=1}^{N} v_{ik}^2 x_i^2)$$</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> roc_auc_score</span><br><span class="line"><span class="keyword">from</span> tqdm <span class="keyword">import</span> tqdm</span><br><span class="line"><span class="keyword">import</span> config</span><br><span class="line"><span class="keyword">import</span> utils</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">FM</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, train_loader, test_loader, var_list, opt_algo=<span class="string">'gd'</span>, </span></span></span><br><span class="line"><span class="function"><span class="params">                       learning_rate=<span class="number">0.01</span>, epoch=<span class="number">10</span>, early_stop_round=None, </span></span></span><br><span class="line"><span class="function"><span class="params">                       l2_w=<span class="number">0.</span>, l2_v=<span class="number">0.</span>, random_seed=None)</span>:</span></span><br><span class="line">        self.graph = tf.Graph()</span><br><span class="line"></span><br><span class="line">        self.train_loader = train_loader</span><br><span class="line">        self.test_loader = test_loader</span><br><span class="line">        self.var_list = var_list</span><br><span class="line">        self.opt_algo = opt_algo</span><br><span class="line">        self.learning_rate = learning_rate</span><br><span class="line">        self.epoch = epoch</span><br><span class="line">        self.early_stop_round = early_stop_round</span><br><span class="line">        self.l2_w = l2_w</span><br><span class="line">        self.l2_v = l2_v</span><br><span class="line">        self.random_seed = random_seed</span><br><span class="line"></span><br><span class="line">        self.train_scores = []</span><br><span class="line">        self.test_scores = []</span><br><span class="line"></span><br><span class="line">        <span class="keyword">with</span> self.graph.as_default():</span><br><span class="line">            <span class="keyword">if</span> self.random_seed <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">                tf.set_random_seed(self.random_seed)</span><br><span class="line">            self.X = tf.sparse_placeholder(config.DTYPE)</span><br><span class="line">            self.y = tf.placeholder(config.DTYPE)</span><br><span class="line"></span><br><span class="line">            self.var_dict = utils.get_var(self.var_list)</span><br><span class="line">            w = self.var_dict[<span class="string">'w'</span>]</span><br><span class="line">            v = self.var_dict[<span class="string">'v'</span>]</span><br><span class="line">            b = self.var_dict[<span class="string">'b'</span>]</span><br><span class="line"></span><br><span class="line">            xw = tf.sparse_tensor_dense_matmul(self.X, w)</span><br><span class="line">            </span><br><span class="line">            X_square = tf.SparseTensor(</span><br><span class="line">                self.X.indices, tf.square(self.X.values), tf.to_int64(tf.shape(self.X))</span><br><span class="line">                )</span><br><span class="line">            xv_square = tf.square(tf.sparse_tensor_dense_matmul(self.X, v))</span><br><span class="line">            v_square = tf.square(v)</span><br><span class="line">            p = <span class="number">0.5</span> * tf.reshape(</span><br><span class="line">                tf.reduce_sum(xv_square - tf.sparse_tensor_dense_matmul(X_square, v_square), <span class="number">1</span>),</span><br><span class="line">                [<span class="number">-1</span>, config.OUTPUT_DIM]</span><br><span class="line">            )</span><br><span class="line">            </span><br><span class="line">            logits = tf.reshape(xw + p + b, [<span class="number">-1</span>])</span><br><span class="line">            self.y_preds = tf.sigmoid(logits)</span><br><span class="line"></span><br><span class="line">            self.loss = tf.reduce_mean(</span><br><span class="line">                tf.nn.sigmoid_cross_entropy_with_logits(labels=self.y, logits=logits)</span><br><span class="line">            ) + self.l2_w * tf.nn.l2_loss(w) + self.l2_v * tf.nn.l2_loss(v)</span><br><span class="line"></span><br><span class="line">            self.optimizer = utils.get_optimizer(self.opt_algo, self.learning_rate, self.loss)</span><br><span class="line"></span><br><span class="line">            self.sess = tf.Session()</span><br><span class="line">            tf.global_variables_initializer().run(session=self.sess)</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">run</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">for</span> num_epoch <span class="keyword">in</span> tqdm(range(self.epoch)):</span><br><span class="line">            train_iter = self.train_loader.sparse_iter()</span><br><span class="line">            train_y_hat = []</span><br><span class="line">            <span class="keyword">for</span> X, y <span class="keyword">in</span> train_iter:</span><br><span class="line">                _, ll = self.sess.run([self.optimizer, self.loss], feed_dict=&#123;self.X: X, self.y: y&#125;)</span><br><span class="line">                y_hat = self.sess.run(self.y_preds, feed_dict=&#123;self.X: X, self.y: <span class="literal">None</span>&#125;)</span><br><span class="line">                train_y_hat.extend(y_hat)</span><br><span class="line">            self.train_scores.append(roc_auc_score(self.train_loader.y, train_y_hat))</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> self.test_loader <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">                test_iter = self.test_loader.sparse_iter()</span><br><span class="line">                test_y_hat = []</span><br><span class="line">                <span class="keyword">for</span> X, y <span class="keyword">in</span> test_iter:</span><br><span class="line">                    y_hat = self.sess.run(self.y_preds, feed_dict=&#123;self.X: X, self.y: <span class="literal">None</span>&#125;)</span><br><span class="line">                    test_y_hat.extend(y_hat)</span><br><span class="line">                self.test_scores.append(roc_auc_score(self.test_loader.y, test_y_hat))</span><br><span class="line">            <span class="keyword">if</span> self.early_stop_round <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">                <span class="keyword">if</span> num_epoch &gt; self.early_stop_round:</span><br><span class="line">                    <span class="keyword">if</span> np.argmax(self.test_scores) == num_epoch - self.early_stop_round <span class="keyword">and</span> self.test_scores[<span class="number">-1</span>] - self.test_scores[num_epoch - self.early_stop_round] &lt; <span class="number">1e-3</span>:</span><br><span class="line">                        <span class="keyword">break</span></span><br></pre></td></tr></table></figure>
<blockquote>
<p>FM从神经网络来说就叫做嵌入（embedding）：</p>
<ul>
<li>FM首先是对离散特征进行嵌入。</li>
<li>之后通过对嵌入后的稠密向量进行内积来进行二阶特征组合。</li>
<li>最后再与线性模型的结果求和进而得到预估点击率。</li>
</ul>
</blockquote>
<h2 id="Field-aware-Factorization-Machines-FFM"><a href="#Field-aware-Factorization-Machines-FFM" class="headerlink" title="Field-aware Factorization Machines (FFM)"></a>Field-aware Factorization Machines (FFM)</h2><p>FFM加上了域的概念，$$v$$由二维参数变成了三维，因此时间复杂度变为$$O(n^2)$$。当数据的维度比较大时，FFM将会极其复杂。</p>
<p><img src="ffm_v.png" alt="FFM参数"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pickle <span class="keyword">as</span> pkl</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> scipy.sparse <span class="keyword">import</span> coo_matrix</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> roc_auc_score</span><br><span class="line"><span class="keyword">from</span> tqdm <span class="keyword">import</span> tqdm</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">FFM</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, train_loader, test_loader, learning_rate, epoch, num_field, </span></span></span><br><span class="line"><span class="function"><span class="params">                 feature2field, early_stop_round=None, dtype=tf.float32, random_seed=None, </span></span></span><br><span class="line"><span class="function"><span class="params">                 factor_dim=<span class="number">3</span>)</span>:</span></span><br><span class="line">        self.graph = tf.Graph()</span><br><span class="line"></span><br><span class="line">        self.train_loader = train_loader</span><br><span class="line">        self.batch_size = train_loader.batch_size</span><br><span class="line">        self.input_dim = train_loader.input_dim</span><br><span class="line">        self.test_loader = test_loader</span><br><span class="line">        self.learning_rate = learning_rate</span><br><span class="line">        self.epoch = epoch</span><br><span class="line">        self.num_field = num_field</span><br><span class="line">        self.feature2field = feature2field</span><br><span class="line"></span><br><span class="line">        self.early_stop_round = early_stop_round</span><br><span class="line">        self.dtype = dtype</span><br><span class="line">        self.random_seed = random_seed</span><br><span class="line">        self.factor_dim = factor_dim</span><br><span class="line"></span><br><span class="line">        self.l2_w = <span class="number">0</span></span><br><span class="line">        self.l2_v = <span class="number">0</span></span><br><span class="line">        self.train_scores = []</span><br><span class="line">        self.test_scores = []</span><br><span class="line"></span><br><span class="line">        <span class="keyword">with</span> self.graph.as_default():</span><br><span class="line">            <span class="keyword">if</span> self.random_seed <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">                tf.set_random_seed(self.random_seed)</span><br><span class="line">            self.X = tf.placeholder(self.dtype, [self.batch_size, self.input_dim])</span><br><span class="line">            self.y = tf.placeholder(self.dtype, [<span class="literal">None</span>, <span class="number">1</span>])</span><br><span class="line"></span><br><span class="line">            <span class="keyword">with</span> tf.variable_scope(<span class="string">'linear_layer'</span>):</span><br><span class="line">                maxval = np.sqrt(<span class="number">6.</span> / np.sum(self.train_loader.input_dim + self.train_loader.output_dim))</span><br><span class="line">                w = tf.Variable(</span><br><span class="line">                        tf.random_uniform(</span><br><span class="line">                            [self.train_loader.input_dim, self.train_loader.output_dim], </span><br><span class="line">                            minval=-maxval, maxval=maxval, dtype=self.dtype</span><br><span class="line">                            ), name=<span class="string">'w'</span>, dtype=self.dtype</span><br><span class="line">                    )</span><br><span class="line">                b = tf.Variable(</span><br><span class="line">                        tf.zeros(</span><br><span class="line">                            [self.train_loader.output_dim], dtype=self.dtype</span><br><span class="line">                            ), name=<span class="string">'b'</span>, dtype=self.dtype</span><br><span class="line">                    )</span><br><span class="line">                linear_terms = tf.matmul(self.X, w) + b</span><br><span class="line"></span><br><span class="line">            <span class="keyword">with</span> tf.variable_scope(<span class="string">'nolinear_layer'</span>):</span><br><span class="line">                maxval = np.sqrt(<span class="number">6.</span> / np.sum(self.train_loader.input_dim + self.num_field + self.factor_dim))</span><br><span class="line">                v = tf.Variable(</span><br><span class="line">                        tf.truncated_normal(</span><br><span class="line">                            [self.train_loader.input_dim, self.num_field, self.factor_dim],</span><br><span class="line">                            mean=<span class="number">0.0</span>, stddev=<span class="number">0.01</span>, dtype=self.dtype</span><br><span class="line">                        )</span><br><span class="line">                    )</span><br><span class="line">                nolinear_sum = tf.zeros([self.batch_size, <span class="number">1</span>], dtype=self.dtype)</span><br><span class="line">                <span class="keyword">for</span> i <span class="keyword">in</span> range(self.input_dim):</span><br><span class="line">                    <span class="keyword">for</span> j <span class="keyword">in</span> range(i+<span class="number">1</span>, self.input_dim):</span><br><span class="line">                        vifj = v[i, self.feature2field[j]]</span><br><span class="line">                        vjfi = v[j, self.feature2field[i]]</span><br><span class="line">                        vivj = tf.reduce_sum(tf.multiply(vifj, vjfi))</span><br><span class="line">                        print(self.X[:, i].shape)</span><br><span class="line">                        xixj = tf.multiply(self.X[:, i], self.X[:, j])</span><br><span class="line">                        nolinear_sum += tf.multiply(vivj, xixj)</span><br><span class="line">                nolinear_sum = tf.reshape(nolinear_sum, [self.batch_size, <span class="number">1</span>])</span><br><span class="line">            </span><br><span class="line">            logits = tf.reshape(linear_terms + nolinear_sum, [<span class="number">-1</span>])</span><br><span class="line">            self.y_preds = tf.sigmoid(logits)</span><br><span class="line"></span><br><span class="line">            self.loss = tf.reduce_mean(</span><br><span class="line">                tf.nn.sigmoid_cross_entropy_with_logits(labels=self.y, logits=logits)</span><br><span class="line">            ) + self.l2_w * tf.nn.l2_loss(w) + self.l2_v * tf.nn.l2_loss(v)</span><br><span class="line"></span><br><span class="line">            self.optimizer = tf.train.GradientDescentOptimizer(self.learning_rate).minimize(self.loss)</span><br><span class="line"></span><br><span class="line">            self.sess = tf.Session()</span><br><span class="line">            tf.global_variables_initializer().run(session=self.sess)</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">run</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">for</span> num_epoch <span class="keyword">in</span> tqdm(range(self.epoch)):</span><br><span class="line">            train_iter = self.train_loader.normal_iter()</span><br><span class="line">            feed_dict = &#123;&#125;</span><br><span class="line">            train_y_hat = []</span><br><span class="line">            <span class="keyword">for</span> X, y <span class="keyword">in</span> train_iter:</span><br><span class="line">                X = X[:,:<span class="number">100</span>]</span><br><span class="line">                l = self.sess.run([self.optimizer, self.loss], feed_dict=&#123;self.X: X, self.y: y&#125;)</span><br><span class="line">                y_hat = self.sess.run(self.y_preds, feed_dict=&#123;self.X: X, self.y: <span class="literal">None</span>&#125;)</span><br><span class="line">                train_y_hat.extend(y_hat)</span><br><span class="line">            self.train_scores.append(roc_auc_score(self.train_loader.y, train_y_hat))</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> self.test_loader <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">                test_iter = self.test_loader.generator()</span><br><span class="line">                feed_dict = &#123;&#125;</span><br><span class="line">                test_y_hat = []</span><br><span class="line">                <span class="keyword">for</span> X, y <span class="keyword">in</span> test_iter:</span><br><span class="line">                    y_hat = self.sess.run(self.y_preds, feed_dict=&#123;self.X: X, self.y: <span class="literal">None</span>&#125;)</span><br><span class="line">                    test_y_hat.extend(y_hat)</span><br><span class="line">                self.test_scores.append(roc_auc_score(self.test_loader.y, test_y_hat))</span><br><span class="line">            <span class="keyword">if</span> self.early_stop_round <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">                <span class="keyword">if</span> num_epoch &gt; self.early_stop_round:</span><br><span class="line">                    <span class="keyword">if</span> np.argmax(self.test_scores) == num_epoch - self.early_stop_round <span class="keyword">and</span> self.test_scores[<span class="number">-1</span>] - self.test_scores[num_epoch - self.early_stop_round] &lt; <span class="number">1e-3</span>:</span><br><span class="line">                        <span class="keyword">break</span></span><br></pre></td></tr></table></figure>
</div><div class="tags"><a href="/tags/CTR/">CTR</a><a href="/tags/LR/">LR</a><a href="/tags/FM-FFM/">FM/FFM</a></div><div class="post-nav"><a class="pre" href="/2019-3-8-sohu-interview/">搜狐推荐算法实习面试经历（Offer）</a><a class="next" href="/2019-2-21-ms-bing-interview/">微软Bing组算法（Game Over）</a></div></div></div></div><div class="pure-u-1-4 hidden_mid_and_down"><div id="sidebar"><div class="widget"><form class="search-form" action="//www.google.com/search" method="get" accept-charset="utf-8" target="_blank"><input type="text" name="q" maxlength="20" placeholder="Search"><input type="hidden" name="sitesearch" value="https://chaideblog.github.io"></form></div><div class="widget"><div class="widget-title"><i class="fa fa-folder-o"> 分类</i></div><ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/Algorithm/">Algorithm</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/BigData/">BigData</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Deep-Learning/">Deep Learning</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Interview/">Interview</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Java/">Java</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Linux/">Linux</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Machine-Learning/">Machine Learning</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Math/">Math</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Python/">Python</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-star-o"> 标签</i></div><div class="tagcloud"><a href="/tags/Probability-theroy/" style="font-size: 15px;">Probability theroy</a> <a href="/tags/Genetic-Algorithm/" style="font-size: 15px;">Genetic Algorithm</a> <a href="/tags/决策树/" style="font-size: 15px;">决策树</a> <a href="/tags/Python语法/" style="font-size: 15px;">Python语法</a> <a href="/tags/VPS/" style="font-size: 15px;">VPS</a> <a href="/tags/Convex-Optimize/" style="font-size: 15px;">Convex Optimize</a> <a href="/tags/Naive-Bayes/" style="font-size: 15px;">Naive Bayes</a> <a href="/tags/Sublime/" style="font-size: 15px;">Sublime</a> <a href="/tags/Random-Forests/" style="font-size: 15px;">Random Forests</a> <a href="/tags/集成学习/" style="font-size: 15px;">集成学习</a> <a href="/tags/Hadoop/" style="font-size: 15px;">Hadoop</a> <a href="/tags/Java/" style="font-size: 15px;">Java</a> <a href="/tags/FFT/" style="font-size: 15px;">FFT</a> <a href="/tags/Apache/" style="font-size: 15px;">Apache</a> <a href="/tags/树莓派/" style="font-size: 15px;">树莓派</a> <a href="/tags/SVM/" style="font-size: 15px;">SVM</a> <a href="/tags/SGD/" style="font-size: 15px;">SGD</a> <a href="/tags/BGD/" style="font-size: 15px;">BGD</a> <a href="/tags/MBGD/" style="font-size: 15px;">MBGD</a> <a href="/tags/SMO/" style="font-size: 15px;">SMO</a> <a href="/tags/标准化/" style="font-size: 15px;">标准化</a> <a href="/tags/回归/" style="font-size: 15px;">回归</a> <a href="/tags/CTR/" style="font-size: 15px;">CTR</a> <a href="/tags/LeetCode/" style="font-size: 15px;">LeetCode</a> <a href="/tags/LightGBM/" style="font-size: 15px;">LightGBM</a> <a href="/tags/RCNN/" style="font-size: 15px;">RCNN</a> <a href="/tags/目标检测/" style="font-size: 15px;">目标检测</a> <a href="/tags/Interview/" style="font-size: 15px;">Interview</a> <a href="/tags/LR/" style="font-size: 15px;">LR</a> <a href="/tags/FM-FFM/" style="font-size: 15px;">FM/FFM</a> <a href="/tags/Recommend-System/" style="font-size: 15px;">Recommend System</a> <a href="/tags/Natural-Language-Processing/" style="font-size: 15px;">Natural Language Processing</a> <a href="/tags/Recommendation/" style="font-size: 15px;">Recommendation</a></div></div><div class="widget"><div class="widget-title"><i class="fa fa-file-o"> 最近文章</i></div><ul class="post-list"><li class="post-list-item"><a class="post-list-link" href="/2019-5-5-Paper-TEM/">基于决策树的可解释嵌入推荐模型：TEM</a></li><li class="post-list-item"><a class="post-list-link" href="/2019-5-4-Transformer/">Transformer理解与Tensorflow代码阅读</a></li><li class="post-list-item"><a class="post-list-link" href="/2019-4-28-Normalization/">深度学习中各种Normalization</a></li><li class="post-list-item"><a class="post-list-link" href="/2019-4-22-recommend-system-1/">《推荐系统实战》读书笔记（一）</a></li><li class="post-list-item"><a class="post-list-link" href="/2019-4-6-toutiao-interview/">字节跳动广告算法团队实习面试经历（Offer）</a></li><li class="post-list-item"><a class="post-list-link" href="/2019-3-13-xiaomi-interview/">小米信息流推荐实习面试经历（Offer）</a></li><li class="post-list-item"><a class="post-list-link" href="/2019-3-8-sohu-interview/">搜狐推荐算法实习面试经历（Offer）</a></li><li class="post-list-item"><a class="post-list-link" href="/2019-2-27-CTR-LR-FM-FFM/">CTR系列（一）：LR、FM/FFM</a></li><li class="post-list-item"><a class="post-list-link" href="/2019-2-21-ms-bing-interview/">微软Bing组算法（Game Over）</a></li><li class="post-list-item"><a class="post-list-link" href="/2019-1-8-PriorityQueue/">【转】Java容器源码分析之PriorityQueue</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-external-link"> 友情链接</i></div><ul></ul><a href="http://www.example1.com/" title="site-name1" target="_blank">site-name1</a><ul></ul><a href="http://www.example2.com/" title="site-name2" target="_blank">site-name2</a><ul></ul><a href="http://www.example3.com/" title="site-name3" target="_blank">site-name3</a></div></div></div><div class="pure-u-1 pure-u-md-3-4"><div id="footer">Copyright © 2019 <a href="/." rel="nofollow">CHAI' blog.</a> Powered by<a rel="nofollow" target="_blank" href="https://hexo.io"> Hexo.</a><a rel="nofollow" target="_blank" href="https://github.com/tufu9441/maupassant-hexo"> Theme</a> by<a rel="nofollow" target="_blank" href="https://github.com/pagecho"> Cho.</a></div></div></div><a class="show" id="rocket" href="#top"></a><script type="text/javascript" src="/js/totop.js?v=0.0.0" async></script><script type="text/javascript" src="//lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.js" async></script><script type="text/javascript" src="/js/fancybox.js?v=0.0.0" async></script><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.css"><script type="text/x-mathjax-config">MathJax.Hub.Config({
  tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}
  });
</script><script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML" async></script><script type="text/javascript" src="/js/codeblock-resizer.js?v=0.0.0"></script><script type="text/javascript" src="/js/smartresize.js?v=0.0.0"></script></div></body></html>