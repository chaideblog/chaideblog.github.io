<!DOCTYPE html><html lang="zh-CN"><head><meta name="generator" content="Hexo 3.8.0"><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description"><title>梯度下降法（BGD、SGD、MBGD）以及Python实现 | CHAI' blog</title><link rel="stylesheet" type="text/css" href="/css/style.css?v=0.0.0"><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/normalize/8.0.1/normalize.min.css"><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/pure/1.0.0/pure-min.css"><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/pure/1.0.0/grids-responsive-min.css"><link rel="stylesheet" href="//lib.baomitu.com/font-awesome/4.7.0/css/font-awesome.min.css"><script type="text/javascript" src="//lib.baomitu.com/jquery/3.4.0/jquery.min.js"></script><link rel="icon" mask sizes="any" href="/favicon.ico"><link rel="Shortcut Icon" type="image/x-icon" href="/favicon.ico"><link rel="apple-touch-icon" href="/apple-touch-icon.png"><link rel="apple-touch-icon-precomposed" href="/apple-touch-icon.png"></head><body><div class="body_container"><div id="header"><div class="site-name"><h1 class="hidden">梯度下降法（BGD、SGD、MBGD）以及Python实现</h1><a id="logo" href="/.">CHAI' blog</a><p class="description"></p></div><div id="nav-menu"><a class="current" href="/."><i class="fa fa-home"> 首页</i></a><a href="/archives/"><i class="fa fa-archive"> 归档</i></a><a href="/about/"><i class="fa fa-user"> 关于</i></a><a href="/atom.xml"><i class="fa fa-rss"> 订阅</i></a></div></div><div class="pure-g" id="layout"><div class="pure-u-1 pure-u-md-3-4"><div class="content_container"><div class="post"><h1 class="post-title">梯度下降法（BGD、SGD、MBGD）以及Python实现</h1><div class="post-meta">Sep 24, 2017<span> | </span><span class="category"><a href="/categories/Machine-Learning/">Machine Learning</a></span></div><div class="clear"><div class="toc-article" id="toc"><div class="toc-title">文章目录</div><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#批量梯度下降法-Batch-Gradient-Descent"><span class="toc-number">1.</span> <span class="toc-text">批量梯度下降法(Batch Gradient Descent)</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#随机梯度下降法-Stochastic-Gradient-Descent"><span class="toc-number">2.</span> <span class="toc-text">随机梯度下降法(Stochastic Gradient Descent)</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#小批量梯度下降法-Mini-batch-Gradient-Descent"><span class="toc-number">3.</span> <span class="toc-text">小批量梯度下降法(Mini-batch Gradient Descent)</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#梯度下降法的一些问题"><span class="toc-number">4.</span> <span class="toc-text">梯度下降法的一些问题</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#参考文献"><span class="toc-number">5.</span> <span class="toc-text">参考文献</span></a></li></ol></div></div><div class="post-content"><p>我们以线性回归算法来对三种梯度下降法进行比较。线性回归函数为：</p>
<p>$$h_\theta = \sum_{j=0}^{n} \theta_{j} x_{j} \tag{1}$$</p>
<p>注意：“线性”表示的是对于参数$\theta$来说是线性的。</p>
<p>相应的惩罚函数为：</p>
<p>$$J(\theta) = \frac{1}{2m} \sum_{i-1}^{m} (h_{\theta} (x^{(i)}) - y^{(i)})^2 \tag{2}$$</p>
<h2 id="批量梯度下降法-Batch-Gradient-Descent"><a href="#批量梯度下降法-Batch-Gradient-Descent" class="headerlink" title="批量梯度下降法(Batch Gradient Descent)"></a>批量梯度下降法(Batch Gradient Descent)</h2><p>对公式(2)求偏导，得到</p>
<p>$$\frac{\partial J(\theta)}{\partial \theta_j} = - \frac{1}{m} \sum_{i=1}^{m} (y^{(i)} - h_{\theta}(x^{(i)}))x_j^{(i)} \tag{3}$$</p>
<p>按照每个参数$\theta$的梯度负方向来更新每个$\theta$：</p>
<p>$$\theta_j^{‘} = \theta_j + \frac{1}{m} \sum_{i=1}^{m} (y^{(i)} - h_{\theta}(x^{(i)}))x_j^{(i)} \tag{4}$$</p>
<p>每次参数$\theta$的更新，都会使用所有的训练集。当m很大时，批量梯度下降法训练的速度比较慢。当惩罚函数$J(\theta)$是一凸函数时，则批量梯度下降法必然会在全局最小值处收敛；否则，目标函数则可能会局部极小值处收敛。</p>
<p>同时，梯度下降法不能以“在线”的形式更新我们的模型，也就是不能再运行中加入新的样本进行运算。</p>
<p><strong>优点：</strong>全局最优解；易于并行实现；</p>
<p><strong>缺点：</strong>当样本数目很多时，训练过程会很慢。</p>
<h2 id="随机梯度下降法-Stochastic-Gradient-Descent"><a href="#随机梯度下降法-Stochastic-Gradient-Descent" class="headerlink" title="随机梯度下降法(Stochastic Gradient Descent)"></a>随机梯度下降法(Stochastic Gradient Descent)</h2><p>因为批量梯度下降法在每次更新前，会对相似的样本求算梯度值，因而它在较大的数据集上的计算会有些冗余。于是，提出了随机梯度下降法来克服这一问题，同时能够“在线”学习。我们将损失函数写成如下形式：</p>
<p>$$J(\theta) = \frac{1}{m} \sum_{i=1}^{m} \frac{1}{2} (y^{(i)} - h_{\theta} (x^{(i)}))^2 \tag{5}$$</p>
<p>随机梯度下降是通过每个样本更新一次，来更新$\theta$：</p>
<p>$$\theta_{j}^{‘} = \theta_{j} + (y^{(i)} - h_{\theta} (x^{(i)})) x_j^{(i)} \tag{6}$$</p>
<p>SGD的问题是训练中噪声很大，SGD并不是一直向最优化方向。随机梯度下降法更新值的方差很大，在频繁的更新之下，它的目标函数有着如下图所示的剧烈波动。</p>
<p><img src="SGD函数波动.png" alt="SGD函数波动"></p>
<p>当惩罚函数不是凸函数时，BGD会使函数落入局部最小值。但是，SGD的收敛过程中的波动，可能是惩罚函数跳入另一个更小的极小值。同样的，因为持续的波动，SGD也难以收敛于特定最小值。但是，当我们慢慢降低学习率的时候，SGD 表现出了与批量梯度下降法相似的收敛过程，也就是说，对非凸函数和凸函数，必然会分别收敛到它们的极小值和最小值。<strong>注意：</strong>在迭代前，需要打乱训练数据集。</p>
<p><strong>优点：</strong>训练速度快；</p>
<p><strong>缺点：</strong>准确度下降；不易于并行实现。</p>
<h2 id="小批量梯度下降法-Mini-batch-Gradient-Descent"><a href="#小批量梯度下降法-Mini-batch-Gradient-Descent" class="headerlink" title="小批量梯度下降法(Mini-batch Gradient Descent)"></a>小批量梯度下降法(Mini-batch Gradient Descent)</h2><p>小批量梯度下降法是上面两种方法的综合，MBGD在每次迭代中使用b个样本（b一般为10）。所以，更新$\theta$的方程为</p>
<p>$$\theta_j^{‘} = \theta_j + \frac{1}{10} \sum_{k=i}^{i+9} (y^{(k)} - h_{\theta}(x^{(k)}))x_j^{(k)} (i = 10p+1; p=1,2,…,m/10-1) \tag{7}$$</p>
<h2 id="梯度下降法的一些问题"><a href="#梯度下降法的一些问题" class="headerlink" title="梯度下降法的一些问题"></a>梯度下降法的一些问题</h2><ul>
<li><p>如何选择合适的学习速率。学习速率太小，函数收敛速度慢；学习速率太大，函数难以收敛，惩罚函数在最小值处震荡。</p>
</li>
<li><p>可以在学习过程中调整学习速率，在学习过程不断减小学习速率。但是，如何调整学习速率？</p>
</li>
</ul>
<blockquote>
<p>在对神经网络最优化非凸的罚函数时，另一个通常面临的挑战，是如何避免目标函数被困在无数的局部最小值中，以导致的未完全优化的情况。Dauphin 及其他人 [19] 认为，这个困难并不来自于局部最小值，而是来自于「鞍点」，也就是在一个方向上斜率是正的、在一个方向上斜率是负的点。这些鞍点通常由一些函数值相同的面环绕，它们在各个方向的梯度值都为 0，所以 SGD 很难从这些鞍点中脱开。</p>
</blockquote>
<h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><ol>
<li><a href="https://www.cnblogs.com/maybe2030/p/5089753.html" target="_blank" rel="noopener">梯度下降法的三种形式BGD、SGD以及MBGD</a></li>
<li><a href="https://www.jiqizhixin.com/articles/2016-11-21-4" target="_blank" rel="noopener">深度解读最流行的优化算法：梯度下降</a></li>
</ol>
</div><div class="tags"><a href="/tags/SGD/">SGD</a><a href="/tags/BGD/">BGD</a><a href="/tags/MBGD/">MBGD</a></div><div class="post-nav"><a class="pre" href="/2017-9-24-Math-Var-Biased/">为什么方差是有偏估计</a><a class="next" href="/2017-9-23-ML-SVM/">支持向量机SVM理论——基本数学原理</a></div></div></div></div><div class="pure-u-1-4 hidden_mid_and_down"><div id="sidebar"><div class="widget"><form class="search-form" action="//www.google.com/search" method="get" accept-charset="utf-8" target="_blank"><input type="text" name="q" maxlength="20" placeholder="Search"><input type="hidden" name="sitesearch" value="https://chaideblog.github.io"></form></div><div class="widget"><div class="widget-title"><i class="fa fa-folder-o"> 分类</i></div><ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/Algorithm/">Algorithm</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/BigData/">BigData</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Deep-Learning/">Deep Learning</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Interview/">Interview</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Java/">Java</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Linux/">Linux</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Machine-Learning/">Machine Learning</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Math/">Math</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Python/">Python</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-star-o"> 标签</i></div><div class="tagcloud"><a href="/tags/Probability-theroy/" style="font-size: 15px;">Probability theroy</a> <a href="/tags/Genetic-Algorithm/" style="font-size: 15px;">Genetic Algorithm</a> <a href="/tags/决策树/" style="font-size: 15px;">决策树</a> <a href="/tags/Python语法/" style="font-size: 15px;">Python语法</a> <a href="/tags/VPS/" style="font-size: 15px;">VPS</a> <a href="/tags/Convex-Optimize/" style="font-size: 15px;">Convex Optimize</a> <a href="/tags/Naive-Bayes/" style="font-size: 15px;">Naive Bayes</a> <a href="/tags/Sublime/" style="font-size: 15px;">Sublime</a> <a href="/tags/Random-Forests/" style="font-size: 15px;">Random Forests</a> <a href="/tags/集成学习/" style="font-size: 15px;">集成学习</a> <a href="/tags/Hadoop/" style="font-size: 15px;">Hadoop</a> <a href="/tags/Java/" style="font-size: 15px;">Java</a> <a href="/tags/FFT/" style="font-size: 15px;">FFT</a> <a href="/tags/Apache/" style="font-size: 15px;">Apache</a> <a href="/tags/树莓派/" style="font-size: 15px;">树莓派</a> <a href="/tags/SVM/" style="font-size: 15px;">SVM</a> <a href="/tags/SGD/" style="font-size: 15px;">SGD</a> <a href="/tags/BGD/" style="font-size: 15px;">BGD</a> <a href="/tags/MBGD/" style="font-size: 15px;">MBGD</a> <a href="/tags/SMO/" style="font-size: 15px;">SMO</a> <a href="/tags/标准化/" style="font-size: 15px;">标准化</a> <a href="/tags/回归/" style="font-size: 15px;">回归</a> <a href="/tags/CTR/" style="font-size: 15px;">CTR</a> <a href="/tags/LeetCode/" style="font-size: 15px;">LeetCode</a> <a href="/tags/LightGBM/" style="font-size: 15px;">LightGBM</a> <a href="/tags/RCNN/" style="font-size: 15px;">RCNN</a> <a href="/tags/目标检测/" style="font-size: 15px;">目标检测</a> <a href="/tags/Interview/" style="font-size: 15px;">Interview</a> <a href="/tags/LR/" style="font-size: 15px;">LR</a> <a href="/tags/FM-FFM/" style="font-size: 15px;">FM/FFM</a> <a href="/tags/Recommend-System/" style="font-size: 15px;">Recommend System</a> <a href="/tags/Natural-Language-Processing/" style="font-size: 15px;">Natural Language Processing</a> <a href="/tags/Recommendation/" style="font-size: 15px;">Recommendation</a></div></div><div class="widget"><div class="widget-title"><i class="fa fa-file-o"> 最近文章</i></div><ul class="post-list"><li class="post-list-item"><a class="post-list-link" href="/2019-5-5-Paper-TEM/">基于决策树的可解释嵌入推荐模型：TEM</a></li><li class="post-list-item"><a class="post-list-link" href="/2019-5-4-Transformer/">Transformer理解与Tensorflow代码阅读</a></li><li class="post-list-item"><a class="post-list-link" href="/2019-4-28-Normalization/">深度学习中各种Normalization</a></li><li class="post-list-item"><a class="post-list-link" href="/2019-4-22-recommend-system-1/">《推荐系统实战》读书笔记（一）</a></li><li class="post-list-item"><a class="post-list-link" href="/2019-4-6-toutiao-interview/">字节跳动广告算法团队实习面试经历（Offer）</a></li><li class="post-list-item"><a class="post-list-link" href="/2019-3-13-xiaomi-interview/">小米信息流推荐实习面试经历（Offer）</a></li><li class="post-list-item"><a class="post-list-link" href="/2019-3-8-sohu-interview/">搜狐推荐算法实习面试经历（Offer）</a></li><li class="post-list-item"><a class="post-list-link" href="/2019-2-27-CTR-LR-FM-FFM/">CTR系列（一）：LR、FM/FFM</a></li><li class="post-list-item"><a class="post-list-link" href="/2019-2-21-ms-bing-interview/">微软Bing组算法（Game Over）</a></li><li class="post-list-item"><a class="post-list-link" href="/2019-1-8-PriorityQueue/">【转】Java容器源码分析之PriorityQueue</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-external-link"> 友情链接</i></div><ul></ul><a href="http://www.example1.com/" title="site-name1" target="_blank">site-name1</a><ul></ul><a href="http://www.example2.com/" title="site-name2" target="_blank">site-name2</a><ul></ul><a href="http://www.example3.com/" title="site-name3" target="_blank">site-name3</a></div></div></div><div class="pure-u-1 pure-u-md-3-4"><div id="footer">Copyright © 2019 <a href="/." rel="nofollow">CHAI' blog.</a> Powered by<a rel="nofollow" target="_blank" href="https://hexo.io"> Hexo.</a><a rel="nofollow" target="_blank" href="https://github.com/tufu9441/maupassant-hexo"> Theme</a> by<a rel="nofollow" target="_blank" href="https://github.com/pagecho"> Cho.</a></div></div></div><a class="show" id="rocket" href="#top"></a><script type="text/javascript" src="/js/totop.js?v=0.0.0" async></script><script type="text/javascript" src="//lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.js" async></script><script type="text/javascript" src="/js/fancybox.js?v=0.0.0" async></script><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.css"><script type="text/x-mathjax-config">MathJax.Hub.Config({
  tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}
  });
</script><script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML" async></script><script type="text/javascript" src="/js/codeblock-resizer.js?v=0.0.0"></script><script type="text/javascript" src="/js/smartresize.js?v=0.0.0"></script></div></body></html>