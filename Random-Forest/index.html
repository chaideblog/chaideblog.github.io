<!DOCTYPE html><html lang="zh-CN"><head><meta name="generator" content="Hexo 3.8.0"><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description"><title>随机森林 | CHAI' blog</title><link rel="stylesheet" type="text/css" href="/css/style.css?v=0.0.0"><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/normalize/8.0.1/normalize.min.css"><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/pure/1.0.0/pure-min.css"><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/pure/1.0.0/grids-responsive-min.css"><link rel="stylesheet" href="//lib.baomitu.com/font-awesome/4.7.0/css/font-awesome.min.css"><script type="text/javascript" src="//lib.baomitu.com/jquery/3.4.0/jquery.min.js"></script><link rel="icon" mask sizes="any" href="/favicon.ico"><link rel="Shortcut Icon" type="image/x-icon" href="/favicon.ico"><link rel="apple-touch-icon" href="/apple-touch-icon.png"><link rel="apple-touch-icon-precomposed" href="/apple-touch-icon.png"></head><body><div class="body_container"><div id="header"><div class="site-name"><h1 class="hidden">随机森林</h1><a id="logo" href="/.">CHAI' blog</a><p class="description"></p></div><div id="nav-menu"><a class="current" href="/."><i class="fa fa-home"> 首页</i></a><a href="/archives/"><i class="fa fa-archive"> 归档</i></a><a href="/about/"><i class="fa fa-user"> 关于</i></a><a href="/atom.xml"><i class="fa fa-rss"> 订阅</i></a></div></div><div class="pure-g" id="layout"><div class="pure-u-1 pure-u-md-3-4"><div class="content_container"><div class="post"><h1 class="post-title">随机森林</h1><div class="post-meta">Nov 30, 2017<span> | </span><span class="category"><a href="/categories/Machine-Learning/">Machine Learning</a></span></div><div class="clear"><div class="toc-article" id="toc"><div class="toc-title">文章目录</div><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#集成学习"><span class="toc-number">1.</span> <span class="toc-text">集成学习</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-Boosting"><span class="toc-number">1.1.</span> <span class="toc-text">1. Boosting</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-Bagging"><span class="toc-number">1.2.</span> <span class="toc-text">2. Bagging</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-Random-Forest"><span class="toc-number">1.3.</span> <span class="toc-text">3. Random Forest</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#随机森林的数学原理"><span class="toc-number">2.</span> <span class="toc-text">随机森林的数学原理</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#随机森林的收敛性"><span class="toc-number">2.1.</span> <span class="toc-text">随机森林的收敛性</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#分类效能和相关度"><span class="toc-number">2.2.</span> <span class="toc-text">分类效能和相关度</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#随机森林算法"><span class="toc-number">3.</span> <span class="toc-text">随机森林算法</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#加入随机性-训练集的子空间-有放回采样"><span class="toc-number">3.1.</span> <span class="toc-text">加入随机性: 训练集的子空间(有放回采样)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#加入随机性-采样特征子空间-无放回采样"><span class="toc-number">3.2.</span> <span class="toc-text">加入随机性: 采样特征子空间(无放回采样)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#加入随机性-加入新特征-合并-低维的投影"><span class="toc-number">3.3.</span> <span class="toc-text">加入随机性: 加入新特征(合并, 低维的投影)</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Python实现随机森林"><span class="toc-number">4.</span> <span class="toc-text">Python实现随机森林</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#sklearn随机森林"><span class="toc-number">5.</span> <span class="toc-text">sklearn随机森林</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#sklearn随机森林参数调优"><span class="toc-number">5.1.</span> <span class="toc-text">sklearn随机森林参数调优</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#参考文献"><span class="toc-number">6.</span> <span class="toc-text">参考文献</span></a></li></ol></div></div><div class="post-content"><h2 id="集成学习"><a href="#集成学习" class="headerlink" title="集成学习"></a>集成学习</h2><h3 id="1-Boosting"><a href="#1-Boosting" class="headerlink" title="1. Boosting"></a>1. Boosting</h3><p>提升方法（Boosting），是一种可以用来减小监督式学习中偏差的机器学习元算法。一个经典的提升算法例子是AdaBoost。一些最近的例子包括LPBoost、TotalBoost、BrownBoost、MadaBoost及LogitBoost。许多提升方法可以在AnyBoost框架下解释为在函数空间利用一个凸的误差函数作梯度下降。</p>
<p>Adaboost是一种迭代算法，其核心思想是针对同一个训练集训练不同的分类器（弱分类器），然后把这些弱分类器集合起来，构成一个更强的最终分类器（强分类器）。Adaboost算法本身是通过改变数据分布来实现的，它根据每次训练集之中每个样本的分类是否正确，以及上次的总体分类的准确率，来确定每个样本的权值。将修改过权值的新数据集送给下层分类器进行训练，最后将每次得到的分类器最后融合起来，作为最后的决策分类器。</p>
<h3 id="2-Bagging"><a href="#2-Bagging" class="headerlink" title="2. Bagging"></a>2. Bagging</h3><p>Bagging算法 （英语：Bootstrap aggregating，引导聚集算法），又称装袋算法，是机器学习领域的一种团体学习算法。最初由Leo Breiman于1994年提出。Bagging算法可与其他分类、回归算法结合，提高其准确率、稳定性的同时，通过降低结果的方差，避免过拟合的发生。</p>
<p>给定一个大小为$n$的训练集$D$，Bagging算法从中均匀、有放回地（即使用自助抽样法）选出$m$个大小为$n^{‘}$的子集$D_{i}$，作为新的训练集。在这$m$个训练集上使用分类、回归等算法，则可得到$m$个模型，再通过取平均值、取多数票等方法，即可得到Bagging的结果。</p>
<h3 id="3-Random-Forest"><a href="#3-Random-Forest" class="headerlink" title="3. Random Forest"></a>3. Random Forest</h3><p>随机森林算法不同于Boosting和Bagging算法，但是随机森林算法和Bagging算法比较相似。随机森林使用了Bagging算法中的随机选择训练集的子集，但是随机森林还随机选取特征空间。因此，随机森林算法降低了偏差，但是能够进一步降低方差。</p>
<h2 id="随机森林的数学原理"><a href="#随机森林的数学原理" class="headerlink" title="随机森林的数学原理"></a>随机森林的数学原理</h2><p>作为衡量一个分类器性能好坏的重要指标，泛化能力指的是一个分类器正确分类训练集以外的样本的能力。泛化误差指的是分类器对训练集之外的样本的判别错误的比例。通过定义随机森林分类间隔函数(margin function)利用大数定律作为理论基础证明得到以下的结论：随着随机森林中决策树的数目的增加，随机森林的泛化误差会趋向于一个有限上界。</p>
<h3 id="随机森林的收敛性"><a href="#随机森林的收敛性" class="headerlink" title="随机森林的收敛性"></a>随机森林的收敛性</h3><p>设样本集为$T \lbrace X, Y \rbrace$，$X$为样本的特征属性，$Y$为其对应的类别属性。对于一组给定的分类器集合$\lbrace h_1(x), h_2(x), ……, h_i(x) \rbrace$，样本点$(x, y)$的间隔函数(margin function)的定义为：</p>
<p>$$mg(x, y) = av_{k} I(h_{k}(x) = y) - \max_{j \neq y} av_{k} I(h_{k}(x) = j) \tag{1}$$</p>
<p>其中，$I(\bullet)$为指示函数，$av_{k}(\bullet)$是取平均值。$mg(x, y)$衡量了分类器集合将样本分类正确的平均票数与将其错分为其他类的平均票数之差。$mg(x, y) &gt; 0$表明这个样本被该组合分类器正确分类，否则表示被错误分类。$mg(x, y)$的数值越大，表明分类器集合对这个样本的分类性能越好，置信度越高。</p>
<p>分类器的泛化误差可表示为：</p>
<p>$$PE^{\ast} = P_{X, Y}(mg(x, y) &lt; 0) \tag{2}$$</p>
<p>在随机森林中，$h_{k}(x) = h(x, \theta_k)$，当森林中树比较多时，$(2)$式遵循强大数定律。</p>
<p><strong>定理 1：</strong>随着森林中树的数目的增加，在所有序列$\theta_i$上，$PE^{\ast}$几乎处处收敛于：</p>
<p>$$P_{X,Y}(P_{\theta}(h(X, \theta) = Y) - \max_{j \neq Y} P_{\theta}(h(X, \theta) = j)  &lt; 0) \tag{3}$$</p>
<p>其中，$\theta$是对应单棵决策树的随机向量，$h(X, \theta)$为基于$X$和$\theta$的分类器的输出。该定理阐述了随机森林在随着树的数目增加时得到一个有限的泛化误差值而不会过拟合的原因。</p>
<h3 id="分类效能和相关度"><a href="#分类效能和相关度" class="headerlink" title="分类效能和相关度"></a>分类效能和相关度</h3><p>随机森林的泛化误差主要取决于以下两个因素：</p>
<ol>
<li>随机森林中单棵树的分类强度(strength)。单棵树的分类强度越大，则随机森林的泛化能力越好。</li>
<li>随机森林中树与树之间的关联程度(correlation)。树与树之间的关联度越小，则随机森林的泛化能力越好。</li>
</ol>
<p><strong>定义 1：</strong>随机森林对于样本$(x, y)$的间隔函数为：</p>
<p>$$mr(x, y) = P_{\theta} (h(x, \theta) = y) - \max_{j \neq y} P_{\theta} (h(x, \theta) = j) \tag{4}$$</p>
<p>分类器集合$\lbrace h(x, \theta) \rbrace$的分类效能为：</p>
<p>$$s = E_{X, Y} mr(x, y) \tag{5}$$</p>
<p>假设$s \ge 0$，由切比雪夫不等式可知：</p>
<p>$$PE^{\ast} \le \frac{var(mr)}{s^2} \tag{6}$$</p>
<p>其中，$var(mr)$是随机森林的间隔函数$mr(x, y)$的方差。</p>
<p>设$\hat{j} (x, y) = \arg \max_{j \ne y} P_{\theta} (h(x, \theta) = j)$，此时有：</p>
<p>$$\begin{equation} mr(x, y) = P_{\theta} (h(x, \theta) = y) - P_{\theta} (h(x, \theta) = \hat{j} (x, y)) \\ = E_{\theta}[(I(h(x, \theta)) = y) - I_{\theta} (h(x, \theta) = \hat{j} (x, y))] \end{equation}\tag{7}$$</p>
<p><strong>定义 2：</strong>元分类器的间隔函数为：</p>
<p>$$rmg(\theta, x, y) = I(h(x, \theta) = y) - I_{\theta} (h(x, \theta) = \hat{j} (x, y)) \tag{8}$$</p>
<p>所以，$mr(x, y)$是$rmg(\theta, x, y)$在$\theta$上的期望值。对于任意的函数$f$，有恒等式如下：$[E_{\theta}f(\theta)]^2 = E_{\theta, \theta^{‘}} rmg(\theta, x, y)rmg(\theta)$。</p>
<p>由上面的式子可以得到$var(mr)$的表达式如下说示：</p>
<p>$$var(mr) = E_{\theta, \theta^{‘}} (cov_{X, Y}(rmg(\theta, x, y), rmg(\theta, x, y))) \\ = E_{\theta, \theta^{‘}} (\rho(\theta, \theta^{‘}) sd(\theta) sd(\theta^{‘})) \tag{9}$$</p>
<p>其中，$\rho(\theta, \theta^{‘})$是当$\theta, \theta^{‘}$固定时，$rmg(\theta, x, y)$和$rmg(\theta^{‘}, x, y)$的相关度；$sd(\theta)$是当$\theta$固定时，$rmg(\theta, x, y)$的标准差。因此有：$var(mr) = \bar{\rho} (E_{\theta} sd(\theta))^2 \le \bar{\rho} (E_{\theta} var(\theta))$，其中，$\bar{\rho}$是相关度$\rho$的平均值。</p>
<p>$$\bar{\rho} = E_{\theta, \theta^{‘}} (\rho(\theta, \theta^{‘})sd(\theta)sd(\theta^{‘})) / E_{\theta, \theta^{‘}} (sd(\theta)sd(\theta^{‘})) \tag{10}$$</p>
<p>$E_{\theta}var(\theta)$满足以下不等式：</p>
<p>$$E_{\theta}var(\theta) \le E_{\theta} (E_{X,Y} rmg(\theta, x, y))^2 - s^2 \le 1 - s^2 \tag{11}$$</p>
<p>综上所述，可以得到方差$var(mr)$的上界为：$var(mr) \le \bar{\rho} (1-s^2)$，因此可以推出：</p>
<p><strong>定理 2：</strong>随机森林泛化误差为</p>
<p>$$PE^{\ast}​\le \bar{\rho} (1-s^2) / s^2 \tag{12}$$</p>
<p>由上述分析可以得知，应尽量增大单棵树的分类效能（直接影响$rmg(x,y)$的值，从而增加s），并且减小分类树之间的相关度$\rho$。当森林中树的数目达到一定值时，随机森林的泛化误差会逐步收敛于一个有限值。所以，随着森林中树的数目的增多，随机森林不会产生过拟合。</p>
<h2 id="随机森林算法"><a href="#随机森林算法" class="headerlink" title="随机森林算法"></a>随机森林算法</h2><h3 id="加入随机性-训练集的子空间-有放回采样"><a href="#加入随机性-训练集的子空间-有放回采样" class="headerlink" title="加入随机性: 训练集的子空间(有放回采样)"></a>加入随机性: 训练集的子空间(有放回采样)</h3><ul>
<li>Bagging: 不太稳定, 变化较大, g(t) 通过投票/平均, 来 reduce variance</li>
<li>CART: 对不同资料相对敏感, variance large, especially fully-grown tree</li>
</ul>
<p><img src="Random Forest.png" alt="Random Forest"></p>
<h3 id="加入随机性-采样特征子空间-无放回采样"><a href="#加入随机性-采样特征子空间-无放回采样" class="headerlink" title="加入随机性: 采样特征子空间(无放回采样)"></a>加入随机性: 采样特征子空间(无放回采样)</h3><p>从100维选10维, 即做了低维度的投影. P是投影矩阵, 每行row是natural basis代表平常单位的各自的方向。</p>
<p><img src="Diversifying_by_Feature_Projection.png" alt="Diversifying by Feature Projection"></p>
<h3 id="加入随机性-加入新特征-合并-低维的投影"><a href="#加入随机性-加入新特征-合并-低维的投影" class="headerlink" title="加入随机性: 加入新特征(合并, 低维的投影)"></a>加入随机性: 加入新特征(合并, 低维的投影)</h3><p><img src="Diversifying_by_Feature_Expansion.png" alt="Diversifying by Feature Expansion"></p>
<h2 id="Python实现随机森林"><a href="#Python实现随机森林" class="headerlink" title="Python实现随机森林"></a>Python实现随机森林</h2><h2 id="sklearn随机森林"><a href="#sklearn随机森林" class="headerlink" title="sklearn随机森林"></a>sklearn随机森林</h2><p><a href="http://sklearn.apachecn.org/cn/0.19.0/modules/classes.html#module-sklearn.ensemble" target="_blank" rel="noopener"><code>sklearn.ensemble</code></a> 模块包含两个基于 <a href="http://sklearn.apachecn.org/cn/0.19.0/modules/tree.html#tree" target="_blank" rel="noopener">随机决策树</a> 的平均算法： RandomForest 算法和 Extra-Trees 算法。 这两种算法都是专门为树而设计的扰动和组合技术(perturb-and-combine techniques)。 这意味着通过在分类器构造过程中引入随机性来创建一组不同的分类器。集成分类器的预测结果是单个分类器预测结果的平均值。</p>
<p>与其他分类器一样，森林分类器必须拟合(fitted)两个数组： 保存训练样本的数组（或稀疏或稠密的）X，大小为 <code>[n_samples, n_features]</code>，和 保存训练样本目标值（类标签）的数组 Y，大小为 <code>[n_samples]</code>:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> RandomForestClassifier</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>X = [[<span class="number">0</span>, <span class="number">0</span>], [<span class="number">1</span>, <span class="number">1</span>]]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>Y = [<span class="number">0</span>, <span class="number">1</span>]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>clf = RandomForestClassifier(n_estimators=<span class="number">10</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>clf = clf.fit(X, Y)</span><br></pre></td></tr></table></figure>
<p>同 <a href="http://sklearn.apachecn.org/cn/0.19.0/modules/tree.html#tree" target="_blank" rel="noopener">决策树</a> 一样，随机森林算法（forests of trees）也能够通过扩展来解决<a href="http://sklearn.apachecn.org/cn/0.19.0/modules/tree.html#tree-multioutput" target="_blank" rel="noopener">多输出问题</a> (如果 Y 的大小是 <code>[n_samples, n_outputs])</code>。</p>
<p>在随机森林中（参见 <a href="http://sklearn.apachecn.org/cn/0.19.0/modules/generated/sklearn.ensemble.ExtraTreesClassifier.html#sklearn.ensemble.ExtraTreesClassifier" target="_blank" rel="noopener"><code>ExtraTreesClassifier</code></a> 和 <a href="http://sklearn.apachecn.org/cn/0.19.0/modules/generated/sklearn.ensemble.ExtraTreesRegressor.html#sklearn.ensemble.ExtraTreesRegressor" target="_blank" rel="noopener"><code>ExtraTreesRegressor</code></a> 类）， 集成模型中的每棵树构建时的样本都是由训练集经过有放回抽样得来的（例如，自助采样法-bootstrap sample，这里采用西瓜书中的译法）。 另外，在构建树的过程中进行结点分割时，选择的分割点不再是所有特征中最佳分割点，而是特征的一个随机子集中的最佳分割点。 由于这种随机性，森林的偏差通常会有略微的增大（相对于单个非随机树的偏差），但是由于取了平均，其方差也会减小，通常能够补偿偏差的增加，从而产生一个总体上更好的模型。</p>
<p>与原始文献<a href="http://sklearn.apachecn.org/cn/0.19.0/modules/ensemble.html#b2001" target="_blank" rel="noopener">[B2001]</a> 不同的是，scikit-learn 的实现是取每个分类器预测概率的平均，而不是让每个分类器对类别进行投票。</p>
<p>下面给出一个sklearn随机森林的例子：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> matplotlib.colors <span class="keyword">import</span> ListedColormap</span><br><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> RandomForestClassifier, AdaBoostClassifier</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> StandardScaler</span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> make_moons, make_circles, make_classification</span><br><span class="line"></span><br><span class="line">point_num = <span class="number">1000</span></span><br><span class="line">h = <span class="number">.02</span>  <span class="comment"># step size in the mesh</span></span><br><span class="line"></span><br><span class="line">clf = RandomForestClassifier(max_depth=<span class="number">5</span>, n_estimators=<span class="number">100</span>, max_features=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">X, y = make_classification(n_samples= point_num, n_features=<span class="number">2</span>, n_redundant=<span class="number">0</span>, n_informative=<span class="number">2</span>, random_state=<span class="number">1</span>, n_clusters_per_class=<span class="number">1</span>)</span><br><span class="line">rng = np.random.RandomState(<span class="number">2</span>)</span><br><span class="line">X += <span class="number">2</span> * rng.uniform(size=X.shape)</span><br><span class="line"></span><br><span class="line">datasets = [make_moons(n_samples=point_num, noise=<span class="number">0.3</span>, random_state=<span class="number">0</span>), make_circles(n_samples=point_num, noise=<span class="number">0.2</span>, factor=<span class="number">0.5</span>, random_state=<span class="number">1</span>), (X, y)]</span><br><span class="line"></span><br><span class="line">figure = plt.figure()</span><br><span class="line"><span class="comment"># iterate over datasets</span></span><br><span class="line"><span class="keyword">for</span> ds_cnt, ds <span class="keyword">in</span> enumerate(datasets):</span><br><span class="line">    <span class="comment"># preprocess dataset, split into training and test part</span></span><br><span class="line">    X, y = ds</span><br><span class="line">    X = StandardScaler().fit_transform(X)</span><br><span class="line">    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=<span class="number">.4</span>, random_state=<span class="number">42</span>)</span><br><span class="line"></span><br><span class="line">    x_min, x_max = X[:, <span class="number">0</span>].min() - <span class="number">.5</span>, X[:, <span class="number">0</span>].max() + <span class="number">.5</span></span><br><span class="line">    y_min, y_max = X[:, <span class="number">1</span>].min() - <span class="number">.5</span>, X[:, <span class="number">1</span>].max() + <span class="number">.5</span></span><br><span class="line">    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># just plot the dataset first</span></span><br><span class="line">    cm = plt.cm.RdBu</span><br><span class="line">    cm_bright = ListedColormap([<span class="string">'#FF0000'</span>, <span class="string">'#0000FF'</span>])</span><br><span class="line">    ax = plt.subplot(len(datasets), <span class="number">1</span>, ds_cnt+<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># iterate over classifiers</span></span><br><span class="line">    clf.fit(X_train, y_train)</span><br><span class="line">    score = clf.score(X_test, y_test)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Plot the decision boundary. For that, we will assign a color to each</span></span><br><span class="line">    <span class="comment"># point in the mesh [x_min, x_max]x[y_min, y_max].</span></span><br><span class="line">    Z = clf.predict_proba(np.c_[xx.ravel(), yy.ravel()])[:, <span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Put the result into a color plot</span></span><br><span class="line">    Z = Z.reshape(xx.shape)</span><br><span class="line">    ax.contourf(xx, yy, Z, cmap=cm, alpha=<span class="number">.8</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Plot also the training points</span></span><br><span class="line">    ax.scatter(X_train[:, <span class="number">0</span>], X_train[:, <span class="number">1</span>], c=y_train, cmap=cm_bright, edgecolors=<span class="string">'k'</span>)</span><br><span class="line">    <span class="comment"># and testing points</span></span><br><span class="line">    <span class="comment"># ax.scatter(X_test[:, 0], X_test[:, 1], c=y_test, cmap=cm_bright, edgecolors='k', alpha=0.6)</span></span><br><span class="line"></span><br><span class="line">    ax.set_xlim(xx.min(), xx.max())</span><br><span class="line">    ax.set_ylim(yy.min(), yy.max())</span><br><span class="line">    ax.set_xticks(())</span><br><span class="line">    ax.set_yticks(())</span><br><span class="line">    ax.set_title(<span class="string">"Random Forest"</span>)</span><br><span class="line">    ax.text(xx.max() - <span class="number">.3</span>, yy.min() + <span class="number">.3</span>, (<span class="string">'%.2f'</span> % score).lstrip(<span class="string">'0'</span>), size=<span class="number">15</span>, horizontalalignment=<span class="string">'right'</span>)</span><br><span class="line"></span><br><span class="line">plt.tight_layout()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p>结果是</p>
<p><img src="Figure_1.png" alt="结果示意图"></p>
<h3 id="sklearn随机森林参数调优"><a href="#sklearn随机森林参数调优" class="headerlink" title="sklearn随机森林参数调优"></a>sklearn随机森林参数调优</h3><p>和GBDT的调参类似，RF需要调参的参数也包括两部分，第一部分是Bagging框架的参数，第二部分是CART决策树的参数。下面我们就对这些参数做一个介绍。</p>
<ul>
<li>RF框架参数</li>
</ul>
<p>首先我们关注于RF的Bagging框架的参数。这里可以和GBDT对比来学习。在scikit-learn 梯度提升树(GBDT)调参小结中我们对GBDT的框架参数做了介绍。GBDT的框架参数比较多，重要的有最大迭代器个数，步长和子采样比例，调参起来比较费力。但是RF则比较简单，这是因为bagging框架里的各个弱学习器之间是没有依赖关系的，这减小的调参的难度。换句话说，达到同样的调参效果，RF调参时间要比GBDT少一些。</p>
<p>下面我来看看RF重要的Bagging框架的参数，由于RandomForestClassifier和RandomForestRegressor参数绝大部分相同，这里会将它们一起讲，不同点会指出。</p>
<ol>
<li><p>n_estimators: 也就是弱学习器的最大迭代次数，或者说最大的弱学习器的个数。一般来说n_estimators太小，容易欠拟合，n_estimators太大，计算量会太大，并且n_estimators到一定的数量后，再增大n_estimators获得的模型提升会很小，所以一般选择一个适中的数值。默认是100。</p>
</li>
<li><p>oob_score :即是否采用袋外样本来评估模型的好坏。默认识False。个人推荐设置为True，因为袋外分数反应了一个模型拟合后的泛化能力。</p>
</li>
<li><p>criterion: 即CART树做划分时对特征的评价标准。分类模型和回归模型的损失函数是不一样的。分类RF对应的CART分类树默认是基尼系数gini,另一个可选择的标准是信息增益。回归RF对应的CART回归树默认是均方差mse，另一个可以选择的标准是绝对值差mae。一般来说选择默认的标准就已经很好的。</p>
</li>
</ol>
<p>从上面可以看出， RF重要的框架参数比较少，主要需要关注的是 n_estimators，即RF最大的决策树个数。</p>
<ul>
<li>RF决策树参数</li>
</ul>
<p>下面我们再来看RF的决策树参数，它要调参的参数基本和GBDT相同，如下:</p>
<ol>
<li><p>RF划分时考虑的最大特征数max_features: 可以使用很多种类型的值，默认是<code>None</code>,意味着划分时考虑所有的特征数；如果是<code>log2</code>意味着划分时最多考虑$log_2 N$个特征；如果是<code>sqrt</code>或者<code>auto</code>意味着划分时最多考虑$\sqrt{N}$个特征。如果是整数，代表考虑的特征绝对数。如果是浮点数，代表考虑特征百分比，即考虑（百分比xN）取整后的特征数。其中N为样本总特征数。一般来说，如果样本特征数不多，比如小于50，我们用默认的<code>None</code>就可以了，如果特征数非常多，我们可以灵活使用刚才描述的其他取值来控制划分时考虑的最大特征数，以控制决策树的生成时间。</p>
</li>
<li><p>决策树最大深度max_depth：默认可以不输入，如果不输入的话，决策树在建立子树的时候不会限制子树的深度。一般来说，数据少或者特征少的时候可以不管这个值。如果模型样本量多，特征也多的情况下，推荐限制这个最大深度，具体的取值取决于数据的分布。常用的可以取值10-100之间。</p>
</li>
<li><p>内部节点再划分所需最小样本数min_samples_split：这个值限制了子树继续划分的条件，如果某节点的样本数少于min_samples_split，则不会继续再尝试选择最优特征来进行划分。 默认是2.如果样本量不大，不需要管这个值。如果样本量数量级非常大，则推荐增大这个值。</p>
</li>
<li><p>叶子节点最少样本数min_samples_leaf：这个值限制了叶子节点最少的样本数，如果某叶子节点数目小于样本数，则会和兄弟节点一起被剪枝。 默认是1,可以输入最少的样本数的整数，或者最少样本数占样本总数的百分比。如果样本量不大，不需要管这个值。如果样本量数量级非常大，则推荐增大这个值。</p>
</li>
<li><p>叶子节点最小的样本权重和min_weight_fraction_leaf：这个值限制了叶子节点所有样本权重和的最小值，如果小于这个值，则会和兄弟节点一起被剪枝。 默认是0，就是不考虑权重问题。一般来说，如果我们有较多样本有缺失值，或者分类树样本的分布类别偏差很大，就会引入样本权重，这时我们就要注意这个值了。</p>
</li>
<li><p>最大叶子节点数max_leaf_nodes: 通过限制最大叶子节点数，可以防止过拟合，默认是”None”，即不限制最大的叶子节点数。如果加了限制，算法会建立在最大叶子节点数内最优的决策树。如果特征不多，可以不考虑这个值，但是如果特征分成多的话，可以加以限制，具体的值可以通过交叉验证得到。</p>
</li>
<li><p>节点划分最小不纯度min_impurity_split:  这个值限制了决策树的增长，如果某节点的不纯度(基于基尼系数，均方差)小于这个阈值，则该节点不再生成子节点。即为叶子节点 。一般不推荐改动默认值1e-7。</p>
</li>
</ol>
<p>上面决策树参数中最重要的包括最大特征数max_features， 最大深度max_depth， 内部节点再划分所需最小样本数min_samples_split和叶子节点最少样本数min_samples_leaf。</p>
<h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><ol>
<li><a href="https://zh.wikipedia.org/wiki/提升方法" target="_blank" rel="noopener">提升方法</a></li>
<li><a href="https://wizardforcel.gitbooks.io/dm-algo-top10/content/adaboost.html" target="_blank" rel="noopener">数据挖掘算法学习（八）Adaboost算法</a></li>
<li><a href="https://zh.wikipedia.org/wiki/Bagging算法" target="_blank" rel="noopener">Bagging算法</a></li>
<li><a href="http://kns.cnki.net/KCMS/detail/detail.aspx?dbcode=CMFD&amp;dbname=CMFD201402&amp;filename=1014224505.nh&amp;uid=WEEvREcwSlJHSldRa1FhcEE0NXh1K1dPZEsvTVlXSGdYbDA1dHI3c3E3RT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4ggI8Fm4gTkoUKaID8j8gFw!!&amp;v=MTI1NTFNcXBFYlBJUjhlWDFMdXhZUzdEaDFUM3FUcldNMUZyQ1VSTDJlWnVWdkZDamhXcjdBVkYyNkdyRzZHdFQ=" target="_blank" rel="noopener">李贞贵. 随机森林改进的若干研究[D].厦门大学,2013.</a></li>
<li><a href="https://clyyuanzi.gitbooks.io/julymlnotes/content/rf.html" target="_blank" rel="noopener">随机森林与决策树</a></li>
<li><a href="http://sklearn.apachecn.org/cn/0.19.0/modules/ensemble.html#forest" target="_blank" rel="noopener">1.11. 集成方法</a></li>
<li><a href="http://www.cnblogs.com/pinard/p/6160412.html" target="_blank" rel="noopener">scikit-learn随机森林调参小结</a></li>
</ol>
</div><div class="tags"><a href="/tags/Random-Forests/">Random Forests</a><a href="/tags/集成学习/">集成学习</a></div><div class="post-nav"><a class="pre" href="/2018-3-28-ML-LightGBM/">LightGBM简介</a><a class="next" href="/Sublime-Setting/">Sublime配置</a></div></div></div></div><div class="pure-u-1-4 hidden_mid_and_down"><div id="sidebar"><div class="widget"><form class="search-form" action="//www.google.com/search" method="get" accept-charset="utf-8" target="_blank"><input type="text" name="q" maxlength="20" placeholder="Search"><input type="hidden" name="sitesearch" value="https://chaideblog.github.io"></form></div><div class="widget"><div class="widget-title"><i class="fa fa-folder-o"> 分类</i></div><ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/Algorithm/">Algorithm</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/BigData/">BigData</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Deep-Learning/">Deep Learning</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Interview/">Interview</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Java/">Java</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Linux/">Linux</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Machine-Learning/">Machine Learning</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Math/">Math</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Python/">Python</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-star-o"> 标签</i></div><div class="tagcloud"><a href="/tags/Probability-theroy/" style="font-size: 15px;">Probability theroy</a> <a href="/tags/Genetic-Algorithm/" style="font-size: 15px;">Genetic Algorithm</a> <a href="/tags/决策树/" style="font-size: 15px;">决策树</a> <a href="/tags/Python语法/" style="font-size: 15px;">Python语法</a> <a href="/tags/VPS/" style="font-size: 15px;">VPS</a> <a href="/tags/Convex-Optimize/" style="font-size: 15px;">Convex Optimize</a> <a href="/tags/Naive-Bayes/" style="font-size: 15px;">Naive Bayes</a> <a href="/tags/Sublime/" style="font-size: 15px;">Sublime</a> <a href="/tags/Random-Forests/" style="font-size: 15px;">Random Forests</a> <a href="/tags/集成学习/" style="font-size: 15px;">集成学习</a> <a href="/tags/Hadoop/" style="font-size: 15px;">Hadoop</a> <a href="/tags/Java/" style="font-size: 15px;">Java</a> <a href="/tags/FFT/" style="font-size: 15px;">FFT</a> <a href="/tags/Apache/" style="font-size: 15px;">Apache</a> <a href="/tags/树莓派/" style="font-size: 15px;">树莓派</a> <a href="/tags/SVM/" style="font-size: 15px;">SVM</a> <a href="/tags/SGD/" style="font-size: 15px;">SGD</a> <a href="/tags/BGD/" style="font-size: 15px;">BGD</a> <a href="/tags/MBGD/" style="font-size: 15px;">MBGD</a> <a href="/tags/SMO/" style="font-size: 15px;">SMO</a> <a href="/tags/标准化/" style="font-size: 15px;">标准化</a> <a href="/tags/回归/" style="font-size: 15px;">回归</a> <a href="/tags/CTR/" style="font-size: 15px;">CTR</a> <a href="/tags/LeetCode/" style="font-size: 15px;">LeetCode</a> <a href="/tags/LightGBM/" style="font-size: 15px;">LightGBM</a> <a href="/tags/RCNN/" style="font-size: 15px;">RCNN</a> <a href="/tags/目标检测/" style="font-size: 15px;">目标检测</a> <a href="/tags/Interview/" style="font-size: 15px;">Interview</a> <a href="/tags/LR/" style="font-size: 15px;">LR</a> <a href="/tags/FM-FFM/" style="font-size: 15px;">FM/FFM</a> <a href="/tags/Recommend-System/" style="font-size: 15px;">Recommend System</a> <a href="/tags/Natural-Language-Processing/" style="font-size: 15px;">Natural Language Processing</a> <a href="/tags/Recommendation/" style="font-size: 15px;">Recommendation</a></div></div><div class="widget"><div class="widget-title"><i class="fa fa-file-o"> 最近文章</i></div><ul class="post-list"><li class="post-list-item"><a class="post-list-link" href="/2019-5-5-Paper-TEM/">基于决策树的可解释嵌入推荐模型：TEM</a></li><li class="post-list-item"><a class="post-list-link" href="/2019-5-4-Transformer/">Transformer理解与Tensorflow代码阅读</a></li><li class="post-list-item"><a class="post-list-link" href="/2019-4-28-Normalization/">深度学习中各种Normalization</a></li><li class="post-list-item"><a class="post-list-link" href="/2019-4-22-recommend-system-1/">《推荐系统实战》读书笔记（一）</a></li><li class="post-list-item"><a class="post-list-link" href="/2019-4-6-toutiao-interview/">字节跳动广告算法团队实习面试经历（Offer）</a></li><li class="post-list-item"><a class="post-list-link" href="/2019-3-13-xiaomi-interview/">小米信息流推荐实习面试经历（Offer）</a></li><li class="post-list-item"><a class="post-list-link" href="/2019-3-8-sohu-interview/">搜狐推荐算法实习面试经历（Offer）</a></li><li class="post-list-item"><a class="post-list-link" href="/2019-2-27-CTR-LR-FM-FFM/">CTR系列（一）：LR、FM/FFM</a></li><li class="post-list-item"><a class="post-list-link" href="/2019-2-21-ms-bing-interview/">微软Bing组算法（Game Over）</a></li><li class="post-list-item"><a class="post-list-link" href="/2019-1-8-PriorityQueue/">【转】Java容器源码分析之PriorityQueue</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-external-link"> 友情链接</i></div><ul></ul><a href="http://www.example1.com/" title="site-name1" target="_blank">site-name1</a><ul></ul><a href="http://www.example2.com/" title="site-name2" target="_blank">site-name2</a><ul></ul><a href="http://www.example3.com/" title="site-name3" target="_blank">site-name3</a></div></div></div><div class="pure-u-1 pure-u-md-3-4"><div id="footer">Copyright © 2019 <a href="/." rel="nofollow">CHAI' blog.</a> Powered by<a rel="nofollow" target="_blank" href="https://hexo.io"> Hexo.</a><a rel="nofollow" target="_blank" href="https://github.com/tufu9441/maupassant-hexo"> Theme</a> by<a rel="nofollow" target="_blank" href="https://github.com/pagecho"> Cho.</a></div></div></div><a class="show" id="rocket" href="#top"></a><script type="text/javascript" src="/js/totop.js?v=0.0.0" async></script><script type="text/javascript" src="//lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.js" async></script><script type="text/javascript" src="/js/fancybox.js?v=0.0.0" async></script><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.css"><script type="text/x-mathjax-config">MathJax.Hub.Config({
  tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}
  });
</script><script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML" async></script><script type="text/javascript" src="/js/codeblock-resizer.js?v=0.0.0"></script><script type="text/javascript" src="/js/smartresize.js?v=0.0.0"></script></div></body></html>