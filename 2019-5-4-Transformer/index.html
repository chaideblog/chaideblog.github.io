<!DOCTYPE html><html lang="zh-CN"><head><meta name="generator" content="Hexo 3.8.0"><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description"><title>Transformer理解与Tensorflow代码阅读 | CHAI' blog</title><link rel="stylesheet" type="text/css" href="/css/style.css?v=0.0.0"><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/normalize/8.0.1/normalize.min.css"><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/pure/1.0.0/pure-min.css"><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/pure/1.0.0/grids-responsive-min.css"><link rel="stylesheet" href="//lib.baomitu.com/font-awesome/4.7.0/css/font-awesome.min.css"><script type="text/javascript" src="//lib.baomitu.com/jquery/3.4.0/jquery.min.js"></script><link rel="icon" mask sizes="any" href="/favicon.ico"><link rel="Shortcut Icon" type="image/x-icon" href="/favicon.ico"><link rel="apple-touch-icon" href="/apple-touch-icon.png"><link rel="apple-touch-icon-precomposed" href="/apple-touch-icon.png"></head><body><div class="body_container"><div id="header"><div class="site-name"><h1 class="hidden">Transformer理解与Tensorflow代码阅读</h1><a id="logo" href="/.">CHAI' blog</a><p class="description"></p></div><div id="nav-menu"><a class="current" href="/."><i class="fa fa-home"> 首页</i></a><a href="/archives/"><i class="fa fa-archive"> 归档</i></a><a href="/about/"><i class="fa fa-user"> 关于</i></a><a href="/atom.xml"><i class="fa fa-rss"> 订阅</i></a></div></div><div class="pure-g" id="layout"><div class="pure-u-1 pure-u-md-3-4"><div class="content_container"><div class="post"><h1 class="post-title">Transformer理解与Tensorflow代码阅读</h1><div class="post-meta">May 4, 2019<span> | </span><span class="category"><a href="/categories/Machine-Learning/">Machine Learning</a></span></div><div class="clear"><div class="toc-article" id="toc"><div class="toc-title">文章目录</div><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#Transformer架构"><span class="toc-number">1.</span> <span class="toc-text">Transformer架构</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#整体流程"><span class="toc-number">1.1.</span> <span class="toc-text">整体流程</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Encode侧"><span class="toc-number">1.1.1.</span> <span class="toc-text">Encode侧</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Decode侧"><span class="toc-number">1.1.2.</span> <span class="toc-text">Decode侧</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Positional-Encoding"><span class="toc-number">1.2.</span> <span class="toc-text">Positional Encoding</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Mask"><span class="toc-number">1.3.</span> <span class="toc-text">Mask</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Self-attention和Multihead-attention"><span class="toc-number">1.4.</span> <span class="toc-text">Self-attention和Multihead-attention</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Self-attention"><span class="toc-number">1.4.1.</span> <span class="toc-text">Self-attention</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Multihead-attention"><span class="toc-number">1.4.2.</span> <span class="toc-text">Multihead-attention</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Layer-Normalization"><span class="toc-number">1.5.</span> <span class="toc-text">Layer Normalization</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#残差结构与Feed-Forward"><span class="toc-number">1.6.</span> <span class="toc-text">残差结构与Feed Forward</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#输出的Linear层和Softmax"><span class="toc-number">1.7.</span> <span class="toc-text">输出的Linear层和Softmax</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#损失函数"><span class="toc-number">2.</span> <span class="toc-text">损失函数</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#参考文章"><span class="toc-number">3.</span> <span class="toc-text">参考文章</span></a></li></ol></div></div><div class="post-content"><p>谷歌2017年发了一篇论文<a href="https://arxiv.org/abs/1706.03762" target="_blank" rel="noopener">Attention is all you need</a>，文中提出了一种新的架构叫做Transformer，用以来实现机器翻译。它抛弃了传统用CNN或者RNN的定式，取得了很好的效果，激起了工业界和学术界的广泛讨论。本文是个人对Transformer的一些理解，和Github上Tensorflow实现的分析。</p>
<h2 id="Transformer架构"><a href="#Transformer架构" class="headerlink" title="Transformer架构"></a>Transformer架构</h2><p><img src="transformer_structure.jpg" alt="transformer_structure"></p>
<p>Transformer模型也是使用经典的encoer-decoder架构，由encoder和decoder两部分组成。左边是encoder部分，一般由6层重复的结构构成。右边是decoder部分，一般由6层重复的结构构成。单词在Transformer里的流向为</p>
<p><img src="The_transformer_encoder_decoder_stack.png" alt="The_transformer_encoder_decoder_stack"></p>
<p>本文将Transformer模型分为8部分介绍，分别为</p>
<ul>
<li>整体流程（包括encoder和decoder）；</li>
<li>Positional Encoding；</li>
<li>Mask机制；</li>
<li>Self-attention和Multihead-attention；</li>
<li>Layer Normalization；</li>
<li>残差结构与Feed Forward；</li>
<li>输出的Linear层和Softmax。</li>
</ul>
<h3 id="整体流程"><a href="#整体流程" class="headerlink" title="整体流程"></a>整体流程</h3><p>Encode侧是一堆Encoder器（论文中是6个Encoder组成的，Encoder的数量可以实验）。Decode侧是相同数量的Decoder器组成的。Encoder的结构完全相同（但它们不共享权重），每一个都分为两个子层：Self-attention和Feed Forward。Decoder的结构也完全相同（同样不共享权重），每一个都分为三个部分：Self-attention、Context-attention（姑且这么叫）和Feed Forward。更加清晰的流程图有</p>
<p><img src="transformer_structure_detail.png" alt="transformer_structure_detail"></p>
<p><img src="transformer_training.gif" alt="transformer_training"></p>
<h4 id="Encode侧"><a href="#Encode侧" class="headerlink" title="Encode侧"></a>Encode侧</h4><p>Encode侧的流程主要为：</p>
<ol>
<li>词向量的embedding和positional encoding；</li>
<li>将embedding向量作为query、key和value输入encoder，得到新的词向量编码再输入下一个encoder，重复6次；</li>
<li>最终得到Encode侧的编码。</li>
</ol>
<p>下面是Encode的Tensorflow代码，来自<a href="https://github.com/Kyubyong/transformer" target="_blank" rel="noopener">Kyubyong: transformer</a>。代码的分析会放在代码的注释中。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">encode</span><span class="params">(self, xs, training=True)</span>:</span></span><br><span class="line">  <span class="keyword">with</span> tf.variable_scope(<span class="string">"encoder"</span>, reuse=tf.AUTO_REUSE):</span><br><span class="line">    x, seqlens, sents1 = xs <span class="comment"># Encoder的输入，包括词向量、句子长度和句子</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 根据x从self.embeddings取出embedding后的向量，d_model为每个单词embedding后的维度</span></span><br><span class="line">    enc = tf.nn.embedding_lookup(self.embeddings, x) <span class="comment"># (N, T1, d_model)   </span></span><br><span class="line">    <span class="comment"># 为什么这里会还有一步scale还没明白</span></span><br><span class="line">    enc *= self.hp.d_model ** <span class="number">0.5</span> <span class="comment"># scale</span></span><br><span class="line">		<span class="comment"># Positional Encoding</span></span><br><span class="line">    enc += positional_encoding(enc, self.hp.maxlen1)</span><br><span class="line">    enc = tf.layers.dropout(enc, self.hp.dropout_rate, training=training)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Encoder结构</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(self.hp.num_blocks): <span class="comment"># self.hp.num_blocks表示Encoder的数量</span></span><br><span class="line">      <span class="keyword">with</span> tf.variable_scope(<span class="string">"num_blocks_&#123;&#125;"</span>.format(i), reuse=tf.AUTO_REUSE):</span><br><span class="line">        <span class="comment"># multihead-attention，注意输入的query、key和value都是enc</span></span><br><span class="line">        enc = multihead_attention(</span><br><span class="line">            queries=enc,</span><br><span class="line">            keys=enc,</span><br><span class="line">            values=enc,</span><br><span class="line">            num_heads=self.hp.num_heads,</span><br><span class="line">            dropout_rate=self.hp.dropout_rate,</span><br><span class="line">            training=training,</span><br><span class="line">            causality=<span class="literal">False</span></span><br><span class="line">            )</span><br><span class="line">        <span class="comment"># feed forward，实际就是一个全连接层</span></span><br><span class="line">        enc = feed_forward(enc, num_units=[self.hp.d_ff, self.hp.d_model])</span><br><span class="line">        memory = enc <span class="comment"># (N, T1, d_model)</span></span><br><span class="line">        <span class="keyword">return</span> memory, sents1</span><br></pre></td></tr></table></figure>
<h4 id="Decode侧"><a href="#Decode侧" class="headerlink" title="Decode侧"></a>Decode侧</h4><p>Decode侧的流程主要为：</p>
<ol>
<li>词向量的embedding和positional encoding；</li>
<li>将embedding向量和Encoder的输出作为Decoder输入，得到新的词向量编码，重复6遍；</li>
<li>最终将Decoder的输出接入一个全连接层，得到翻译结果。</li>
</ol>
<p>Decode侧的代码与Encode侧有许多相似，具体为</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">decode</span><span class="params">(self, ys, memory, training=True)</span>:</span></span><br><span class="line">    <span class="keyword">with</span> tf.variable_scope(<span class="string">"decoder"</span>, reuse=tf.AUTO_REUSE):</span><br><span class="line">        decoder_inputs, y, seqlens, sents2 = ys <span class="comment"># Decoder的输入，相比Encoder的输入只多了y</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># embedding (N, T2, d_model)</span></span><br><span class="line">        dec = tf.nn.embedding_lookup(self.embeddings, decoder_inputs)</span><br><span class="line">        dec *= self.hp.d_model ** <span class="number">0.5</span> <span class="comment"># scale</span></span><br><span class="line">        dec += positional_encoding(dec, self.hp.maxlen2)</span><br><span class="line">        dec = tf.layers.dropout(dec, self.hp.dropout_rate, training=training)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Decoder结构</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(self.hp.num_blocks): <span class="comment"># Decoder和Encoder的数量一样</span></span><br><span class="line">            <span class="keyword">with</span> tf.variable_scope(<span class="string">"num_blocks_&#123;&#125;"</span>.format(i), reuse=tf.AUTO_REUSE):</span><br><span class="line">                <span class="comment"># Decoder的输入的Self-attention，注意query、key和value都是dec</span></span><br><span class="line">                dec = multihead_attention(</span><br><span class="line">                    queries=dec,</span><br><span class="line">                    keys=dec,</span><br><span class="line">                    values=dec,</span><br><span class="line">                    num_heads=self.hp.num_heads,</span><br><span class="line">                    dropout_rate=self.hp.dropout_rate,</span><br><span class="line">                    training=training,</span><br><span class="line">                    causality=<span class="literal">True</span>,</span><br><span class="line">                    scope=<span class="string">"self_attention"</span></span><br><span class="line">                    )</span><br><span class="line"></span><br><span class="line">                <span class="comment"># Decoder的Self-attention的输入和Encoder的输出做Contex-attention</span></span><br><span class="line">                <span class="comment"># 注意query为dec，key和value是memory，即Encoder的输出</span></span><br><span class="line">                dec = multihead_attention(</span><br><span class="line">                    queries=dec,</span><br><span class="line">                    keys=memory,</span><br><span class="line">                    values=memory,</span><br><span class="line">                    num_heads=self.hp.num_heads,</span><br><span class="line">                    dropout_rate=self.hp.dropout_rate,</span><br><span class="line">                    training=training,</span><br><span class="line">                    causality=<span class="literal">False</span>,</span><br><span class="line">                    scope=<span class="string">"vanilla_attention"</span></span><br><span class="line">                    )</span><br><span class="line">                <span class="comment"># Feed Forward</span></span><br><span class="line">                dec = feed_forward(dec, num_units=[self.hp.d_ff, self.hp.d_model])</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 最终是一个全连接层。因为self.embeddings是一个d_model*vocab_size的矩阵，</span></span><br><span class="line">    <span class="comment"># 维度很大，训练起来困难。因此，输出的矩阵和输入的矩阵共享，并不会减少太多精度</span></span><br><span class="line">    weights = tf.transpose(self.embeddings) <span class="comment"># (d_model, vocab_size)</span></span><br><span class="line">    logits = tf.einsum(<span class="string">'ntd,dk-&gt;ntk'</span>, dec, weights) <span class="comment"># (N, T2, vocab_size)</span></span><br><span class="line">    y_hat = tf.to_int32(tf.argmax(logits, axis=<span class="number">-1</span>))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> logits, y_hat, y, sents2</span><br></pre></td></tr></table></figure>
<h3 id="Positional-Encoding"><a href="#Positional-Encoding" class="headerlink" title="Positional Encoding"></a>Positional Encoding</h3><blockquote>
<p> 由于Transformer没有用到CNN和RNN，因此句子单词之间的位置信息就没有利用到。显然，这些信息对于翻译来说是非常有用的，同样一句话，每个单词的意思能够准确的翻译出来，但如果顺序不对，表达出来的意思就截然不同了。举个栗子感受一下，原句：”A man went through the Big Buddhist Temple“, 翻译成：”人过大佛寺“和”寺佛大过人“，意思就完全不同了。</p>
<p>那么如何表达一个序列的位置信息呢？对于某一个单词来说，他的位置信息主要有两个方面：一是绝对位置，二是相对位置。绝对位置决定了单词在一个序列中的第几个位置，相对位置决定了序列的流向。作者利用了正弦函数和余弦函数来进行位置编码：</p>
<p>$$PE(pos, 2i) = \sin (pos / 10000^{2i / d_{model}})​$$</p>
<p>$$PE(pos, 2i+1) = \cos (pos / 10000^{2i / d_{model}})$$</p>
<p>上述分析来自<a href="https://lonepatient.top/2019/01/17/BERT-Transformer.html" target="_blank" rel="noopener">eamlife’s blog：Transformer原理和实现</a></p>
</blockquote>
<p><img src="positional_encoding.png" alt="positional_encoding"></p>
<p>其中，$$pos$$表示单词在句子中的位置，$$i$$表示经过embedding后维度为d_model的向量的索引。</p>
<blockquote>
<p>我们来考察一下第一个公式，看是否每个位置都能得到一个唯一的值作为编码。为简单起见，不妨令$$i=0​$$，那么：</p>
<p>$$PE(pos,0)=\sin(pos)$$</p>
<p>我们反过来想，假如存在位置$$j$$和$$k$$的编码值相同，那么就有：</p>
<p>$$\sin(i) = \sin(j)$$</p>
<p>$$i,j$$为非负整数且$$i$$不等于$$j$$</p>
<p>以上两式需要同时满足，可等价为：</p>
<p>$$i=(−1)k \cdot j+k \cdot \pi$$</p>
<p>$$i,j$$为非负整数且$$i$$不等于$$j$$且$$k​$$为整数</p>
<p>同时成立，这就意味着：</p>
<p>$$\pi =[i−(−1)k \cdot j]k$$</p>
<p>这显然是不可能的，因为左边是个无理数（无限不循环小数），而右边是个有理数。通过反证法就证明了在这种表示下，每个位置确实有唯一的编码。</p>
<p>上面的讨论并未考虑$$i$$的作用。$$i$$决定了频率的大小，不同的$$i$$可以看成是不同的频率空间中的编码，是相互正交的，通过改变$$i$$的值，就能得到多维度的编码，类似于词向量的维度。这里$$2i \le 512(d_{model})$$, 一共512维。想象一下，当$$2i$$大于d_model时会出现什么情况，这时sin函数的周期会变得非常大，函数值会非常接近于0，这显然不是我们希望看到的，因为这样和词向量就不在一个量级了，位置编码的作用被削弱了。另外，值得注意的是，位置编码是不参与训练的，而词向量是参与训练的。作者通过实验发现，位置编码参与训练与否对最终的结果并无影响。</p>
<p>上述分析来自<a href="https://lonepatient.top/2019/01/17/BERT-Transformer.html" target="_blank" rel="noopener">eamlife’s blog：Transformer原理和实现</a></p>
</blockquote>
<p>下面是Positional Encoding的代码，并没有什么特别的trick。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">positional_encoding</span><span class="params">(inputs,</span></span></span><br><span class="line"><span class="function"><span class="params">                        maxlen,</span></span></span><br><span class="line"><span class="function"><span class="params">                        masking=True,</span></span></span><br><span class="line"><span class="function"><span class="params">                        scope=<span class="string">"positional_encoding"</span>)</span>:</span></span><br><span class="line">    E = inputs.get_shape().as_list()[<span class="number">-1</span>] <span class="comment"># 固定的d_model</span></span><br><span class="line">    <span class="comment"># N表示batch数量，T为模型embedding维度</span></span><br><span class="line">    N, T = tf.shape(inputs)[<span class="number">0</span>], tf.shape(inputs)[<span class="number">1</span>]</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">with</span> tf.variable_scope(scope, reuse=tf.AUTO_REUSE):</span><br><span class="line">        <span class="comment"># position indices</span></span><br><span class="line">        position_ind = tf.tile(tf.expand_dims(tf.range(T), <span class="number">0</span>), [N, <span class="number">1</span>]) <span class="comment"># (N, T)</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># First part of the PE function: sin and cos argument</span></span><br><span class="line">        position_enc = np.array([</span><br><span class="line">            [pos / np.power(<span class="number">10000</span>, (i-i%<span class="number">2</span>)/E) <span class="keyword">for</span> i <span class="keyword">in</span> range(E)]</span><br><span class="line">            <span class="keyword">for</span> pos <span class="keyword">in</span> range(maxlen)])</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Second part, apply the cosine to even columns and sin to odds.</span></span><br><span class="line">        position_enc[:, <span class="number">0</span>::<span class="number">2</span>] = np.sin(position_enc[:, <span class="number">0</span>::<span class="number">2</span>])  <span class="comment"># dim 2i</span></span><br><span class="line">        position_enc[:, <span class="number">1</span>::<span class="number">2</span>] = np.cos(position_enc[:, <span class="number">1</span>::<span class="number">2</span>])  <span class="comment"># dim 2i+1</span></span><br><span class="line">        position_enc = tf.convert_to_tensor(position_enc, tf.float32) <span class="comment"># (maxlen, E)</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># lookup</span></span><br><span class="line">        outputs = tf.nn.embedding_lookup(position_enc, position_ind)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 因为后面句子的embedding需要和position_enc相加，然后输入到mask里。</span></span><br><span class="line">        <span class="comment"># 但是mask是根据是否为0判断是否Padding Mask。因为position_enc一定</span></span><br><span class="line">        <span class="comment"># 不为0，所以句子补零的部分不需要加position_enc。</span></span><br><span class="line">        <span class="keyword">if</span> masking:</span><br><span class="line">            outputs = tf.where(tf.equal(inputs, <span class="number">0</span>), inputs, outputs)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> tf.to_float(outputs)</span><br></pre></td></tr></table></figure>
<h3 id="Mask"><a href="#Mask" class="headerlink" title="Mask"></a>Mask</h3><p>mask顾名思义就是<strong>掩码</strong>，在我们这里的意思大概就是<strong>对某些值进行掩盖，使其不产生效果</strong>。Transformer模型里面涉及两种mask。分别是Padding Masking和Sequence Masking。其中，Padding Masking在所有的scaled dot-product attention里面都需要用到，而Sequence Masking只有在Decoder的self-attention里面用到。</p>
<p>####Padding Masking</p>
<p>由于Transformer中输入序列长度是不一样的，因此需要对输入序列进行<strong>补齐</strong>。具体来说，就是给在较短的序列后面填充0。因为这些填充的位置，其实是没什么意义的，所以我们的attention机制不应该把注意力放在这些位置上。同时，0经过softmax后并不是0，并且0上的梯度很大，所以我们需要进行一些处理。Padding Masking的做法是在补齐位置的值加上一个非常大的负数（可以是负无穷），经过softmax后，这些位置的概率就会接近0。</p>
<p>####Sequence Masking</p>
<p>Sequence masking是为了使得Decoder不能看见未来的信息。也就是对于一个序列，在time_step为t的时刻，我们的解码输出应该只能依赖于t时刻之前的输出，而不能依赖t之后的输出。因此我们需要想一个办法，把t之后的信息给隐藏起来。</p>
<p>Sequence masking的做法是用一个上三角矩阵，上三角的值全为1，下三角的值全为0，对角线也是0。将输出和该上三角矩阵相乘，就可以得到Sequence masking的结果了。举个例子：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">outputs:</span><br><span class="line">        I    am   a    robot</span><br><span class="line"> I  [[[[<span class="number">0.8</span>, <span class="number">0.2</span>, <span class="number">0.3</span>, <span class="number">0.9</span>]</span><br><span class="line"> am    [<span class="number">0.2</span>, <span class="number">0.9</span>, <span class="number">0.6</span>, <span class="number">0.4</span>]</span><br><span class="line"> a     [<span class="number">0.3</span>, <span class="number">0.6</span>, <span class="number">0.8</span>, <span class="number">0.7</span>]</span><br><span class="line"> robot [<span class="number">1.2</span>, <span class="number">0.6</span>, <span class="number">2.1</span>, <span class="number">3.2</span>]]]]</span><br><span class="line"></span><br><span class="line">mask:</span><br><span class="line">    [[[[<span class="number">1</span>,  <span class="number">0</span>,   <span class="number">0</span>,  <span class="number">0</span>],</span><br><span class="line">       [<span class="number">1</span>,  <span class="number">1</span>,   <span class="number">0</span>,  <span class="number">0</span>],</span><br><span class="line">       [<span class="number">1</span>,  <span class="number">1</span>,   <span class="number">1</span>,  <span class="number">0</span>],</span><br><span class="line">       [<span class="number">1</span>,  <span class="number">1</span>,   <span class="number">1</span>,  <span class="number">1</span>]]]]</span><br><span class="line">  </span><br><span class="line">outputs * mask:</span><br><span class="line">        I    am   a    robot</span><br><span class="line"> I  [[[[<span class="number">0.8</span>, <span class="number">0</span>,   <span class="number">0</span>,   <span class="number">0</span>],</span><br><span class="line"> am    [<span class="number">0.2</span>, <span class="number">0.9</span>, <span class="number">0</span>,   <span class="number">0</span>],</span><br><span class="line"> a     [<span class="number">0.3</span>, <span class="number">0.6</span>, <span class="number">0.8</span>, <span class="number">0</span>],</span><br><span class="line"> robot [<span class="number">1.2</span>, <span class="number">0.6</span>, <span class="number">2.1</span>, <span class="number">3.2</span>]]]]</span><br></pre></td></tr></table></figure>
<p>预测”am”的下一个词时，只有”I”和”am”参与了attention，其他的词（相对于”am”为未来的词）都不应该起作用，因此权重被清除。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">mask</span><span class="params">(inputs, queries=None, keys=None, type=None)</span>:</span></span><br><span class="line">    padding_num = <span class="number">-2</span> ** <span class="number">32</span> + <span class="number">1</span> <span class="comment"># -inf</span></span><br><span class="line">    <span class="keyword">if</span> type <span class="keyword">in</span> (<span class="string">'k'</span>, <span class="string">'key'</span>, <span class="string">'keys'</span>):</span><br><span class="line">        <span class="comment"># generate masks</span></span><br><span class="line">        <span class="comment"># 如果一行全为0才认为可以被mask掉</span></span><br><span class="line">        masks = tf.sign(tf.reduce_sum(tf.abs(keys), axis=<span class="number">-1</span>)) <span class="comment"># (N, T_k)</span></span><br><span class="line">        masks = tf.expand_dims(masks, <span class="number">1</span>) <span class="comment"># (N, 1, T_k)</span></span><br><span class="line">        masks = tf.tile(masks, [<span class="number">1</span>, tf.shape(queries)[<span class="number">1</span>], <span class="number">1</span>]) <span class="comment"># (N, T_q, T_k)</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Apply masks to inputs</span></span><br><span class="line">        paddings = tf.ones_like(inputs) * padding_num</span><br><span class="line">        outputs = tf.where(tf.equal(masks, <span class="number">0</span>), paddings, inputs) <span class="comment"># (N, T_q, T_k)</span></span><br><span class="line">    <span class="keyword">elif</span> type <span class="keyword">in</span> (<span class="string">"q"</span>, <span class="string">"query"</span>, <span class="string">"queries"</span>):</span><br><span class="line">        <span class="comment"># Generate masks</span></span><br><span class="line">        masks = tf.sign(tf.reduce_sum(tf.abs(queries), axis=<span class="number">-1</span>))  <span class="comment"># (N, T_q)</span></span><br><span class="line">        masks = tf.expand_dims(masks, <span class="number">-1</span>)  <span class="comment"># (N, T_q, 1)</span></span><br><span class="line">        masks = tf.tile(masks, [<span class="number">1</span>, <span class="number">1</span>, tf.shape(keys)[<span class="number">1</span>]])  <span class="comment"># (N, T_q, T_k)</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Apply masks to inputs</span></span><br><span class="line">        outputs = inputs*masks</span><br><span class="line">    <span class="keyword">elif</span> type <span class="keyword">in</span> (<span class="string">"f"</span>, <span class="string">"future"</span>, <span class="string">"right"</span>):</span><br><span class="line">        diag_vals = tf.ones_like(inputs[<span class="number">0</span>, :, :])  <span class="comment"># (T_q, T_k)</span></span><br><span class="line">        tril = tf.linalg.LinearOperatorLowerTriangular(diag_vals).to_dense() <span class="comment"># (T_q, T_k)</span></span><br><span class="line">        masks = tf.tile(tf.expand_dims(tril, <span class="number">0</span>), [tf.shape(inputs)[<span class="number">0</span>], <span class="number">1</span>, <span class="number">1</span>]) <span class="comment"># (N, T_q, T_k)</span></span><br><span class="line"></span><br><span class="line">        paddings = tf.ones_like(masks) * padding_num</span><br><span class="line">        outputs = tf.where(tf.equal(masks, <span class="number">0</span>), paddings, inputs)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        print(<span class="string">"Check if you entered type correctly!"</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> outputs</span><br></pre></td></tr></table></figure>
<p>上面是Mask的代码，其中<code>type=&#39;key&#39;</code>和<code>type=&#39;query&#39;</code>对应的是Padding Masking，<code>type=&#39;future&#39;</code>对应的是Sequence masking。</p>
<h3 id="Self-attention和Multihead-attention"><a href="#Self-attention和Multihead-attention" class="headerlink" title="Self-attention和Multihead-attention"></a>Self-attention和Multihead-attention</h3><p>Multihead-attention实际上是由多个Self-attention构成的。</p>
<h4 id="Self-attention"><a href="#Self-attention" class="headerlink" title="Self-attention"></a>Self-attention</h4><p>attention是指，对于某个时刻的输出，它在输入上各个部分的注意力。这个注意力实际上可以理解为权重。attention机制也可以分成很多种。<a href="https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html" target="_blank" rel="noopener">Attention? Attention!</a> 一文有一张比较全面的表格：</p>
<p><img src="attention_mechanism_table.png" alt="attention_mechanism_table"></p>
<p>第一种的additive attention在seq2seq模型里面作为attention机制使用。additive attention对于输入序列隐状态$$h_i​$$和输出序列的隐状态$$s_t​$$直接合并，变成$$[s_t;h_i]​$$，然后加入一层Tanh网络。</p>
<p>但是，Self-attention对这个过程进行了简化，直接使用$$h_i​$$和$$s_t​$$的点乘。而Self是指attention的输出也是attention的输入。论文<a href="https://arxiv.org/abs/1706.03762" target="_blank" rel="noopener">Attention is all you need</a>里面对于attention机制的描述是这样的：</p>
<blockquote>
<p>An attention function can be described as a query and a set of key-value pairs to an output, where the query, keys, values, and output are all vectors. The output is computed as a weighted sum of the values, where the weight assigned to each value is computed by a compatibility of the query with the corresponding key.</p>
</blockquote>
<p>也就是说<strong>通过计算Q和K之间的相似程度来选择V！</strong>下面我们详细介绍上面提到query、key和value。</p>
<p><img src="self-attention_1.png" alt="self-attention_1"></p>
<p>query是输入句子和输出句子的描述，而key和value是对模型的描述。上图清晰地显示了两个单词的query、key和value是如何工作的。在实际实现中，上述计算以矩阵形式完成，以便更快地进行处理。 </p>
<p><img src="self-attention-matrix-calculation.png" alt="self-attention-matrix-calculation"></p>
<p>第一步是计算query，key和value矩阵。我们通过embedding将句子表示为矩阵X，并将其乘以我们训练过的权重矩阵（WQ，WK，WV）来实现。X矩阵中的每一行对应于输入句子中的一个单词。</p>
<p><img src="self-attention-matrix-calculation-2.png" alt="self-attention-matrix-calculation-2"></p>
<p>最后，我们可以在一个公式中浓缩步骤2到6来计算Self-attention层的输出。scaled dot-product attention和dot-product attention唯一的区别就是，scaled dot-product attention有一个缩放因子$$1/\sqrt{d_k}$$。用来防止点积的结果很大，使结果处于softmax梯度很小的位置。</p>
<h4 id="Multihead-attention"><a href="#Multihead-attention" class="headerlink" title="Multihead-attention"></a>Multihead-attention</h4><p><img src="Multihead-attention.png" alt="Multihead-attention"></p>
<p>上图左侧是Self-attention的结构，右边是Multihead-attention的结构。Multihead-attention实际上是Self-attention的一个堆叠（论文中数量是8个），用来增加Self-attention的表达能力。</p>
<p><img src="transformer_multi-headed_self-attention-recap.png" alt="transformer_multi-headed_self-attention-recap"></p>
<p>Multihead-attention的实现为</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">multihead_attention</span><span class="params">(queries, keys, values,</span></span></span><br><span class="line"><span class="function"><span class="params">                        num_heads=<span class="number">8</span>, # head的数量</span></span></span><br><span class="line"><span class="function"><span class="params">                        dropout_rate=<span class="number">0.</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                        training=True,</span></span></span><br><span class="line"><span class="function"><span class="params">                        causality=False, # sequence masking</span></span></span><br><span class="line"><span class="function"><span class="params">                        scope=<span class="string">'multihead_attention'</span>)</span>:</span></span><br><span class="line">    d_model = queries.get_shape().aslist()[<span class="number">-1</span>]</span><br><span class="line">    <span class="keyword">with</span> tf.variable_scope(scope, reuse=tf.AUTO_REUSE):</span><br><span class="line">        <span class="comment"># 添加Q、K、V层，大小为(T_q, d_model)，d_model实际考虑了head的数量。</span></span><br><span class="line">        <span class="comment"># d_model = real_d_model * num_heads = 512 * 8</span></span><br><span class="line">        Q = tf.layers.dense(queries, d_model, use_bias=<span class="literal">False</span>) <span class="comment"># (N, T_q, d_model)</span></span><br><span class="line">        K = tf.layers.dense(keys, d_model, use_bias=<span class="literal">False</span>) <span class="comment"># (N, T_k, d_model)</span></span><br><span class="line">        V = tf.layers.dense(values, d_model, use_bias=<span class="literal">False</span>) <span class="comment"># (N, T_v, d_model)</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Split and concat</span></span><br><span class="line">        Q_ = tf.concat(tf.split(Q, num_heads, axis=<span class="number">2</span>), axis=<span class="number">0</span>) <span class="comment"># (h*N, T_q, d_model/h)</span></span><br><span class="line">        K_ = tf.concat(tf.split(K, num_heads, axis=<span class="number">2</span>), axis=<span class="number">0</span>) <span class="comment"># (h*N, T_k, d_model/h)</span></span><br><span class="line">        V_ = tf.concat(tf.split(V, num_heads, axis=<span class="number">2</span>), axis=<span class="number">0</span>) <span class="comment"># (h*N, T_v, d_model/h)</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Attention</span></span><br><span class="line">        outputs = scaled_dot_product_attention(</span><br><span class="line">            Q_, K_, V_, causality, dropout_rate, training</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Restore shape, (N, T_q, d_model)</span></span><br><span class="line">        outputs = tf.concat([tf.split(outputs, num_heads, axis=<span class="number">0</span>), axis=<span class="number">2</span>])</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Residual connection</span></span><br><span class="line">        outputs += queies</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Normalize</span></span><br><span class="line">        outputs = layer_norm(outputs)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> outputs</span><br></pre></td></tr></table></figure>
<p>最后，我们总结一下query、key和value的用途（Self-attention在Transformer中一共出现在三个地方）：</p>
<ul>
<li>在Encoder的Self-attention中，query、key、value都来自同一个地方（相等），他们是上一层Encoder的输出。对于第一层Encoder，它们就是word embedding和Positional Encoding相加得到的输入。</li>
<li>在Decoder的self-attention中，query、key、value都来自于同一个地方（相等），它们是上一层decoder的输出。对于第一层decoder，它们就是word embedding和positional encoding相加得到的输入。但是对于decoder，我们不希望它能获得下一个time step（即将来的信息），因此我们需要进行sequence masking。</li>
<li>在context attention中，query来自于decoder的上一层的输出，key和value来自于Encoder的输出，key和value是一样的。</li>
<li>key和value的维度一样，而query的维度可以与key、value不同。</li>
</ul>
<p>Scaled dot-product attention的实现为</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">scaled_dot_product_attention</span><span class="params">(Q, K, V,</span></span></span><br><span class="line"><span class="function"><span class="params">                                 causality=False, dropout_rate=<span class="number">0.</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                                 training=True, </span></span></span><br><span class="line"><span class="function"><span class="params">                                 scope=<span class="string">'scaled_dot_product_attention'</span>)</span>:</span></span><br><span class="line">    <span class="keyword">with</span> tf.variable_scope(scope, reuse=tf.AUTO_REUSE):</span><br><span class="line">        d_k = Q.get_shape().as_list()[<span class="number">-1</span>]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Q * K.T</span></span><br><span class="line">        outputs = tf.matmul(Q, tf.transpose(K, [<span class="number">0</span>, <span class="number">2</span>, <span class="number">1</span>])) <span class="comment"># (N, T_q, T_k)</span></span><br><span class="line">        <span class="comment"># (Q * K.T) / sqrt(d_k)</span></span><br><span class="line">        outputs /= d_k ** <span class="number">0.5</span></span><br><span class="line">        <span class="comment"># key masking</span></span><br><span class="line">        outputs = mask(outputs, Q, K, type=<span class="string">'key'</span>)</span><br><span class="line">        <span class="comment"># causality or future blinding masking</span></span><br><span class="line">        <span class="keyword">if</span> causality:</span><br><span class="line">            outputs = mask(outputs, type=<span class="string">'future'</span>)</span><br><span class="line">        <span class="comment"># softmax</span></span><br><span class="line">        outputs = tf.nn.softmax(outputs)</span><br><span class="line">        attention = tf.transpose(outputs, [<span class="number">0</span>, <span class="number">2</span>, <span class="number">1</span>])</span><br><span class="line">        tf.summary.image(<span class="string">'attention'</span>, tf.expand_dims(attention[:<span class="number">1</span>], <span class="number">-1</span>))</span><br><span class="line">        <span class="comment"># query masking</span></span><br><span class="line">        outputs = mask(outputs, Q, K, type=<span class="string">'query'</span>)</span><br><span class="line">        <span class="comment"># dropout</span></span><br><span class="line">        outputs = tf.layers.dropout(outputs, rate=dropout_rate, trianing=training)</span><br><span class="line">        <span class="comment"># weighted sum (context vectors)</span></span><br><span class="line">        outputs = tf.matmul(outputs, V) <span class="comment"># (N, T_q, d_v)</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> outputs</span><br></pre></td></tr></table></figure>
<p>Scaled dot-product attention中包含了Padding Masking和Sequence Masking。首先根据key进行Padding Masking，然后选择是否进行Sequence Masking，最后根据query进行Padding Masking。这里为什么分为两步Padding Masking，还没有搞清楚。</p>
<h3 id="Layer-Normalization"><a href="#Layer-Normalization" class="headerlink" title="Layer Normalization"></a>Layer Normalization</h3><p>各种Normalization的方式已经在<a href="!--￼10--">上一篇文章</a>中有所介绍。复述一下，Batch Normalization不适合Transformer的原因为</p>
<ul>
<li>对batch_size非常敏感。</li>
<li>不能很方便地用于RNN。对于不等长的文本来说，有些很长的输入，其他句子没有这么长，后面的词的均值就是本身。</li>
</ul>
<p><img src="layer_normalization.png" alt="layer_normalization"></p>
<p>为了避免这两个问题，Layer Normalization就应运而生了。Layer Normalization的主要变化在于，不再对batch中的N个样本在各个维度做归一化，而是针对同一层的所有神经元做归一化。</p>
<p>$$\mu = \sum_i{x_i},  \sigma= \sqrt{\sum_i{(x_i-\mu)^2}+\epsilon}$$</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">layer_norm</span><span class="params">(inputs, epsilon=<span class="number">1e-8</span>, scope=<span class="string">'layer_norm'</span>)</span>:</span></span><br><span class="line">    <span class="keyword">with</span> tf.variable_scope(scope, reuse=tf.AUTO_REUSE):</span><br><span class="line">        inputs_shape = inputs.get_shape()</span><br><span class="line">        params_shape = inputs_shape[<span class="number">-1</span>:]</span><br><span class="line"></span><br><span class="line">        mean, var = tf.nn.moments(inputs, [<span class="number">-1</span>], keep_dims=<span class="literal">True</span>) <span class="comment"># 计算均值和方差, C*H*w</span></span><br><span class="line">        beta = tf.get_variable(<span class="string">'beta'</span>, params_shape, initializer=tf.zeros_initializer())</span><br><span class="line">        gamma = tf.get_variable(<span class="string">'gamma'</span>, params_shape, initializer=tf.ones_initialize())</span><br><span class="line">        normalized = (inputs - mean) / ((var + epsilon) ** (<span class="number">.5</span>))</span><br><span class="line">        outputs = gamma * normalized + beta</span><br><span class="line">    <span class="keyword">return</span> outputs</span><br></pre></td></tr></table></figure>
<h3 id="残差结构与Feed-Forward"><a href="#残差结构与Feed-Forward" class="headerlink" title="残差结构与Feed Forward"></a>残差结构与Feed Forward</h3><p><img src="transformer_resideual_layer_norm_2.png" alt="transformer_resideual_layer_norm_2"></p>
<p>残差网络是将输出表述为输入和输入的一个非线性变换的线性叠加，解决了深层网络的训练问题。</p>
<p>每个Encoder中，Multihead-attention后会接一个前馈网络。这个前馈网络其实是两个全连接层，这两层的作用等价于两个 kenerl_size=1的一维卷积操作。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">feed_forward</span><span class="params">(inputs, num_units, scope=<span class="string">"positionwise_feedforward"</span>)</span>:</span></span><br><span class="line">    <span class="keyword">with</span> tf.variable_scope(scope, reuse=tf.AUTO_REUSE):</span><br><span class="line">        <span class="comment"># Inner layer</span></span><br><span class="line">        outputs = tf.layers.dense(inputs, num_units[<span class="number">0</span>], activation=tf.nn.relu)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Outer layer</span></span><br><span class="line">        outputs = tf.layers.dense(outputs, num_units[<span class="number">1</span>])</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Residual connection</span></span><br><span class="line">        outputs += inputs</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Normalize</span></span><br><span class="line">        outputs = ln(outputs)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> outputs</span><br></pre></td></tr></table></figure>
<h3 id="输出的Linear层和Softmax"><a href="#输出的Linear层和Softmax" class="headerlink" title="输出的Linear层和Softmax"></a>输出的Linear层和Softmax</h3><p>Decoder的输出是浮点数向量。我们通过一个线性层和Softmax层将向量转化为单词。线性层是一个简单的全连接网络，它将Decoder产生的向量投影到一个更大的向量中，称为logits向量。</p>
<p>假设我们的模型知道从训练数据集中学到的10,000个独特的英语单词（我们的模型的“输出词汇表”）。所以，logits向量的维度为10,000，每个神经元对应于一个唯一单词的得分。然后，softmax层将这些分数转换为概率（全部为正，全部加起来为1.0），并选择具有最高概率的单元，并且将与其相关联的单词作为该time-step的输出。</p>
<p>需要注意的是，因为embedding矩阵的维度为$$vocab_size \times d_model$$，通常为$$10000 \times 512$$。这个矩阵参数非常多，训练困难。因此，输出的weights矩阵共享输入的单词的embedding矩阵。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Final linear projection (embedding weights are shared)</span></span><br><span class="line">weights = tf.transpose(self.embeddings) <span class="comment"># (d_model, vocab_size)</span></span><br><span class="line">logits = tf.einsum(<span class="string">'ntd,dk-&gt;ntk'</span>, dec, weights) <span class="comment"># (N, T2, vocab_size)</span></span><br><span class="line">y_hat = tf.to_int32(tf.argmax(logits, axis=<span class="number">-1</span>))</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> logits, y_hat, y, sents2</span><br></pre></td></tr></table></figure>
<h2 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h2><blockquote>
<p>在训练过程中，一个未经训练的模型将会通过完全相同的正向传递。但由于我们对标记的训练数据进行训练，我们可以将其的输出与实际正确的输出进行比较。</p>
<p>假设我们输出的词汇仅包括6个单词（a, am, I, thanks, student以及eos<end of sentence的缩写>）。</end></p>
<p><img src="/Users/chaijinlong/Github/chaideblog/hexo/source/_posts/2019-5-4-Transformer/output_vocabulary_1.png" alt="output_vocabulary_1"></p>
<p>一旦定义了输出词汇，我们就可以使用相同宽度的向量来表示词汇表中的每个单词，这被称为one-hot编码。举个例子，我们可以用下图中的向量来表示单词“am”：</p>
<p><img src="/Users/chaijinlong/Github/chaideblog/hexo/source/_posts/2019-5-4-Transformer/output_vocabulary_2.png" alt="output_vocabulary_2"></p>
<p>现在，让我们来讨论这个模型的损失函数——在训练阶段我们优化的指标可用以引导一个训练有素的精确模型。假设我们的第一步是用一个简单的例子进行训练——将“merci”翻译为“thanks”。这意味着，我们希望输出的是表示单词”thanks“的概率分布。但由于这个模型没有经过充分的训练，因此这目前还不可能发生。</p>
<p><img src="/Users/chaijinlong/Github/chaideblog/hexo/source/_posts/2019-5-4-Transformer/output_vocabulary_3.png" alt="output_vocabulary_3"></p>
<p>由于模型的参数是随机初始化的，未经训练的模型对每个单词的任意值产生概率分布。我们可以将其与实际输出进行比较，然后使用反向传播调整所有模型的权重，使输出更接近所需的输出。我们可以通过计算交叉熵和KL散度来衡量两个分布的相似性。</p>
<p>请注意，这只是一个十分简单地例子。实际上，我们会使用一个句子而非一个单词。比如，输入“je suis étudiant”，期望输出“I am a student”。这意味着，我们想要我们的模型连续地输出概率分布，其中：</p>
<ul>
<li>每个概率分布由一个维度等于词汇表大小的向量来表示；</li>
<li>在本例中，第一个概率分布中概率最大的是与”I”相关的维度；</li>
<li>在本例中，第二格概率分布中概率最大的是与”am“相关的维度；</li>
<li>一直重复输出，直到输出的概率分布显示<end of sentence>的符号。</end></li>
</ul>
<p><img src="/Users/chaijinlong/Github/chaideblog/hexo/source/_posts/2019-5-4-Transformer/output_vocabulary_4.png" alt="output_vocabulary_4"></p>
<p>在一个足够大的数据集中对模型进行了足量时间的训练之后，我们生成的概率分布如下图所示：</p>
<p><img src="/Users/chaijinlong/Github/chaideblog/hexo/source/_posts/2019-5-4-Transformer/output_vocabulary_5.png" alt="output_vocabulary_5"></p>
<p>通过训练，该模型将输出我们所期望的正确翻译。请注意，即便不可能在时间步中输出，每个位置依然能获得一点概率，这是softmax一个十分有用的属性，可以改善训练过程。</p>
<p>现在，因为模型每次只能产生一个输出，我们可以假设模型从概率分布中选择了最大概率的单词，并舍弃掉其他单词。这是一种方法（称为贪婪解码）。另外一种方法是选择概率第一第二大的单词（比如，“I”和“a”），然后下一步运行模型两次：第一次假设第一个输出位置的单词为“I”，第二次假设第一个输出位置的单词是“me”，哪个版本的错误更少就保留那一版本的位置1和2。继续重复操作来确定后续的位置。这一方法被称为“beam search”，在我们的例子中beam_size是2（因为我们在计算了位置1和位置2的beams之后比较了结果），top_beam也是2（因为我们保留了2个单词）。你可以对这两个超参数进行实验。</p>
</blockquote>
<h2 id="参考文章"><a href="#参考文章" class="headerlink" title="参考文章"></a>参考文章</h2><ol>
<li><a href="https://jalammar.github.io/illustrated-word2vec/" target="_blank" rel="noopener">Jay Alammar: The Illustrated Word2vec</a></li>
<li><a href="https://lonepatient.top/2019/01/17/BERT-Transformer.html" target="_blank" rel="noopener">eamlife’s blog：Transformer原理和实现</a></li>
<li><a href="https://lonepatient.top/2019/01/09/BERT-self-Attention.html" target="_blank" rel="noopener">eamlife’s blog：self—attention</a></li>
<li><a href="https://github.com/Kyubyong/transformer" target="_blank" rel="noopener">Kyubyong: transformer</a></li>
<li><a href="https://luozhouyang.github.io/transformer/" target="_blank" rel="noopener">罗周杨：Transformer模型的PyTorch实现</a></li>
<li><a href="https://mp.weixin.qq.com/s/QeUh44tV9KzsmF-j13-zqA" target="_blank" rel="noopener">立夏：35张图带你了解Transformer</a></li>
</ol>
</div><div class="tags"><a href="/tags/Natural-Language-Processing/">Natural Language Processing</a></div><div class="post-nav"><a class="pre" href="/2019-5-5-Paper-TEM/">基于决策树的可解释嵌入推荐模型：TEM</a><a class="next" href="/2019-4-28-Normalization/">深度学习中各种Normalization</a></div></div></div></div><div class="pure-u-1-4 hidden_mid_and_down"><div id="sidebar"><div class="widget"><form class="search-form" action="//www.google.com/search" method="get" accept-charset="utf-8" target="_blank"><input type="text" name="q" maxlength="20" placeholder="Search"><input type="hidden" name="sitesearch" value="https://chaideblog.github.io"></form></div><div class="widget"><div class="widget-title"><i class="fa fa-folder-o"> 分类</i></div><ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/Algorithm/">Algorithm</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/BigData/">BigData</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Deep-Learning/">Deep Learning</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Interview/">Interview</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Java/">Java</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Linux/">Linux</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Machine-Learning/">Machine Learning</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Math/">Math</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Python/">Python</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-star-o"> 标签</i></div><div class="tagcloud"><a href="/tags/Probability-theroy/" style="font-size: 15px;">Probability theroy</a> <a href="/tags/Genetic-Algorithm/" style="font-size: 15px;">Genetic Algorithm</a> <a href="/tags/决策树/" style="font-size: 15px;">决策树</a> <a href="/tags/Python语法/" style="font-size: 15px;">Python语法</a> <a href="/tags/VPS/" style="font-size: 15px;">VPS</a> <a href="/tags/Convex-Optimize/" style="font-size: 15px;">Convex Optimize</a> <a href="/tags/Naive-Bayes/" style="font-size: 15px;">Naive Bayes</a> <a href="/tags/Sublime/" style="font-size: 15px;">Sublime</a> <a href="/tags/Random-Forests/" style="font-size: 15px;">Random Forests</a> <a href="/tags/集成学习/" style="font-size: 15px;">集成学习</a> <a href="/tags/Hadoop/" style="font-size: 15px;">Hadoop</a> <a href="/tags/Java/" style="font-size: 15px;">Java</a> <a href="/tags/FFT/" style="font-size: 15px;">FFT</a> <a href="/tags/Apache/" style="font-size: 15px;">Apache</a> <a href="/tags/树莓派/" style="font-size: 15px;">树莓派</a> <a href="/tags/SVM/" style="font-size: 15px;">SVM</a> <a href="/tags/SGD/" style="font-size: 15px;">SGD</a> <a href="/tags/BGD/" style="font-size: 15px;">BGD</a> <a href="/tags/MBGD/" style="font-size: 15px;">MBGD</a> <a href="/tags/SMO/" style="font-size: 15px;">SMO</a> <a href="/tags/标准化/" style="font-size: 15px;">标准化</a> <a href="/tags/回归/" style="font-size: 15px;">回归</a> <a href="/tags/CTR/" style="font-size: 15px;">CTR</a> <a href="/tags/LeetCode/" style="font-size: 15px;">LeetCode</a> <a href="/tags/LightGBM/" style="font-size: 15px;">LightGBM</a> <a href="/tags/RCNN/" style="font-size: 15px;">RCNN</a> <a href="/tags/目标检测/" style="font-size: 15px;">目标检测</a> <a href="/tags/Interview/" style="font-size: 15px;">Interview</a> <a href="/tags/LR/" style="font-size: 15px;">LR</a> <a href="/tags/FM-FFM/" style="font-size: 15px;">FM/FFM</a> <a href="/tags/Recommend-System/" style="font-size: 15px;">Recommend System</a> <a href="/tags/Natural-Language-Processing/" style="font-size: 15px;">Natural Language Processing</a> <a href="/tags/Recommendation/" style="font-size: 15px;">Recommendation</a></div></div><div class="widget"><div class="widget-title"><i class="fa fa-file-o"> 最近文章</i></div><ul class="post-list"><li class="post-list-item"><a class="post-list-link" href="/2019-5-5-Paper-TEM/">基于决策树的可解释嵌入推荐模型：TEM</a></li><li class="post-list-item"><a class="post-list-link" href="/2019-5-4-Transformer/">Transformer理解与Tensorflow代码阅读</a></li><li class="post-list-item"><a class="post-list-link" href="/2019-4-28-Normalization/">深度学习中各种Normalization</a></li><li class="post-list-item"><a class="post-list-link" href="/2019-4-22-recommend-system-1/">《推荐系统实战》读书笔记（一）</a></li><li class="post-list-item"><a class="post-list-link" href="/2019-4-6-toutiao-interview/">字节跳动广告算法团队实习面试经历（Offer）</a></li><li class="post-list-item"><a class="post-list-link" href="/2019-3-13-xiaomi-interview/">小米信息流推荐实习面试经历（Offer）</a></li><li class="post-list-item"><a class="post-list-link" href="/2019-3-8-sohu-interview/">搜狐推荐算法实习面试经历（Offer）</a></li><li class="post-list-item"><a class="post-list-link" href="/2019-2-27-CTR-LR-FM-FFM/">CTR系列（一）：LR、FM/FFM</a></li><li class="post-list-item"><a class="post-list-link" href="/2019-2-21-ms-bing-interview/">微软Bing组算法（Game Over）</a></li><li class="post-list-item"><a class="post-list-link" href="/2019-1-8-PriorityQueue/">【转】Java容器源码分析之PriorityQueue</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-external-link"> 友情链接</i></div><ul></ul><a href="http://www.example1.com/" title="site-name1" target="_blank">site-name1</a><ul></ul><a href="http://www.example2.com/" title="site-name2" target="_blank">site-name2</a><ul></ul><a href="http://www.example3.com/" title="site-name3" target="_blank">site-name3</a></div></div></div><div class="pure-u-1 pure-u-md-3-4"><div id="footer">Copyright © 2019 <a href="/." rel="nofollow">CHAI' blog.</a> Powered by<a rel="nofollow" target="_blank" href="https://hexo.io"> Hexo.</a><a rel="nofollow" target="_blank" href="https://github.com/tufu9441/maupassant-hexo"> Theme</a> by<a rel="nofollow" target="_blank" href="https://github.com/pagecho"> Cho.</a></div></div></div><a class="show" id="rocket" href="#top"></a><script type="text/javascript" src="/js/totop.js?v=0.0.0" async></script><script type="text/javascript" src="//lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.js" async></script><script type="text/javascript" src="/js/fancybox.js?v=0.0.0" async></script><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.css"><script type="text/x-mathjax-config">MathJax.Hub.Config({
  tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}
  });
</script><script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML" async></script><script type="text/javascript" src="/js/codeblock-resizer.js?v=0.0.0"></script><script type="text/javascript" src="/js/smartresize.js?v=0.0.0"></script></div></body></html>