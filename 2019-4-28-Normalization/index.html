<!DOCTYPE html><html lang="zh-CN"><head><meta name="generator" content="Hexo 3.8.0"><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description"><title>深度学习中各种Normalization | CHAI' blog</title><link rel="stylesheet" type="text/css" href="/css/style.css?v=0.0.0"><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/normalize/8.0.1/normalize.min.css"><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/pure/1.0.0/pure-min.css"><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/pure/1.0.0/grids-responsive-min.css"><link rel="stylesheet" href="//lib.baomitu.com/font-awesome/4.7.0/css/font-awesome.min.css"><script type="text/javascript" src="//lib.baomitu.com/jquery/3.4.0/jquery.min.js"></script><link rel="icon" mask sizes="any" href="/favicon.ico"><link rel="Shortcut Icon" type="image/x-icon" href="/favicon.ico"><link rel="apple-touch-icon" href="/apple-touch-icon.png"><link rel="apple-touch-icon-precomposed" href="/apple-touch-icon.png"></head><body><div class="body_container"><div id="header"><div class="site-name"><h1 class="hidden">深度学习中各种Normalization</h1><a id="logo" href="/.">CHAI' blog</a><p class="description"></p></div><div id="nav-menu"><a class="current" href="/."><i class="fa fa-home"> 首页</i></a><a href="/archives/"><i class="fa fa-archive"> 归档</i></a><a href="/about/"><i class="fa fa-user"> 关于</i></a><a href="/atom.xml"><i class="fa fa-rss"> 订阅</i></a></div></div><div class="pure-g" id="layout"><div class="pure-u-1 pure-u-md-3-4"><div class="content_container"><div class="post"><h1 class="post-title">深度学习中各种Normalization</h1><div class="post-meta">Apr 28, 2019<span> | </span><span class="category"><a href="/categories/Machine-Learning/">Machine Learning</a></span></div><div class="clear"><div class="toc-article" id="toc"><div class="toc-title">文章目录</div><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#写在前面"><span class="toc-number">1.</span> <span class="toc-text">写在前面</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#为什么需要Normalization"><span class="toc-number">2.</span> <span class="toc-text">为什么需要Normalization</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#独立同分布与白化"><span class="toc-number">2.1.</span> <span class="toc-text">独立同分布与白化</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#深度学习中的Internal-Covariate-Shift"><span class="toc-number">2.2.</span> <span class="toc-text">深度学习中的Internal Covariate Shift</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Normalization的通用框架与主流方法"><span class="toc-number">3.</span> <span class="toc-text">Normalization的通用框架与主流方法</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#各种各样的Normalization"><span class="toc-number">4.</span> <span class="toc-text">各种各样的Normalization</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Batch-Normalization"><span class="toc-number">4.1.</span> <span class="toc-text">Batch Normalization</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Layer-Normalization"><span class="toc-number">4.2.</span> <span class="toc-text">Layer Normalization</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Instance-Normalization"><span class="toc-number">4.3.</span> <span class="toc-text">Instance Normalization</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Group-Normalization"><span class="toc-number">4.4.</span> <span class="toc-text">Group Normalization</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Weight-Normalization"><span class="toc-number">4.5.</span> <span class="toc-text">Weight Normalization</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Cosine-Normalization"><span class="toc-number">4.6.</span> <span class="toc-text">Cosine Normalization</span></a></li></ol></li></ol></div></div><div class="post-content"><h2 id="写在前面"><a href="#写在前面" class="headerlink" title="写在前面"></a>写在前面</h2><p>本文是一些文章的总结，大量借鉴了其他博主的文章，因此先将其他人的文章链接给出：</p>
<ol>
<li><a href="https://yifdu.github.io/2018/12/26/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%AD%E5%90%84%E7%A7%8DNormalization/" target="_blank" rel="noopener">深度菜鸟：深度学习中各种Normalization</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/33173246" target="_blank" rel="noopener">Juliuszh：详解深度学习中的Normalization，BN/LN/WN</a></li>
<li><a href="http://skyhigh233.com/blog/2017/07/21/norm/" target="_blank" rel="noopener">SkyHigh：加速网络收敛——BN、LN、WN与selu</a></li>
</ol>
<h2 id="为什么需要Normalization"><a href="#为什么需要Normalization" class="headerlink" title="为什么需要Normalization"></a>为什么需要Normalization</h2><h3 id="独立同分布与白化"><a href="#独立同分布与白化" class="headerlink" title="独立同分布与白化"></a>独立同分布与白化</h3><p>独立同分布并非所有机器学习模型的必然要求（比如Naive Bayes模型就建立在特征彼此独立的基础之上，而Logistic Regression和神经网络则在非独立的特征数据上依然可以训练出很好的模型），但独立同分布的数据可以简化常规机器学习模型的训练，提升机器学习模型的预测能力，这已经是一个共识了。</p>
<p>在把数据喂给机器学习模型之前，白化（whitening）是一个重要的数据预处理步骤，白化一般包含两个目的：</p>
<ol>
<li>去除特征之间的相关性——独立；</li>
<li>使得所有特征具有相同的均值和方差——同分布</li>
</ol>
<p>白化最经典的方法就是PCA。</p>
<h3 id="深度学习中的Internal-Covariate-Shift"><a href="#深度学习中的Internal-Covariate-Shift" class="headerlink" title="深度学习中的Internal Covariate Shift"></a>深度学习中的Internal Covariate Shift</h3><p>深度神经网络模型的训练为什么会很困难？其中一个重要的原因是，深度神经网络涉及到很多层的叠加，而每一层的参数更新会导致上一层输入数据分布发生变化，通过层层叠加，高层的输入分布会非常剧烈，这就使得高层需要不断去重新适应底层的参数更新。Google将这一现象总结为内部协变量转换(Internal Covariate Shift, ICS)。</p>
<p>在统计机器学习中的一个经典假设是源空间和目标空间的数据分布是一致的。如果不一致，那就会出现新的机器学习问题，如transfer learning、domain adaptation等。</p>
<p>而covariate shift就是分布不一致假设之下的一个分支问题，它是指源空间和目标空间的条件概率是一致的，但其边缘概率不同，即：对所有$$x \in X$$</p>
<p>$$P_s(Y \vert X=x) = P_t (Y \vert X=x)$$</p>
<p>但是$$P_s(X) \ne P_t(X)$$，对于神经网络的各层输出，由于他们经过了层内操作作用，各层的输入信号的分布显然不同，而且差异会随着网络深度增大而增大，可是他们所能”指示”的样本标记(label)仍然是不变的，这便符合了covariate shift的定义。</p>
<p>简而言之，每个神经元的输入数据不再是”独立同分布”</p>
<ol>
<li>上层参数需要不断适应新的输入数据分布，降低了学习速度</li>
<li>下层输入的变化可能趋向于变大或者变小，导致上层落入饱和区，使得学习过早停止</li>
<li>每层的更新都会影响到其他层，因此每层的参数更新策略需要尽可能谨慎</li>
</ol>
<h2 id="Normalization的通用框架与主流方法"><a href="#Normalization的通用框架与主流方法" class="headerlink" title="Normalization的通用框架与主流方法"></a>Normalization的通用框架与主流方法</h2><p>以神经网络中的一个普通神经元为例。神经元接收一组输入向量$$x=(x_1, x_2, \cdots, x_d)​$$通过某种运算后，输出一个标量值：</p>
<p>$$y=f(x)$$</p>
<p>由于ICS问题的存在，$$x$$的分布可能相差很大。要解决独立同分布的问题，“理论正确”的方法就是对每一层的数据都进行白化操作。然而标准的白化操作代价高昂，特别是我们还希望白化操作是可微的，保证白化操作可以通过反向传播来更新梯度。</p>
<p>因此，以BN为代表的Normalization方法退而求其次，进行了简化的白化操作。基本思想是：在将$$x$$送给神经元之前，先对其做<strong>平移和伸缩变换</strong>， 将$$x$$的分布规范化成在固定区间范围的标准分布。</p>
<p>通用变换框架就如下所示：</p>
<p>$$h=f\left(g\cdot\frac{x-\mu}{\sigma}+b\right)$$</p>
<p>这个公式中的各个参数：</p>
<ol>
<li><p>$$\mu$$是<strong>平移参数</strong>（shift parameter），$$\sigma$$是<strong>缩放参数</strong>（scale parameter）。通过这两个参数进行shift和scale变换：$$\hat{x}=\frac{x-\mu}{\sigma}$$得到的数据符合均值为0、方差为1的标准分布。</p>
</li>
<li><p>$$b$$是<strong>再平移参数</strong>（re-shift parameter），$$g$$是<strong>再缩放参数</strong>（re-scale parameter）。将上一步得到的$$\hat{x}$$进一步变换为：$$y=g \cdot \hat{x} + b$$。</p>
</li>
</ol>
<p>最终得到的数据符合均值为$$b​$$、方差为$$g^2​$$的分布。</p>
<h2 id="各种各样的Normalization"><a href="#各种各样的Normalization" class="headerlink" title="各种各样的Normalization"></a>各种各样的Normalization</h2><p><img src="Norm.png" alt="Norm"></p>
<p>从左到右一次是BN、LN、IN、GN。深度网络中的数据维度一般是[N, C, H, W]或者[N, H, W，C]格式，N是batch size，H/W是feature的高/宽，C是feature的channel，压缩H/W至一个维度，其三维的表示如上图！</p>
<p>四种Normalization的工作方式：</p>
<ul>
<li>BN主要在Batch的维度上Norm，归一化维度为$$N \times H \times W$$，对batch中对应的channel归一化；</li>
<li>LN避开了batch维度，归一化的维度为$$C \times H \times W$$；</li>
<li>IN归一化的维度为$$H \times W$$；</li>
<li>而GN介于LN与IN之间，其首先将channel分为许多组（group）,对每一组做归一化，及先将feature的维度由[N,C,H,W]reshape为[N,G,C//G,H,W]，归一化的维度为$$C//G \times H \times W$$。</li>
</ul>
<h3 id="Batch-Normalization"><a href="#Batch-Normalization" class="headerlink" title="Batch Normalization"></a>Batch Normalization</h3><p>Batch Normalization于2015年由Google提出，开Normalization之先河。其规范化针对单个神经元进行，利用网络训练时一个mini-batch的数据来计算该神经元$$x_i$$的均值和方差,因而称为Batch Normalization。</p>
<p>$$\mu_i = \frac{1}{M} \sum{x_i}, \sigma_i= \sqrt{\frac{1}{M} \sum{(x_i - \mu_i)^2} + \epsilon}$$</p>
<p>其中$$M$$是mini-batch的大小。</p>
<p>BN是在batch这个维度上的Normalization，但是这个维度并不是固定不变的，比如训练和测试时一般不一样，一般都是训练的时候在训练集上通过滑动平均，预先计算好均值mean和方差variance,在测试的时候不再计算这些值，而是直接调用这些预先计算好的平均mean和方差variance参数。但是，当训练数据和测试数据分布有差别时，训练时预计算好的数据并不能代表测试数据，这就导致在训练、验证、测试这三个阶段存在inconsistency。</p>
<p>此外过小的batchsize会导致其性能下降，一般来说每GPU上batch设为32最合适，但是对于一些其他深度学习任务batchsize往往只有1-2，比如目标检测、图像分割、视频分类上，输入的图像数据很大，较大的batchsize显存吃不消。</p>
<p>明确了问题，解决时就考虑在归一化的时候避开batch这个维度是不是可行呢，于是就出现了layer normalization和instance normalization等工作。</p>
<h3 id="Layer-Normalization"><a href="#Layer-Normalization" class="headerlink" title="Layer Normalization"></a>Layer Normalization</h3><p>$$\mu = \sum_i{x_i},  \sigma= \sqrt{\sum_i{(x_i-\mu)^2}+\epsilon}$$</p>
<p>其中$$i$$枚举了该层所有的输入神经元。对应到标准公式中，四大参数$$\mu$$、$$\sigma$$、$$g$$、$$b$$均为标量（BN中是向量），所有输入共享一个规范化变换。</p>
<p>LN针对单个训练样本进行，不依赖于其他数据，因此可以避免BN中受mini-batch数据分布影响问题，可以用于小mini-batch场景、动态网络场景和RNN，特别是自然语言处理领域。此外，LN不需要保存mini-batch的均值和方差，节省了额外的存储空间</p>
<p>但是，BN的转换是针对单个神经元可训练的——不同神经元的输入经过再平移和再放缩后分布在不同的区间，而LN对于一整层的神经元训练得到同一个转换——所有的输入都在同一个区间范围内。如果不同输入特征不属于相似的类别(比如颜色和大小)，那么LN的处理可能会降低模型的表达能力。</p>
<h3 id="Instance-Normalization"><a href="#Instance-Normalization" class="headerlink" title="Instance Normalization"></a>Instance Normalization</h3><p>在GAN和style transfer的任务中，目前的IN要好于BN，IN主要用于对单张图像的数据做处理，而BN主要对Batch的数据做处理。由于BN在训练时每个batch的均值和方差会由于shuffle都会改变,所以可以理解为一种数据增强，而IN可以理解为对数据做一个归一化的操作。</p>
<p>换句话说，BN的计算是要受其他样本影响的，由于每个batch的均值和标准差不稳定，对于单个数据而言，相对于是引入了噪声，但在分类这种问题上，结果和数据的整体分布有关系，因此需要通过BN获得数据的整体分布。而IN的信息都是来自于图片自身，相当于对全局信息做了一次整合和调整，在图像转换这种问题上，BN获得的整体信息不会带来任何收益，带来的噪声反而会弱化实例之间的独立性：这类生成式方法每张图片自己的风格比较独立不应该与batch中其他的样本产生太大联系。</p>
<h3 id="Group-Normalization"><a href="#Group-Normalization" class="headerlink" title="Group Normalization"></a>Group Normalization</h3><p>group normalization优化了BN在较小的mini-batch情况下表现不太好的劣质。批量维度进行归一化会带来一些问题——批量统计估算不准确导致批量变小时，BN的误差会迅速增加。在训练大型网络和将特征转移到计算机视觉任务中(包括检测、分割和视频)，内存消耗限制了只能使用小批量的BN。事实上，GN的极端情况就是LN和IN</p>
<p>在深度学习没有火起来之前，提取特征通常是使用SIFT、HOG和GIST特征，这些特征有一个共性，都具有按group表示的特性，每一个group由相同种类直方图的构建而成，这些特征通常是对在每个直方图（histogram）或每个方向（orientation）上进行组归一化（group-wise norm）而得到。</p>
<p>从深度学习上来讲，完全可以认为卷积提取的特征是一种非结构化的特征或者向量，拿网络的第一层卷积为例，卷积层中的的卷积核filter1和此卷积核的其他经过transform过的版本filter2（transform可以是horizontal flipping等），在同一张图像上学习到的特征应该是具有相同的分布，那么，具有相同的特征可以被分到同一个group中，按照个人理解，每一层有很多的卷积核，这些核学习到的特征并不完全是独立的，某些特征具有相同的分布，因此可以被group。</p>
<h3 id="Weight-Normalization"><a href="#Weight-Normalization" class="headerlink" title="Weight Normalization"></a>Weight Normalization</h3><p>前面我们讲的模型框架$$h=f\left(g\cdot\frac{x-\mu}{\sigma}+b\right)$$中，经过规范化之后的$$y$$作为输入送到下一个神经元，应用以$$w$$为参数的$$f_w(\cdot)$$函数定义的变换。最普遍的变换是线性变换，即</p>
<p>$$f_w(x)=w \cdot x$$</p>
<p>BN和LN均将规范化应用于输入的特征数据$$x$$，而WN则另辟蹊径，将规范化应用于线性变换函数的权重$$w$$，这就是WN名称的来源。具体而言，WN 提出的方案是，<strong>将权重向量$$w$$两部分</strong>：</p>
<p>$${w} = g \cdot \hat{v} = g \cdot \frac{v}{\vert \vert v \vert \vert}​$$</p>
<p>其中$$v​$$，因此这一权重分解的方式将权重向量的欧氏范数进行了固定，从而实现了正则化的效果。</p>
<p><strong>乍一看，这一方法似乎脱离了我们前文所讲的通用框架？</strong></p>
<p>并没有。其实从最终实现的效果来看，异曲同工。我们来推导一下看。</p>
<p>$$f_w(WN(x))=w\cdot WN(x) = g\cdot\frac{v}{\vert \vert v \vert \vert} \cdot x \= v\cdot g\cdot\frac{x}{\vert \vert v \vert \vert}=f_v(g\cdot\frac{x}{\vert \vert v \vert \vert})$$</p>
<p>对照一下前述框架：</p>
<p>$$h=f\left(g\cdot\frac{x-\mu}{\sigma}+b\right)$$</p>
<p>我们只需令：</p>
<p>$$\sigma = { \vert \vert v \vert \vert},  \mu=0,  b=0$$</p>
<p>就完美地对号入座了！</p>
<p>回忆一下，BN和LN是用输入的特征数据的方差对输入数据进行scale，而WN则是用神经元的权重的欧氏范式对输入数据进行scale。<strong>虽然在原始方法中分别进行的是特征数据规范化和参数的规范化，但本质上都实现了对数据的规范化，只是用于 scale 的参数来源不同。</strong></p>
<p>另外，我们看到这里的规范化只是对数据进行了scale，而没有进行shift，因为我们简单地令$$\mu=0$$。</p>
<p>WN的规范化不直接使用输入数据的统计量，因此避免了BN过于依赖mini-batch的不足，以及LN每层唯一转换器的限制，同时也可以用于动态网络结构。</p>
<h3 id="Cosine-Normalization"><a href="#Cosine-Normalization" class="headerlink" title="Cosine Normalization"></a>Cosine Normalization</h3><p>我们要对数据进行规范化的原因，是数据经过神经网络的计算之后可能会变得很大，导致数据分布的方差爆炸，而这一问题的根源就是我们的计算方式——点积，权重向量$$w$$和特征数据向量$$x$$的点积。</p>
<p>我们知道向量点积是衡量两个向量相似度的方法之一。哪还有没有其他的相似度衡量方法呢？夹角余弦就是其中之一啊！而且关键的是，夹角余弦是有确定界的啊，$$[-1, 1]$$的取值范围。</p>
<p>Cosine Normalization不处理权重向量$$w$$，也不处理特征数据向量$$x$$，就改了一下线性变换的函数：</p>
<p>$$f_w(x) = \cos \theta = \frac{w \cdot x}{\vert  \vert w \vert  \vert \cdot \vert  \vert x \vert  \vert}$$</p>
<p>其中$$\theta$$是$$w$$和$$x$$的夹角，所有的数据就都是$$[-1, 1]$$区间范围之内的了！</p>
<p>CN通过用余弦计算代替内积计算实现了规范化。原始的内积计算，其几何意义是输入向量在权重向量上的投影，既包含二者的夹角信息，也包含两个向量的scale信息。去掉scale信息，可能导致表达能力的下降，因此也引起了一些争议和讨论。具体效果如何，可能需要在特定的场景下深入实验。</p>
</div><div class="tags"><a href="/tags/Recommend-System/">Recommend System</a></div><div class="post-nav"><a class="pre" href="/2019-5-4-Transformer/">Transformer理解与Tensorflow代码阅读</a><a class="next" href="/2019-4-22-recommend-system-1/">《推荐系统实战》读书笔记（一）</a></div></div></div></div><div class="pure-u-1-4 hidden_mid_and_down"><div id="sidebar"><div class="widget"><form class="search-form" action="//www.google.com/search" method="get" accept-charset="utf-8" target="_blank"><input type="text" name="q" maxlength="20" placeholder="Search"><input type="hidden" name="sitesearch" value="https://chaideblog.github.io"></form></div><div class="widget"><div class="widget-title"><i class="fa fa-folder-o"> 分类</i></div><ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/Algorithm/">Algorithm</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/BigData/">BigData</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Deep-Learning/">Deep Learning</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Interview/">Interview</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Java/">Java</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Linux/">Linux</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Machine-Learning/">Machine Learning</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Math/">Math</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Python/">Python</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-star-o"> 标签</i></div><div class="tagcloud"><a href="/tags/Probability-theroy/" style="font-size: 15px;">Probability theroy</a> <a href="/tags/Genetic-Algorithm/" style="font-size: 15px;">Genetic Algorithm</a> <a href="/tags/决策树/" style="font-size: 15px;">决策树</a> <a href="/tags/Python语法/" style="font-size: 15px;">Python语法</a> <a href="/tags/VPS/" style="font-size: 15px;">VPS</a> <a href="/tags/Convex-Optimize/" style="font-size: 15px;">Convex Optimize</a> <a href="/tags/Naive-Bayes/" style="font-size: 15px;">Naive Bayes</a> <a href="/tags/Sublime/" style="font-size: 15px;">Sublime</a> <a href="/tags/Random-Forests/" style="font-size: 15px;">Random Forests</a> <a href="/tags/集成学习/" style="font-size: 15px;">集成学习</a> <a href="/tags/Hadoop/" style="font-size: 15px;">Hadoop</a> <a href="/tags/Java/" style="font-size: 15px;">Java</a> <a href="/tags/FFT/" style="font-size: 15px;">FFT</a> <a href="/tags/Apache/" style="font-size: 15px;">Apache</a> <a href="/tags/树莓派/" style="font-size: 15px;">树莓派</a> <a href="/tags/SVM/" style="font-size: 15px;">SVM</a> <a href="/tags/SGD/" style="font-size: 15px;">SGD</a> <a href="/tags/BGD/" style="font-size: 15px;">BGD</a> <a href="/tags/MBGD/" style="font-size: 15px;">MBGD</a> <a href="/tags/SMO/" style="font-size: 15px;">SMO</a> <a href="/tags/标准化/" style="font-size: 15px;">标准化</a> <a href="/tags/回归/" style="font-size: 15px;">回归</a> <a href="/tags/CTR/" style="font-size: 15px;">CTR</a> <a href="/tags/LeetCode/" style="font-size: 15px;">LeetCode</a> <a href="/tags/LightGBM/" style="font-size: 15px;">LightGBM</a> <a href="/tags/RCNN/" style="font-size: 15px;">RCNN</a> <a href="/tags/目标检测/" style="font-size: 15px;">目标检测</a> <a href="/tags/Interview/" style="font-size: 15px;">Interview</a> <a href="/tags/LR/" style="font-size: 15px;">LR</a> <a href="/tags/FM-FFM/" style="font-size: 15px;">FM/FFM</a> <a href="/tags/Recommend-System/" style="font-size: 15px;">Recommend System</a> <a href="/tags/Natural-Language-Processing/" style="font-size: 15px;">Natural Language Processing</a> <a href="/tags/Recommendation/" style="font-size: 15px;">Recommendation</a></div></div><div class="widget"><div class="widget-title"><i class="fa fa-file-o"> 最近文章</i></div><ul class="post-list"><li class="post-list-item"><a class="post-list-link" href="/2019-5-5-Paper-TEM/">基于决策树的可解释嵌入推荐模型：TEM</a></li><li class="post-list-item"><a class="post-list-link" href="/2019-5-4-Transformer/">Transformer理解与Tensorflow代码阅读</a></li><li class="post-list-item"><a class="post-list-link" href="/2019-4-28-Normalization/">深度学习中各种Normalization</a></li><li class="post-list-item"><a class="post-list-link" href="/2019-4-22-recommend-system-1/">《推荐系统实战》读书笔记（一）</a></li><li class="post-list-item"><a class="post-list-link" href="/2019-4-6-toutiao-interview/">字节跳动广告算法团队实习面试经历（Offer）</a></li><li class="post-list-item"><a class="post-list-link" href="/2019-3-13-xiaomi-interview/">小米信息流推荐实习面试经历（Offer）</a></li><li class="post-list-item"><a class="post-list-link" href="/2019-3-8-sohu-interview/">搜狐推荐算法实习面试经历（Offer）</a></li><li class="post-list-item"><a class="post-list-link" href="/2019-2-27-CTR-LR-FM-FFM/">CTR系列（一）：LR、FM/FFM</a></li><li class="post-list-item"><a class="post-list-link" href="/2019-2-21-ms-bing-interview/">微软Bing组算法（Game Over）</a></li><li class="post-list-item"><a class="post-list-link" href="/2019-1-8-PriorityQueue/">【转】Java容器源码分析之PriorityQueue</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-external-link"> 友情链接</i></div><ul></ul><a href="http://www.example1.com/" title="site-name1" target="_blank">site-name1</a><ul></ul><a href="http://www.example2.com/" title="site-name2" target="_blank">site-name2</a><ul></ul><a href="http://www.example3.com/" title="site-name3" target="_blank">site-name3</a></div></div></div><div class="pure-u-1 pure-u-md-3-4"><div id="footer">Copyright © 2019 <a href="/." rel="nofollow">CHAI' blog.</a> Powered by<a rel="nofollow" target="_blank" href="https://hexo.io"> Hexo.</a><a rel="nofollow" target="_blank" href="https://github.com/tufu9441/maupassant-hexo"> Theme</a> by<a rel="nofollow" target="_blank" href="https://github.com/pagecho"> Cho.</a></div></div></div><a class="show" id="rocket" href="#top"></a><script type="text/javascript" src="/js/totop.js?v=0.0.0" async></script><script type="text/javascript" src="//lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.js" async></script><script type="text/javascript" src="/js/fancybox.js?v=0.0.0" async></script><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.css"><script type="text/x-mathjax-config">MathJax.Hub.Config({
  tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}
  });
</script><script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML" async></script><script type="text/javascript" src="/js/codeblock-resizer.js?v=0.0.0"></script><script type="text/javascript" src="/js/smartresize.js?v=0.0.0"></script></div></body></html>