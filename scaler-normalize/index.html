<!DOCTYPE html><html lang="zh-CN"><head><meta name="generator" content="Hexo 3.8.0"><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description"><title>标准化/归一化方法 | CHAI' blog</title><link rel="stylesheet" type="text/css" href="/css/style.css?v=0.0.0"><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/normalize/8.0.1/normalize.min.css"><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/pure/1.0.0/pure-min.css"><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/pure/1.0.0/grids-responsive-min.css"><link rel="stylesheet" href="//lib.baomitu.com/font-awesome/4.7.0/css/font-awesome.min.css"><script type="text/javascript" src="//lib.baomitu.com/jquery/3.4.0/jquery.min.js"></script><link rel="icon" mask sizes="any" href="/favicon.ico"><link rel="Shortcut Icon" type="image/x-icon" href="/favicon.ico"><link rel="apple-touch-icon" href="/apple-touch-icon.png"><link rel="apple-touch-icon-precomposed" href="/apple-touch-icon.png"></head><body><div class="body_container"><div id="header"><div class="site-name"><h1 class="hidden">标准化/归一化方法</h1><a id="logo" href="/.">CHAI' blog</a><p class="description"></p></div><div id="nav-menu"><a class="current" href="/."><i class="fa fa-home"> 首页</i></a><a href="/archives/"><i class="fa fa-archive"> 归档</i></a><a href="/about/"><i class="fa fa-user"> 关于</i></a><a href="/atom.xml"><i class="fa fa-rss"> 订阅</i></a></div></div><div class="pure-g" id="layout"><div class="pure-u-1 pure-u-md-3-4"><div class="content_container"><div class="post"><h1 class="post-title">标准化/归一化方法</h1><div class="post-meta">Jul 27, 2018<span> | </span><span class="category"><a href="/categories/Machine-Learning/">Machine Learning</a></span></div><div class="clear"><div class="toc-article" id="toc"><div class="toc-title">文章目录</div><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#标准化"><span class="toc-number">1.</span> <span class="toc-text">标准化</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#scale"><span class="toc-number">1.1.</span> <span class="toc-text">scale</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#StandardScaler"><span class="toc-number">1.2.</span> <span class="toc-text">StandardScaler</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#MinMaxScaler"><span class="toc-number">1.3.</span> <span class="toc-text">MinMaxScaler</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#MaxAbsScaler"><span class="toc-number">1.4.</span> <span class="toc-text">MaxAbsScaler</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#缩放稀疏（矩阵）数据"><span class="toc-number">1.5.</span> <span class="toc-text">缩放稀疏（矩阵）数据</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#缩放有离群值的数据"><span class="toc-number">1.6.</span> <span class="toc-text">缩放有离群值的数据</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#归一化"><span class="toc-number">2.</span> <span class="toc-text">归一化</span></a></li></ol></div></div><div class="post-content"><p>标准化是将数据去均值和方差按比例缩放，而归一化是指缩放单个样本以具有单位范数的过程，两者稍微有些区别。</p>
<h2 id="标准化"><a href="#标准化" class="headerlink" title="标准化"></a>标准化</h2><p>数据集的<strong>标准化</strong>对大多数机器学习算法来说是<strong>常见的要求</strong>。如果个别特征或多或少看起来不是很像标准正态分布(<strong>具有零均值和单位方差</strong>)，那么它们的表现力可能会较差。</p>
<p>在实际情况中，我们经常忽略特征的分布形状，直接经过去均值来对某个特征进行中心化，再通过除以非常量特征(non-constant features)的标准差进行缩放。</p>
<p>例如，在机器学习算法的目标函数(例如SVM的RBF内核或线性模型的l1和l2正则化)，许多学习算法中目标函数的基础都是假设所有的特征都是零均值并且具有同一阶数上的方差。如果某个特征的方差比其他特征大几个数量级，那么它就会在学习算法中占据主导位置，导致学习器并不能像我们说期望的那样，从其他特征中学习。</p>
<h3 id="scale"><a href="#scale" class="headerlink" title="scale"></a>scale</h3><p>以sklearn为例，函数<code>scale</code>为数组形状的数据集的标准化提供了一个快捷实现：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">from</span> sklearn <span class="keyword">import</span> preprocessing</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>X_train = np.array([[ <span class="number">1.</span>, <span class="number">-1.</span>,  <span class="number">2.</span>],</span><br><span class="line"><span class="meta">... </span>                    [ <span class="number">2.</span>,  <span class="number">0.</span>,  <span class="number">0.</span>],</span><br><span class="line"><span class="meta">... </span>                    [ <span class="number">0.</span>,  <span class="number">1.</span>, <span class="number">-1.</span>]])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>X_scaled = preprocessing.scale(X_train)</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>X_scaled                                          </span><br><span class="line">array([[ <span class="number">0.</span>  ..., <span class="number">-1.22</span>...,  <span class="number">1.33</span>...],</span><br><span class="line">       [ <span class="number">1.22</span>...,  <span class="number">0.</span>  ..., <span class="number">-0.26</span>...],</span><br><span class="line">       [<span class="number">-1.22</span>...,  <span class="number">1.22</span>..., <span class="number">-1.06</span>...]])</span><br></pre></td></tr></table></figure>
<p>经过缩放后的数据具有零均值以及标准方差：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>X_scaled.mean(axis=<span class="number">0</span>)</span><br><span class="line">array([ <span class="number">0.</span>,  <span class="number">0.</span>,  <span class="number">0.</span>])</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>X_scaled.std(axis=<span class="number">0</span>)</span><br><span class="line">array([ <span class="number">1.</span>,  <span class="number">1.</span>,  <span class="number">1.</span>])</span><br></pre></td></tr></table></figure>
<h3 id="StandardScaler"><a href="#StandardScaler" class="headerlink" title="StandardScaler"></a>StandardScaler</h3><p><code>preprocessing</code>模块还提供了一个实用类<code>StandardScaler</code>，它实现了转化器的API来计算训练集上的平均值和标准偏差，以便以后能够在测试集上重新应用相同的变换。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>scaler = preprocessing.StandardScaler().fit(X_train)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>scaler</span><br><span class="line">StandardScaler(copy=<span class="literal">True</span>, with_mean=<span class="literal">True</span>, with_std=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>scaler.mean_                                      </span><br><span class="line">array([ <span class="number">1.</span> ...,  <span class="number">0.</span> ...,  <span class="number">0.33</span>...])</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>scaler.scale_                                       </span><br><span class="line">array([ <span class="number">0.81</span>...,  <span class="number">0.81</span>...,  <span class="number">1.24</span>...])</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>scaler.transform(X_train)                           </span><br><span class="line">array([[ <span class="number">0.</span>  ..., <span class="number">-1.22</span>...,  <span class="number">1.33</span>...],</span><br><span class="line">       [ <span class="number">1.22</span>...,  <span class="number">0.</span>  ..., <span class="number">-0.26</span>...],</span><br><span class="line">       [<span class="number">-1.22</span>...,  <span class="number">1.22</span>..., <span class="number">-1.06</span>...]])</span><br></pre></td></tr></table></figure>
<p>缩放类对象可以在新的数据上实现和训练集相同缩放操作：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>X_test = [[<span class="number">-1.</span>, <span class="number">1.</span>, <span class="number">0.</span>]]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>scaler.transform(X_test)                </span><br><span class="line">array([[<span class="number">-2.44</span>...,  <span class="number">1.22</span>..., <span class="number">-0.26</span>...]])</span><br></pre></td></tr></table></figure>
<p>一种标准化是将特征缩放到给定的最小值和最大值之间，通常在零和一之间，或者也可以将每个特征的最大绝对值转换至单位大小。可以分别使用<code>MinMaxScaler</code>和<code>MaxAbsScaler</code>实现。</p>
<h3 id="MinMaxScaler"><a href="#MinMaxScaler" class="headerlink" title="MinMaxScaler"></a>MinMaxScaler</h3><p>使用这种缩放的目的包括实现特征极小方差的鲁棒性以及在稀疏矩阵中保留零元素。</p>
<p>以下是一个将简单的数据矩阵缩放到<code>[0, 1]</code>的例子：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>X_train = np.array([[ <span class="number">1.</span>, <span class="number">-1.</span>,  <span class="number">2.</span>],</span><br><span class="line"><span class="meta">... </span>                    [ <span class="number">2.</span>,  <span class="number">0.</span>,  <span class="number">0.</span>],</span><br><span class="line"><span class="meta">... </span>                    [ <span class="number">0.</span>,  <span class="number">1.</span>, <span class="number">-1.</span>]])</span><br><span class="line">...</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>min_max_scaler = preprocessing.MinMaxScaler()</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>X_train_minmax = min_max_scaler.fit_transform(X_train)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>X_train_minmax</span><br><span class="line">array([[ <span class="number">0.5</span>       ,  <span class="number">0.</span>        ,  <span class="number">1.</span>        ],</span><br><span class="line">       [ <span class="number">1.</span>        ,  <span class="number">0.5</span>       ,  <span class="number">0.33333333</span>],</span><br><span class="line">       [ <span class="number">0.</span>        ,  <span class="number">1.</span>        ,  <span class="number">0.</span>        ]])</span><br></pre></td></tr></table></figure>
<p>同样的转换实例可以被用与在训练过程中不可见的测试数据，实现和训练数据一致的缩放和移位操作：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>X_test = np.array([[ <span class="number">-3.</span>, <span class="number">-1.</span>,  <span class="number">4.</span>]])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>X_test_minmax = min_max_scaler.transform(X_test)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>X_test_minmax</span><br><span class="line">array([[<span class="number">-1.5</span>       ,  <span class="number">0.</span>        ,  <span class="number">1.66666667</span>]])</span><br></pre></td></tr></table></figure>
<p>可以检查缩放器（scaler）属性，来观察在训练集中学习到的转换操作的基本性质：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>min_max_scaler.scale_                             </span><br><span class="line">array([ <span class="number">0.5</span>       ,  <span class="number">0.5</span>       ,  <span class="number">0.33</span>...])</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>min_max_scaler.min_                               </span><br><span class="line">array([ <span class="number">0.</span>        ,  <span class="number">0.5</span>       ,  <span class="number">0.33</span>...])</span><br></pre></td></tr></table></figure>
<p>如果给<code>MinMaxScaler</code>提供一个明确的 <code>feature_range=(min, max)</code> ，完整的公式是：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">X_std = (X - X.min(axis=0)) / (X.max(axis=0) - X.min(axis=0))</span><br><span class="line">X_scaled = X_std * (max - min) + min</span><br></pre></td></tr></table></figure>
<h3 id="MaxAbsScaler"><a href="#MaxAbsScaler" class="headerlink" title="MaxAbsScaler"></a>MaxAbsScaler</h3><p>类<code>MaxAbsScaler</code>的工作原理非常相似，但是它只通过除以每个特征的最大值将训练数据特征缩放至 <code>[-1, 1]</code> 范围内，这就意味着，训练数据应该是已经零中心化或者是稀疏数据。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>X_train = np.array([[ <span class="number">1.</span>, <span class="number">-1.</span>,  <span class="number">2.</span>],</span><br><span class="line"><span class="meta">... </span>                    [ <span class="number">2.</span>,  <span class="number">0.</span>,  <span class="number">0.</span>],</span><br><span class="line"><span class="meta">... </span>                    [ <span class="number">0.</span>,  <span class="number">1.</span>, <span class="number">-1.</span>]])</span><br><span class="line">...</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>max_abs_scaler = preprocessing.MaxAbsScaler()</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>X_train_maxabs = max_abs_scaler.fit_transform(X_train)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>X_train_maxabs                <span class="comment"># doctest +NORMALIZE_WHITESPACE^</span></span><br><span class="line">array([[ <span class="number">0.5</span>, <span class="number">-1.</span> ,  <span class="number">1.</span> ],</span><br><span class="line">       [ <span class="number">1.</span> ,  <span class="number">0.</span> ,  <span class="number">0.</span> ],</span><br><span class="line">       [ <span class="number">0.</span> ,  <span class="number">1.</span> , <span class="number">-0.5</span>]])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>X_test = np.array([[ <span class="number">-3.</span>, <span class="number">-1.</span>,  <span class="number">4.</span>]])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>X_test_maxabs = max_abs_scaler.transform(X_test)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>X_test_maxabs                 </span><br><span class="line">array([[<span class="number">-1.5</span>, <span class="number">-1.</span> ,  <span class="number">2.</span> ]])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>max_abs_scaler.scale_         </span><br><span class="line">array([ <span class="number">2.</span>,  <span class="number">1.</span>,  <span class="number">2.</span>])</span><br></pre></td></tr></table></figure>
<h3 id="缩放稀疏（矩阵）数据"><a href="#缩放稀疏（矩阵）数据" class="headerlink" title="缩放稀疏（矩阵）数据"></a><a href="http://sklearn.apachecn.org/cn/0.19.0/modules/preprocessing.html#id4" target="_blank" rel="noopener">缩放稀疏（矩阵）数据</a></h3><blockquote>
<p>中心化稀疏(矩阵)数据会破坏数据的稀疏结构，因此很少有一个比较明智的实现方式。但是缩放稀疏输入是有意义的，尤其是当几个特征在不同的量级范围时。</p>
<p><a href="http://sklearn.apachecn.org/cn/0.19.0/modules/generated/sklearn.preprocessing.MaxAbsScaler.html#sklearn.preprocessing.MaxAbsScaler" target="_blank" rel="noopener"><code>MaxAbsScaler</code></a> 以及 <a href="http://sklearn.apachecn.org/cn/0.19.0/modules/generated/sklearn.preprocessing.maxabs_scale.html#sklearn.preprocessing.maxabs_scale" target="_blank" rel="noopener"><code>maxabs_scale</code></a> 是专为缩放数据而设计的，并且是缩放数据的推荐方法。但是， <a href="http://sklearn.apachecn.org/cn/0.19.0/modules/generated/sklearn.preprocessing.scale.html#sklearn.preprocessing.scale" target="_blank" rel="noopener"><code>scale</code></a> 和 <a href="http://sklearn.apachecn.org/cn/0.19.0/modules/generated/sklearn.preprocessing.StandardScaler.html#sklearn.preprocessing.StandardScaler" target="_blank" rel="noopener"><code>StandardScaler</code></a> 也能够接受 <code>scipy.sparse</code> 作为输入，只要参数 <code>with_mean=False</code> 被准确传入它的构造器。否则会出现 <code>ValueError</code> 的错误，因为默认的中心化会破坏稀疏性，并且经常会因为分配过多的内存而使执行崩溃。 <a href="http://sklearn.apachecn.org/cn/0.19.0/modules/generated/sklearn.preprocessing.RobustScaler.html#sklearn.preprocessing.RobustScaler" target="_blank" rel="noopener"><code>RobustScaler</code></a>不能适应稀疏输入，但你可以在稀疏输入使用 <code>transform</code> 方法。</p>
<p>注意，缩放器同时接受压缩的稀疏行和稀疏列(参见 <code>scipy.sparse.csr_matrix</code> 以及 <code>scipy.sparse.csc_matrix</code> )。任何其他稀疏输入将会 <strong>转化为压缩稀疏行表示</strong> 。为了避免不必要的内存复制，建议在上游(早期)选择CSR或CSC表示。</p>
<p>最后，最后，如果已经中心化的数据并不是很大，使用 <code>toarray</code> 方法将输入的稀疏矩阵显式转换为数组是另一种选择。</p>
</blockquote>
<h3 id="缩放有离群值的数据"><a href="#缩放有离群值的数据" class="headerlink" title="缩放有离群值的数据"></a><a href="http://sklearn.apachecn.org/cn/0.19.0/modules/preprocessing.html#id5" target="_blank" rel="noopener">缩放有离群值的数据</a></h3><blockquote>
<p>如果你的数据包含许多异常值，使用均值和方差缩放可能并不是一个很好的选择。这种情况下，你可以使用 <a href="http://sklearn.apachecn.org/cn/0.19.0/modules/generated/sklearn.preprocessing.robust_scale.html#sklearn.preprocessing.robust_scale" target="_blank" rel="noopener"><code>robust_scale</code></a>以及 <a href="http://sklearn.apachecn.org/cn/0.19.0/modules/generated/sklearn.preprocessing.RobustScaler.html#sklearn.preprocessing.RobustScaler" target="_blank" rel="noopener"><code>RobustScaler</code></a> 作为替代品。它们对你的数据的中心和范围使用更有鲁棒性的估计。</p>
<p><strong>参考:</strong></p>
<p>更多关于中心化和缩放数据的重要性讨论在此FAQ中提及: <a href="http://www.faqs.org/faqs/ai-faq/neural-nets/part2/section-16.html" target="_blank" rel="noopener">Should I normalize/standardize/rescale the data?</a></p>
<p><strong>Scaling vs Whitening 有时候独立地中心化和缩放数据是不够的，因为下游的机器学习模型能够对特征之间的线性依赖做出一些假设(这对模型的学习过程来说是不利的)。</strong></p>
<p>要解决这个问题，你可以使用 <a href="http://sklearn.apachecn.org/cn/0.19.0/modules/generated/sklearn.decomposition.PCA.html#sklearn.decomposition.PCA" target="_blank" rel="noopener"><code>sklearn.decomposition.PCA</code></a> 或 <a href="http://sklearn.apachecn.org/cn/0.19.0/modules/generated/sklearn.decomposition.RandomizedPCA.html#sklearn.decomposition.RandomizedPCA" target="_blank" rel="noopener"><code>sklearn.decomposition.RandomizedPCA</code></a> 并指定参数 <code>whiten=True</code> 来更多移除特征间的线性关联。</p>
<p><strong>在回归中缩放目标变量</strong></p>
<p><a href="http://sklearn.apachecn.org/cn/0.19.0/modules/generated/sklearn.preprocessing.scale.html#sklearn.preprocessing.scale" target="_blank" rel="noopener"><code>scale</code></a> 以及 <a href="http://sklearn.apachecn.org/cn/0.19.0/modules/generated/sklearn.preprocessing.StandardScaler.html#sklearn.preprocessing.StandardScaler" target="_blank" rel="noopener"><code>StandardScaler</code></a> 可以直接处理一维数组。在回归中，缩放目标/相应变量时非常有用。</p>
</blockquote>
<h2 id="归一化"><a href="#归一化" class="headerlink" title="归一化"></a>归一化</h2><p>归一化方法的主要有两种形式：一种是把数变为$(0,1)$之间的小数，一种是把有量纲表达式变为无量纲表达式。如果计划使用二次形式(如点积或任何其他核函数)来量化任何样本间的相似度，则此过程将非常有用。</p>
<p>这个观点基于 <a href="https://en.wikipedia.org/wiki/Vector_Space_Model" target="_blank" rel="noopener">向量空间模型(Vector Space Model)</a> ，经常在文本分类和内容聚类中使用。</p>
<p>函数<code>normalize</code>提供了一个快速简单的方法在类似数组的数据集上执行操作，使用 <code>l1</code> 或 <code>l2</code> 范式：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>X = [[ <span class="number">1.</span>, <span class="number">-1.</span>,  <span class="number">2.</span>],</span><br><span class="line"><span class="meta">... </span>     [ <span class="number">2.</span>,  <span class="number">0.</span>,  <span class="number">0.</span>],</span><br><span class="line"><span class="meta">... </span>     [ <span class="number">0.</span>,  <span class="number">1.</span>, <span class="number">-1.</span>]]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>X_normalized = preprocessing.normalize(X, norm=<span class="string">'l2'</span>)</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>X_normalized                                      </span><br><span class="line">array([[ <span class="number">0.40</span>..., <span class="number">-0.40</span>...,  <span class="number">0.81</span>...],</span><br><span class="line">       [ <span class="number">1.</span>  ...,  <span class="number">0.</span>  ...,  <span class="number">0.</span>  ...],</span><br><span class="line">       [ <span class="number">0.</span>  ...,  <span class="number">0.70</span>..., <span class="number">-0.70</span>...]])</span><br></pre></td></tr></table></figure>
<p><code>preprocessing</code> 预处理模块提供的<code>Normalizer</code>工具类使用 <code>Transformer</code> API 实现了相同的操作（在这种情况下， <code>fit</code> 方法是无用的：该类是无状态的，因为该操作独立对待样本）。</p>
<p>在这之后归一化实例可以被使用在样本向量中，像任何其他转换器一样:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>normalizer.transform(X)                            </span><br><span class="line">array([[ <span class="number">0.40</span>..., <span class="number">-0.40</span>...,  <span class="number">0.81</span>...],</span><br><span class="line">       [ <span class="number">1.</span>  ...,  <span class="number">0.</span>  ...,  <span class="number">0.</span>  ...],</span><br><span class="line">       [ <span class="number">0.</span>  ...,  <span class="number">0.70</span>..., <span class="number">-0.70</span>...]])</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>normalizer.transform([[<span class="number">-1.</span>,  <span class="number">1.</span>, <span class="number">0.</span>]])             </span><br><span class="line">array([[<span class="number">-0.70</span>...,  <span class="number">0.70</span>...,  <span class="number">0.</span>  ...]])</span><br></pre></td></tr></table></figure>
<p><strong>稀疏(数据)输入</strong></p>
<p>函数<code>normalize</code>接收<strong>来自scipy.sparse的密集类数组数据和稀疏矩阵</strong>作为输入。</p>
<p>对于稀疏输入，在被提交给高效Cython例程前，数据被<strong>转化为压缩的稀疏行形式</strong> (参见 <code>scipy.sparse.csr_matrix</code> )。为了避免不必要的内存复制，推荐在上游选择CSR表示。</p>
</div><div class="tags"><a href="/tags/标准化/">标准化</a></div><div class="post-nav"><a class="pre" href="/2018-7-24-CTR模型/">CTR模型介绍</a><a class="next" href="/Regression-Summary/">回归问题总结</a></div></div></div></div><div class="pure-u-1-4 hidden_mid_and_down"><div id="sidebar"><div class="widget"><form class="search-form" action="//www.google.com/search" method="get" accept-charset="utf-8" target="_blank"><input type="text" name="q" maxlength="20" placeholder="Search"><input type="hidden" name="sitesearch" value="https://chaideblog.github.io"></form></div><div class="widget"><div class="widget-title"><i class="fa fa-folder-o"> 分类</i></div><ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/Algorithm/">Algorithm</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/BigData/">BigData</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Deep-Learning/">Deep Learning</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Interview/">Interview</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Java/">Java</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Linux/">Linux</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Machine-Learning/">Machine Learning</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Math/">Math</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Python/">Python</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-star-o"> 标签</i></div><div class="tagcloud"><a href="/tags/Probability-theroy/" style="font-size: 15px;">Probability theroy</a> <a href="/tags/Genetic-Algorithm/" style="font-size: 15px;">Genetic Algorithm</a> <a href="/tags/决策树/" style="font-size: 15px;">决策树</a> <a href="/tags/Python语法/" style="font-size: 15px;">Python语法</a> <a href="/tags/VPS/" style="font-size: 15px;">VPS</a> <a href="/tags/Convex-Optimize/" style="font-size: 15px;">Convex Optimize</a> <a href="/tags/Naive-Bayes/" style="font-size: 15px;">Naive Bayes</a> <a href="/tags/Sublime/" style="font-size: 15px;">Sublime</a> <a href="/tags/Random-Forests/" style="font-size: 15px;">Random Forests</a> <a href="/tags/集成学习/" style="font-size: 15px;">集成学习</a> <a href="/tags/Hadoop/" style="font-size: 15px;">Hadoop</a> <a href="/tags/Java/" style="font-size: 15px;">Java</a> <a href="/tags/FFT/" style="font-size: 15px;">FFT</a> <a href="/tags/Apache/" style="font-size: 15px;">Apache</a> <a href="/tags/树莓派/" style="font-size: 15px;">树莓派</a> <a href="/tags/SVM/" style="font-size: 15px;">SVM</a> <a href="/tags/SGD/" style="font-size: 15px;">SGD</a> <a href="/tags/BGD/" style="font-size: 15px;">BGD</a> <a href="/tags/MBGD/" style="font-size: 15px;">MBGD</a> <a href="/tags/SMO/" style="font-size: 15px;">SMO</a> <a href="/tags/标准化/" style="font-size: 15px;">标准化</a> <a href="/tags/回归/" style="font-size: 15px;">回归</a> <a href="/tags/CTR/" style="font-size: 15px;">CTR</a> <a href="/tags/LeetCode/" style="font-size: 15px;">LeetCode</a> <a href="/tags/LightGBM/" style="font-size: 15px;">LightGBM</a> <a href="/tags/RCNN/" style="font-size: 15px;">RCNN</a> <a href="/tags/目标检测/" style="font-size: 15px;">目标检测</a> <a href="/tags/Interview/" style="font-size: 15px;">Interview</a> <a href="/tags/LR/" style="font-size: 15px;">LR</a> <a href="/tags/FM-FFM/" style="font-size: 15px;">FM/FFM</a> <a href="/tags/Recommend-System/" style="font-size: 15px;">Recommend System</a> <a href="/tags/Natural-Language-Processing/" style="font-size: 15px;">Natural Language Processing</a> <a href="/tags/Recommendation/" style="font-size: 15px;">Recommendation</a></div></div><div class="widget"><div class="widget-title"><i class="fa fa-file-o"> 最近文章</i></div><ul class="post-list"><li class="post-list-item"><a class="post-list-link" href="/2019-5-5-Paper-TEM/">基于决策树的可解释嵌入推荐模型：TEM</a></li><li class="post-list-item"><a class="post-list-link" href="/2019-5-4-Transformer/">Transformer理解与Tensorflow代码阅读</a></li><li class="post-list-item"><a class="post-list-link" href="/2019-4-28-Normalization/">深度学习中各种Normalization</a></li><li class="post-list-item"><a class="post-list-link" href="/2019-4-22-recommend-system-1/">《推荐系统实战》读书笔记（一）</a></li><li class="post-list-item"><a class="post-list-link" href="/2019-4-6-toutiao-interview/">字节跳动广告算法团队实习面试经历（Offer）</a></li><li class="post-list-item"><a class="post-list-link" href="/2019-3-13-xiaomi-interview/">小米信息流推荐实习面试经历（Offer）</a></li><li class="post-list-item"><a class="post-list-link" href="/2019-3-8-sohu-interview/">搜狐推荐算法实习面试经历（Offer）</a></li><li class="post-list-item"><a class="post-list-link" href="/2019-2-27-CTR-LR-FM-FFM/">CTR系列（一）：LR、FM/FFM</a></li><li class="post-list-item"><a class="post-list-link" href="/2019-2-21-ms-bing-interview/">微软Bing组算法（Game Over）</a></li><li class="post-list-item"><a class="post-list-link" href="/2019-1-8-PriorityQueue/">【转】Java容器源码分析之PriorityQueue</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-external-link"> 友情链接</i></div><ul></ul><a href="http://www.example1.com/" title="site-name1" target="_blank">site-name1</a><ul></ul><a href="http://www.example2.com/" title="site-name2" target="_blank">site-name2</a><ul></ul><a href="http://www.example3.com/" title="site-name3" target="_blank">site-name3</a></div></div></div><div class="pure-u-1 pure-u-md-3-4"><div id="footer">Copyright © 2019 <a href="/." rel="nofollow">CHAI' blog.</a> Powered by<a rel="nofollow" target="_blank" href="https://hexo.io"> Hexo.</a><a rel="nofollow" target="_blank" href="https://github.com/tufu9441/maupassant-hexo"> Theme</a> by<a rel="nofollow" target="_blank" href="https://github.com/pagecho"> Cho.</a></div></div></div><a class="show" id="rocket" href="#top"></a><script type="text/javascript" src="/js/totop.js?v=0.0.0" async></script><script type="text/javascript" src="//lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.js" async></script><script type="text/javascript" src="/js/fancybox.js?v=0.0.0" async></script><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.css"><script type="text/x-mathjax-config">MathJax.Hub.Config({
  tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}
  });
</script><script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML" async></script><script type="text/javascript" src="/js/codeblock-resizer.js?v=0.0.0"></script><script type="text/javascript" src="/js/smartresize.js?v=0.0.0"></script></div></body></html>