<!DOCTYPE html><html lang="zh-CN"><head><meta name="generator" content="Hexo 3.8.0"><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description"><title>支持向量机SVM理论——基本数学原理 | CHAI' blog</title><link rel="stylesheet" type="text/css" href="/css/style.css?v=0.0.0"><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/normalize/8.0.1/normalize.min.css"><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/pure/1.0.0/pure-min.css"><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/pure/1.0.0/grids-responsive-min.css"><link rel="stylesheet" href="//lib.baomitu.com/font-awesome/4.7.0/css/font-awesome.min.css"><script type="text/javascript" src="//lib.baomitu.com/jquery/3.4.0/jquery.min.js"></script><link rel="icon" mask sizes="any" href="/favicon.ico"><link rel="Shortcut Icon" type="image/x-icon" href="/favicon.ico"><link rel="apple-touch-icon" href="/apple-touch-icon.png"><link rel="apple-touch-icon-precomposed" href="/apple-touch-icon.png"></head><body><div class="body_container"><div id="header"><div class="site-name"><h1 class="hidden">支持向量机SVM理论——基本数学原理</h1><a id="logo" href="/.">CHAI' blog</a><p class="description"></p></div><div id="nav-menu"><a class="current" href="/."><i class="fa fa-home"> 首页</i></a><a href="/archives/"><i class="fa fa-archive"> 归档</i></a><a href="/about/"><i class="fa fa-user"> 关于</i></a><a href="/atom.xml"><i class="fa fa-rss"> 订阅</i></a></div></div><div class="pure-g" id="layout"><div class="pure-u-1 pure-u-md-3-4"><div class="content_container"><div class="post"><h1 class="post-title">支持向量机SVM理论——基本数学原理</h1><div class="post-meta">Sep 23, 2017<span> | </span><span class="category"><a href="/categories/Machine-Learning/">Machine Learning</a></span></div><div class="clear"><div class="toc-article" id="toc"><div class="toc-title">文章目录</div><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#基本形式"><span class="toc-number">1.</span> <span class="toc-text">基本形式</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#对偶问题"><span class="toc-number">2.</span> <span class="toc-text">对偶问题</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#软间隔"><span class="toc-number">3.</span> <span class="toc-text">软间隔</span></a></li></ol></div></div><div class="post-content"><h2 id="基本形式"><a href="#基本形式" class="headerlink" title="基本形式"></a>基本形式</h2><p>给定训练样本集$D={(x_1,y_1),(x_2,y_2),…,(x_m,y_m)},y_i\in{-1,+1}$，分类学习的思想就是在训练集$D$的样本空间内找到一个划分超平面，将样本尽可能地分离开。</p>
<p>在样本空间中，划分超平面可以通过如下的线性方程来描述：</p>
<p>$$\omega^{T}x+b=0,\omega=(\omega_1,\omega_2,…,\omega_d) \tag{1}$$</p>
<p>$\omega$作为法向量，决定了超平面的方向；$b$作为位移项，决定了平面距离原点的距离。该式确定了唯一的超平面，不难看出决定超平面的仅仅是$\omega$和$b$两个量，所以后面仅使用$(\omega,b)$代表超平面。根据上式，样本空间任意点$x$到超平面的距离为</p>
<p>$$r=\frac{|\omega^{T}x+b|}{\parallel\omega\parallel} \tag{2}$$</p>
<blockquote>
<p>上面的公式可以由二维、三维推广来：</p>
<p><img src="SVM_1.jpg" alt="超平面距离"></p>
</blockquote>
<p>假设超平面$(\omega,b)$能够将训练样本正确分类，即对$(x_i,y_i) \in D$</p>
<p>$$\begin{cases} \omega^{‘T}x+b^{‘} &gt; 0, &amp; y_i=+1 \ \omega^{‘T}x+b^{‘} &lt; 0, &amp; y_i=-1 \end{cases} \tag{3}$$</p>
<p>通过一定的缩放变换，可以得到下式：</p>
<p>$$\begin{cases} \omega^{T}x+b \geq +1, &amp; y_i=+1 \ \omega^{T}x+b \leq -1, &amp; y_i=-1 \end{cases} \tag{4}$$</p>
<blockquote>
<p>若超平面$(\omega^{‘}, b^{‘})$能将训练样本正确分类，则总存在缩放变换$\zeta \omega^{‘} \to \omega$和$\zeta b^{‘} \to b$，使(4)式成立。</p>
</blockquote>
<p>每个样本都对应了一个特征向量，特殊地，将距离超平面最近同时使上式等号成立的样本称为“支持向量”。支持向量到超平面的距离之和称为“间隔”：</p>
<p>$$\gamma=\frac{2}{\parallel\omega\parallel} \tag{5}$$</p>
<p><img src="SVM_2.png" alt="支持向量"></p>
<p>为了找到“最大间隔”的划分超平面，即求解下面的凸优化问题</p>
<p>$$\max \limits_{\omega,b} \frac{2}{\parallel\omega\parallel} \tag{6}$$</p>
<p>$$s.t. y_i(\omega^{T}x_i+b) \geq 1,i=1,2,…,m.$$</p>
<p>转化为凸优化理论的标准形式为：</p>
<p>$$\min \limits_{\omega,b} \frac{1}{2}\parallel\omega\parallel^2 \tag{7}$$</p>
<p>$$s.t. y_i(\omega^{T}x_i+b) \geq 1,i=1,2,…,m.$$</p>
<p>其中，$\parallel\omega^{-1}\parallel$转化为$\parallel\omega\parallel^2$的形式，是为了后面对偶变换的方便。            </p>
<h2 id="对偶问题"><a href="#对偶问题" class="headerlink" title="对偶问题"></a>对偶问题</h2><p>上面的问题是一个凸二次规划问题，可以使用拉格朗日乘子法转化为“对偶问题”(<a href="The_Lagrange_dual_function.pdf">拉格朗日乘子法简介</a>)。</p>
<p>拉格朗日对偶问题的思路是将上面的约束条件引入目标函数，假设对本问题的约束条添加拉格朗日乘子$\lambda_i\geq0$，则本问题的拉格朗日对偶函数可写为：</p>
<p>$$L(\omega,b,\lambda)=\frac{1}{2}\parallel\omega\parallel^2+\sum_{i=1}^{m} \lambda_i(1-y_i(\omega^{T}x_i+b)) \tag{8}$$</p>
<p>令$L(\omega,b,\lambda)$对$\omega$和$b$的偏导为0，可得</p>
<p>$$\omega = \sum_{i=1}^{m} \lambda_i y_i x_i \tag{9}$$</p>
<p>$$0 = \sum_{i=1}^{m} \lambda_i y_i \tag{10}$$ </p>
<blockquote>
<p>实值函数对矩阵/向量的导数：</p>
<ul>
<li>要点：求导结果与自变量同型，且每个元素就是对自变量的相应分量求导</li>
<li><p>若函数$f: \mathcal{R}^{m \times n}  \to \mathcal{R}$，则$\partial{f} / \partial{X}$也是一个$m \times n$维矩阵，且$( \partial{f} / \partial{X} ) _{ij} = \partial{f} / \partial{x} _{ij}$。也可以使用劈形算子将导数记作$\bigtriangledown _{X} f$。</p>
</li>
<li><p>由于向量是矩阵的特殊情形，根据上面的定义也可以得到自变量为向量时的定义：若函数$f: \mathcal{R}^{m}  \to \mathcal{R}$，则$\partial{f} / \partial{X}$也是一个m维向量，且$( \partial{f} / \partial{X} ) _{i} = \partial{f} / \partial{x} _{i}$。若自变量是行向量则结果为行向量，可记作$\bigtriangledown _{X^T} f$或$\partial{f} / \partial{X} ^T$；若自变量是列向量则求导结果为列向量，可记作$\bigtriangledown _X f$或$\partial{f} / \partial{X}$。</p>
</li>
</ul>
<p>来源：<a href="机器学习中的矩阵、向量求导.pdf">机器学习中的矩阵、向量求导.pdf</a></p>
</blockquote>
<p>将上面式(9)(10)带回式(8)，即得到对偶问题</p>
<p>$$\max \limits_{\lambda} \sum_{i=1}^{m} \lambda_i - \frac{1}{2} \sum_{i=1}^{m} \sum_{j=1}^{m} \lambda_i \lambda_j y_i y_j x_i^{T} x_j \tag{11}$$</p>
<p>$$s.t.\sum_{i=1}^{m} \lambda_i y_i = 0, \lambda_i \geq 0,i=1,2,…,m.$$</p>
<blockquote>
<p>证明过程：</p>
<p>$$\frac{1}{2}\parallel\omega\parallel^2+\sum_{i=1}^{m} \lambda_i(1-y_i(\omega^{T}x_i+b)) \ = \frac{1}{2} \omega^2 + \sum_{i = 1}^{m} \lambda_{i} - \sum_{i = 1}^{m} \lambda_{i} y_{i} \omega^{T} x_{i} - \sum_{i = 1}^{m} \lambda_{i} y_{i} b $$</p>
<p>因为，$\sum_{i=1}^{m} \lambda_i y_i = 0$，所以$(\sum_{i=1}^{m} \lambda_i y_i) b = 0$。又因为$\omega=\sum_{i=1}^{m} \lambda_i y_i x_i$,</p>
<p>$$= \frac{1}{2} \omega^2 + \sum_{i=1}^{m} \lambda_i - \omega^T \omega \ = - \frac{1}{2} \omega^2 + \sum_{i=1}^m \lambda_i \ = \sum_{i=1}^m \lambda_i - \frac{1}{2} { \sum_{i=1}^m \lambda_i y_i x_i \cdot \sum_{i=j}^m \lambda_j y_j x_j^T } \ = \sum_{i=1}^m \lambda_i - \frac{1}{2} \sum_{i=1}^m \sum_{i=j}^m \lambda_i \lambda_j y_i y_j x_i x_j^T$$</p>
</blockquote>
<p>根据拉格朗日对偶原理，对偶问题的最优值等于原问题的最优值，必须满足KKT条件。KKT条件包括：</p>
<p>$$\begin{cases} \bigtriangledown_x L( x^<em>, \lambda^</em>, b^<em> ) = 0 \ \bigtriangledown_{\lambda} L( x^</em>, \lambda^<em>, b^</em> ) = 0 \ \bigtriangledown_{b} L( x^<em>, \lambda^</em>, b^<em> ) = 0 \ \lambda_{i} ^</em> g ( x_i ^<em> ) = 0 \ \lambda_i \ge 0 \ g ( x_i ) \ge 0 \ h ( x_j ^</em> ) = 0 \end{cases} \tag{12}$$</p>
<p>这明显是一个二次规划，我们可以通过SMO算法求解。SMO的基本思想就是固定$\lambda_{i}$，然后求解$\lambda_{i}$上的极值。这里，由于有$\sum_{i=1}^{m} \lambda_i y_i$约束，所以选择两个变量$\lambda_{i}$和$\lambda_{j}$，并固定其他变量。SMO不断执行下面的操作，直到收敛：</p>
<ul>
<li><p>选取一对变量$\lambda_{i}$和$\lambda_{j}$；</p>
</li>
<li><p>固定$\lambda_{i}$和$\lambda_{j}$之外的参数，根据式(13)，更新$\lambda_{i}$和$\lambda_{j}$。</p>
</li>
</ul>
<p>注意选择变量$\lambda_{i}$和$\lambda_{j}$，需要选择对应的样本间间隔最大。式(11)可以转化为一个二次方程，可以直接求极值。</p>
<p>求解得到$\lambda$后，可求得$\omega$和$b$：</p>
<ol>
<li><p>$\omega$的求法：带回$\omega=\sum_{i=1}^{m} \lambda_i y_i x_i$即可求得，</p>
</li>
<li><p>$b$的求法：由于支持向量$(x_s,y_s)$有$y_s f(x_s)=1$，即</p>
</li>
</ol>
<p>$$y_s (\sum_{i \in S} \lambda_i y_i x_i^T x_s + b) = 1 \tag{13}$$</p>
<p>其中，$S = \lbrace i  \vert \lambda_i &gt; 0, i = 1, 2, …, m \rbrace$。</p>
<p>实际上现实任务中，常采取求支持向量求解的平均值</p>
<p>$$b=\frac{1}{|S|} \sum_{s \in S} (y_s - \sum_{i \in S} \lambda_i y_i x_i^T x_s) \tag{14}$$</p>
<p>继而可以求解出超平面方程</p>
<p>$$f(x)=\omega^T x + b = \sum_{i=1}^{m} \lambda_i y_i x_i^T x + b \tag{15}$$</p>
<h2 id="软间隔"><a href="#软间隔" class="headerlink" title="软间隔"></a>软间隔</h2><p>实际问题中，有时无法找到恰当的超平面分割数据，或者难以避免SVM过拟合。所以，引入“软间隔”的概念，允许某些数据点不满足</p>
<p>$$y_i (\omega^T x_i + b) \ge 1 \tag{16}$$</p>
<p>通过修改公式(7)，引入松弛变量，可以在最大化间隔时，不满足约束的样本尽可能少。</p>
<p>$$\min \limits_{\omega,b} \frac{1}{2}\parallel\omega\parallel^2 + C \sum_{i}^{m} \ell_{0/1} (y_i (\omega^T x_i + b) - 1) \tag{17}$$</p>
<p>其中，$C&gt;0$是一个常数，当$C \to \infty$强迫所有样本满足式(17)。$\ell_{0/1}$是0/1损失函数，类似的函数还有</p>
<p>hinge损失：$\ell_{hinge}(z) = \max(0, 1-z)$</p>
<p>指数损失：$\ell_{exp}(z) = \exp (-z)$</p>
<p>对率损失：$\ell_{log}(z) = \log(1 + \exp(-z))$</p>
<p>引入松弛变量$\xi_i \ge 0$，可以将式(17)改成</p>
<p>$$\min \limits_{\omega,b} \frac{1}{2}\parallel\omega\parallel^2 + C \sum_{i}^{m} \xi_i \ s.t. \quad y_i(\omega^{T}x_i+b) \geq 1 - \xi_i \ \xi_i \ge 0 ,i=1,2,…,m. \tag{18}$$</p>
<p>因此，式(18)的拉格朗日函数为</p>
<p>$$L(\omega,b,\lambda, \xi, \mu)=\frac{1}{2}\parallel\omega\parallel^2 + C\sum_{i=1}^{m} \xi_i + \sum_{i=1}^{m} \lambda_i(1 - \xi_i - y_i(\omega^{T}x_i+b)) - \sum_{i=1}^{m} \mu_i \xi_i \tag{19}$$</p>
<p>其中，$\lambda_i \ge 0, \mu_i \ge 0$是拉格朗日乘子。式(19)的偏导数为0，可得</p>
<p>$$\omega = \sum_{i=1}^{m} \lambda_i y_i x_i \tag{20}$$</p>
<p>$$0 = \sum_{i=1}^{m} \lambda_i y_i \tag{21}$$</p>
<p>$$C = \lambda_i + \mu_i \tag{22}$$</p>
<p>将式(20)(21)(22)带回公式(19)，得到对偶函数为</p>
<p>$$\max \limits_{\lambda} \sum_{i=1}^{m} \lambda_i - \frac{1}{2} \sum_{i=1}^{m} \sum_{j=1}^{m} \lambda_i \lambda_j y_i y_j x_i^{T} x_j \ s.t. \quad \sum_{i=1}^{m} \lambda_i y_i = 0, \ 0 \le  \lambda_i \le C ,i=1,2,…,m. \tag{23}$$</p>
<p>此时的KKT条件为</p>
<p>$$\begin{cases} \lambda_i \ge 0, \mu_i \ge 0, \ y_i f(x_i) - 1 + \xi_i \ge 0, \ \lambda_i (y_if(x_i) - 1 + \xi_i) = 0, \ \xi_i \ge 0, \mu_i \xi_i = 0.  \end{cases} \tag{24}$$</p>
<p>软间隔的求解方式并没有太大变化，仅是$\lambda$参数多了上界。</p>
</div><div class="tags"><a href="/tags/SVM/">SVM</a></div><div class="post-nav"><a class="pre" href="/2017-9-24-ML-Gradient-Descent/">梯度下降法（BGD、SGD、MBGD）以及Python实现</a><a class="next" href="/2017-7-20-Apache-Cloud-Storage/">Apache搭建自己的网盘服务器</a></div></div></div></div><div class="pure-u-1-4 hidden_mid_and_down"><div id="sidebar"><div class="widget"><form class="search-form" action="//www.google.com/search" method="get" accept-charset="utf-8" target="_blank"><input type="text" name="q" maxlength="20" placeholder="Search"><input type="hidden" name="sitesearch" value="https://chaideblog.github.io"></form></div><div class="widget"><div class="widget-title"><i class="fa fa-folder-o"> 分类</i></div><ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/Algorithm/">Algorithm</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/BigData/">BigData</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Deep-Learning/">Deep Learning</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Interview/">Interview</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Java/">Java</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Linux/">Linux</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Machine-Learning/">Machine Learning</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Math/">Math</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Python/">Python</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-star-o"> 标签</i></div><div class="tagcloud"><a href="/tags/Probability-theroy/" style="font-size: 15px;">Probability theroy</a> <a href="/tags/Genetic-Algorithm/" style="font-size: 15px;">Genetic Algorithm</a> <a href="/tags/决策树/" style="font-size: 15px;">决策树</a> <a href="/tags/Python语法/" style="font-size: 15px;">Python语法</a> <a href="/tags/VPS/" style="font-size: 15px;">VPS</a> <a href="/tags/Convex-Optimize/" style="font-size: 15px;">Convex Optimize</a> <a href="/tags/Naive-Bayes/" style="font-size: 15px;">Naive Bayes</a> <a href="/tags/Sublime/" style="font-size: 15px;">Sublime</a> <a href="/tags/Random-Forests/" style="font-size: 15px;">Random Forests</a> <a href="/tags/集成学习/" style="font-size: 15px;">集成学习</a> <a href="/tags/Hadoop/" style="font-size: 15px;">Hadoop</a> <a href="/tags/Java/" style="font-size: 15px;">Java</a> <a href="/tags/FFT/" style="font-size: 15px;">FFT</a> <a href="/tags/Apache/" style="font-size: 15px;">Apache</a> <a href="/tags/树莓派/" style="font-size: 15px;">树莓派</a> <a href="/tags/SVM/" style="font-size: 15px;">SVM</a> <a href="/tags/SGD/" style="font-size: 15px;">SGD</a> <a href="/tags/BGD/" style="font-size: 15px;">BGD</a> <a href="/tags/MBGD/" style="font-size: 15px;">MBGD</a> <a href="/tags/SMO/" style="font-size: 15px;">SMO</a> <a href="/tags/标准化/" style="font-size: 15px;">标准化</a> <a href="/tags/回归/" style="font-size: 15px;">回归</a> <a href="/tags/CTR/" style="font-size: 15px;">CTR</a> <a href="/tags/LeetCode/" style="font-size: 15px;">LeetCode</a> <a href="/tags/LightGBM/" style="font-size: 15px;">LightGBM</a> <a href="/tags/RCNN/" style="font-size: 15px;">RCNN</a> <a href="/tags/目标检测/" style="font-size: 15px;">目标检测</a> <a href="/tags/Interview/" style="font-size: 15px;">Interview</a> <a href="/tags/LR/" style="font-size: 15px;">LR</a> <a href="/tags/FM-FFM/" style="font-size: 15px;">FM/FFM</a> <a href="/tags/Recommend-System/" style="font-size: 15px;">Recommend System</a> <a href="/tags/Natural-Language-Processing/" style="font-size: 15px;">Natural Language Processing</a> <a href="/tags/Recommendation/" style="font-size: 15px;">Recommendation</a></div></div><div class="widget"><div class="widget-title"><i class="fa fa-file-o"> 最近文章</i></div><ul class="post-list"><li class="post-list-item"><a class="post-list-link" href="/2019-5-5-Paper-TEM/">基于决策树的可解释嵌入推荐模型：TEM</a></li><li class="post-list-item"><a class="post-list-link" href="/2019-5-4-Transformer/">Transformer理解与Tensorflow代码阅读</a></li><li class="post-list-item"><a class="post-list-link" href="/2019-4-28-Normalization/">深度学习中各种Normalization</a></li><li class="post-list-item"><a class="post-list-link" href="/2019-4-22-recommend-system-1/">《推荐系统实战》读书笔记（一）</a></li><li class="post-list-item"><a class="post-list-link" href="/2019-4-6-toutiao-interview/">字节跳动广告算法团队实习面试经历（Offer）</a></li><li class="post-list-item"><a class="post-list-link" href="/2019-3-13-xiaomi-interview/">小米信息流推荐实习面试经历（Offer）</a></li><li class="post-list-item"><a class="post-list-link" href="/2019-3-8-sohu-interview/">搜狐推荐算法实习面试经历（Offer）</a></li><li class="post-list-item"><a class="post-list-link" href="/2019-2-27-CTR-LR-FM-FFM/">CTR系列（一）：LR、FM/FFM</a></li><li class="post-list-item"><a class="post-list-link" href="/2019-2-21-ms-bing-interview/">微软Bing组算法（Game Over）</a></li><li class="post-list-item"><a class="post-list-link" href="/2019-1-8-PriorityQueue/">【转】Java容器源码分析之PriorityQueue</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-external-link"> 友情链接</i></div><ul></ul><a href="http://www.example1.com/" title="site-name1" target="_blank">site-name1</a><ul></ul><a href="http://www.example2.com/" title="site-name2" target="_blank">site-name2</a><ul></ul><a href="http://www.example3.com/" title="site-name3" target="_blank">site-name3</a></div></div></div><div class="pure-u-1 pure-u-md-3-4"><div id="footer">Copyright © 2019 <a href="/." rel="nofollow">CHAI' blog.</a> Powered by<a rel="nofollow" target="_blank" href="https://hexo.io"> Hexo.</a><a rel="nofollow" target="_blank" href="https://github.com/tufu9441/maupassant-hexo"> Theme</a> by<a rel="nofollow" target="_blank" href="https://github.com/pagecho"> Cho.</a></div></div></div><a class="show" id="rocket" href="#top"></a><script type="text/javascript" src="/js/totop.js?v=0.0.0" async></script><script type="text/javascript" src="//lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.js" async></script><script type="text/javascript" src="/js/fancybox.js?v=0.0.0" async></script><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.css"><script type="text/x-mathjax-config">MathJax.Hub.Config({
  tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}
  });
</script><script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML" async></script><script type="text/javascript" src="/js/codeblock-resizer.js?v=0.0.0"></script><script type="text/javascript" src="/js/smartresize.js?v=0.0.0"></script></div></body></html>